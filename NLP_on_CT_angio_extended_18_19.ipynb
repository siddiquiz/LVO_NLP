{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Input Excel File, change path accordingly\n",
    "raw_dir = \"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Data/RAPID_18_19.xlsx\"\n",
    "raw_df=pd.read_excel(io=raw_dir)\n",
    "# print(raw_df.columns)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LVO</th>\n",
       "      <th>Side</th>\n",
       "      <th>site occlusion</th>\n",
       "      <th>CTA CTP Report Text</th>\n",
       "      <th>RAPID Core</th>\n",
       "      <th>RAPID Penumbra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LVO Side site occlusion                                CTA CTP Report Text  \\\n",
       "0    0  NaN            NaN  EXAMINATION: Computed tomography angiography (...   \n",
       "1    0  NaN            NaN  EXAMINATION: Computed tomography angiography (...   \n",
       "2    0  NaN            NaN  EXAMINATION: Computed tomography angiography (...   \n",
       "3    0  NaN            NaN  EXAMINATION: Computed tomography angiography (...   \n",
       "4    0  NaN            NaN  EXAMINATION: Computed tomography angiography (...   \n",
       "\n",
       "   RAPID Core  RAPID Penumbra  \n",
       "0         NaN             NaN  \n",
       "1        13.0             5.0  \n",
       "2         NaN             NaN  \n",
       "3         NaN             NaN  \n",
       "4         NaN             NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_df = raw_df[[\"LVO\",\"Side\",\"site occlusion\",\"CTA CTP Report Text\",\"RAPID Core\",\"RAPID Penumbra\"]]\n",
    "trim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncertain LVO 1?\n",
    "# trim_df.iloc[504]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter if LVO uncertain\n",
    "# filtered_df = trim_df[~trim_df[\"LVO\"].str.contains(\"?\",regex=False).fillna(False)]\n",
    "# filtered_df.iloc[50]\n",
    "filtered_df = trim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that reads the angio report cerebral artery table\n",
    "#from pandas dataframe\n",
    "#and outputs a python dictionary\n",
    "#\n",
    "#If \"no occlusion\" is present for a given artery, value is marked as 0\n",
    "#Otherwise, the program looks to free-body text for clarification\n",
    "#These values are marked with 'None'\n",
    "import copy\n",
    "import re\n",
    "\n",
    "class ca_templates_reader:\n",
    "    \n",
    "    def __init__(self,df_column):\n",
    "        \n",
    "        self.df_column = df_column\n",
    "        \n",
    "        #cerebral arteries dictionary\n",
    "        self.main_dict = {\n",
    "            \"L CCA\": None,\n",
    "            \"L carotid bifurcation\": None,\n",
    "            \"L ICA proximal\": None,\n",
    "            \"L ICA distal\": None,\n",
    "            \"L ICA terminus\": None,\n",
    "            \"L M1\": None,\n",
    "            \"L M2 branches\": None,\n",
    "            \"L A2\": None,\n",
    "            \"R CCA\": None,\n",
    "            \"R carotid bifurcation\": None,\n",
    "            \"R ICA proximal\": None,\n",
    "            \"R ICA distal\": None,\n",
    "            \"R ICA terminus\": None,\n",
    "            \"R M1\": None,\n",
    "            \"R M2 branches\": None,\n",
    "            \"R A2\": None,\n",
    "            \"L Vertebral Artery\": None,\n",
    "            \"R Vertebral Artery\": None,\n",
    "            \"Basilar Artery\": None,\n",
    "            \"L PCA\": None,\n",
    "            \"R PCA\": None\n",
    "\n",
    "        }\n",
    "        \n",
    "        self.ca_pattern_dict = copy.deepcopy(self.main_dict)\n",
    "    \n",
    "    def compile_regex(self):\n",
    "        #Compiling regex patterns of the cerebral artery strings\n",
    "        #For optimal performance\n",
    "        \n",
    "        for ca in self.ca_pattern_dict:\n",
    "            self.ca_pattern_dict[ca]=re.compile(ca+':.*\\n')\n",
    "        \n",
    "    \n",
    "    def read_ca_template(self, text):\n",
    "        \n",
    "        #Example cerebral arteries template line:\n",
    "        #R CCA: No occlusion or significant stenosis\n",
    "\n",
    "        no_occlusion = re.compile('(n|N)o occlusion')\n",
    "    #     for file in raw_text_files:\n",
    "        curr_dict = copy.deepcopy(self.main_dict)\n",
    "        for ca in curr_dict:\n",
    "            #Searching for template text for each cerebral artery\n",
    "            #Some follow up reports may not have the template\n",
    "            \n",
    "            search = self.ca_pattern_dict[ca].search(text)\n",
    "            if search is not None:\n",
    "                #string of ca template line\n",
    "                sent = search.group()\n",
    "                #print(sent)\n",
    "                #Expect match most cases, change dict value to 0 (no occlusion)\n",
    "                #If 'no occlusion' not in cerebral a. desc, leave with None value (default flag)\n",
    "                #Will determine later\n",
    "                if no_occlusion.search(sent) is not None:\n",
    "                    curr_dict[ca] = 0\n",
    "        return curr_dict\n",
    "    #         ca_dicts.append(curr_dict)\n",
    "    \n",
    "    def read_templates(self):\n",
    "        #List to store the cerebral artery dictionaries for report\n",
    "        ca_dicts = []\n",
    "        for report in self.df_column:\n",
    "            ca_dicts.append(self.read_ca_template(report))\n",
    "        return ca_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = ca_templates_reader(filtered_df[\"CTA CTP Report Text\"])\n",
    "ctr.compile_regex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class report_text_extractor:\n",
    "    \n",
    "    def __init__(self, df_column):\n",
    "        self.df_column = df_column\n",
    "        self.report_template_pattern = re.compile('Left anterior circulation:[\\s\\S]*IMPRESSION', re.I)\n",
    "        #List of all cerebral artery templates from the reports\n",
    "        self.report_templates = self.extract_from_reports(self.report_template_pattern)\n",
    "\n",
    "\n",
    "        self.impression_pattern = re.compile('IMPRESSION:[\\s\\S]*', re.I)\n",
    "        #List of all impressions from the reports\n",
    "        self.impressions = self.extract_from_reports(self.impression_pattern)\n",
    "        \n",
    "        #List of all CT Perfusion sections from reports\n",
    "        self.ctp_pattern = re.compile('CT Perfusion:[\\s\\S]*IMPRESSION', re.I)\n",
    "        self.ctp_sections = self.extract_from_reports(self.ctp_pattern)\n",
    "        \n",
    "        \n",
    "    def get_report_templates(self):\n",
    "        return self.report_templates\n",
    "    \n",
    "    def get_impressions(self):\n",
    "        return self.impressions\n",
    "\n",
    "    def get_ctps(self):\n",
    "        return self.ctp_sections\n",
    "    \n",
    "    def extract_from_reports(self,pattern):\n",
    "        report_extractions = []\n",
    "        for report in self.df_column:\n",
    "            search = pattern.search(report)\n",
    "            if search:\n",
    "                report_extractions.append(search.group())\n",
    "            else:\n",
    "                report_extractions.append(None)\n",
    "        return report_extractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left anterior circulation:\n",
      "L CCA: no occlusion or significant stenosis\n",
      "L carotid bifurcation: no occlusion or significant stenosis\n",
      "L ICA proximal: no occlusion. Moderate stenosis secondary to atherosclerotic plaque.\n",
      "L ICA distal: no occlusion or significant stenosis\n",
      "L ICA terminus: There is atherosclerosis of the supraclinoid left internal carotid artery with resultant near complete occlusion versus occlusion at the left internal carotid artery terminus.  This appears similar to the prior CT angiogram from --/--/----.  The left anterior cerebral artery and middle cerebral artery likely fill via the circle of Willis.\n",
      "L M1: no occlusion or significant stenosis. Mildly diminished caliber. \n",
      "L M2 branches: no occlusion or significant stenosis\n",
      "L A2: no occlusion or significant stenosis\n",
      "There are diminished distal vessels on the left within the parietal lobe at the site of prior infarct. \n",
      "\n",
      "Right anterior circulation:\n",
      "R CCA: no occlusion or significant stenosis\n",
      "R carotid bifurcation: no occlusion or significant stenosis\n",
      "R ICA proximal: no occlusion or significant stenosis. There is atherosclerosis with <50% stenosis. \n",
      "R ICA distal: There is atherosclerotic calcification within the right cavernous segment with resultant moderate stenosis.  \n",
      "R ICA terminus: no occlusion or significant stenosis\n",
      "R M1: no occlusion or significant stenosis\n",
      "R M2 branches: no occlusion or significant stenosis\n",
      "R A2: no occlusion or significant stenosis\n",
      " \n",
      "Posterior circulation:\n",
      "L Vertebral Artery: The left vertebral artery is diminutive and is not visualized within the majority of the cervical region.\n",
      "R Vertebral Artery: no occlusion or significant stenosis.  The right vertebral artery is dominant.\n",
      "Basilar Artery: no occlusion or significant stenosis\n",
      "L PCA: no occlusion or significant stenosis\n",
      "R PCA: no occlusion or significant stenosis\n",
      "\n",
      "No cerebral aneurysm is seen. There is no evidence for an arteriovenous malformation. There is no suspicious cervical lymphadenopathy. There is no significant cervical spondylosis. Limited views of the lung apices demonstrate no focal consolidation.  \n",
      "\n",
      "CT Perfusion:\n",
      "\n",
      "Estimated ischemic core volume (rCBF < 0.3): 0 mL\n",
      "Estimated hypoperfusion volume (Tmax > 6 sec): 84 mL\n",
      "\n",
      "\n",
      "IMPRESSION\n",
      "//////////////////////////////////////////////////\n",
      "IMPRESSION:\n",
      "1.  High-grade stenosis versus possible occlusion at the left internal carotid artery supraclinoid segment/terminus with evidence of lenticulostriate collateral vessels and filling of the left anterior cerebral artery and middle cerebral artery likely via the circle of Willis.  The left MCA M1 segment remains slightly smaller than the contralateral side.  Overall these findings appear similar to the --/--/---- study. These findings correspond to the large area of hypoperfusion abnormality identified on RAPID.\n",
      "\n",
      "2.  Hypoplastic left vertebral artery with nonvisualization throughout the cervical portion, likely related to chronic occlusion.\n",
      "\n",
      "3. Chronic left frontal and parietal infarcts with paucity of vessels within the distal MCA vessels at the site of chronic parietal infarct. \n",
      "\n",
      "\n",
      "The findings were discussed with Dr. Harrison by Dr. Eric J. Fischer at the CT scanner on 07/21/2019 at approximately 4:00 AM.  Subsequent findings regarding the high-grade stenosis versus possible occlusion at the left ICA terminus were also discussed by Dr. Jayson Lavie with Dr. Harrison on 07/21/2019 at 4:24 AM.\n",
      "\n",
      "\n",
      "**\n",
      "\n",
      "Dictated by: Eric John Fischer, M.D.\n",
      "\n",
      "The radiology attending physician has personally reviewed this study, and had reviewed and/or edited this written report and agrees with it.\n",
      "\n",
      "Electronically signed by: Rami W Eldaya, M.D.\n",
      "//////////////////////////////////////////////////\n"
     ]
    }
   ],
   "source": [
    "# rte = report_text_extractor(filtered_df[\"CTA CTP Report Text\"])\n",
    "# templates = rte.get_report_templates()\n",
    "# impressions = rte.get_impressions()\n",
    "# ctps = rte.get_ctps()\n",
    "# print(templates[163])\n",
    "# print('/'*50)\n",
    "# print(impressions[163])\n",
    "# print('/'*50)\n",
    "#Extract ischemic core volume and hypoperfusion values in mLprint(ctps[522])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract ischemic core and hypoperfusion volumes in mL\n",
    "class ctp_volumes_extractor:\n",
    "    def __init__(self,ctp_sections_list):\n",
    "        self.ctps = ctp_sections_list\n",
    "        self.volume_pattern = re.compile('[0-9]* mL')\n",
    "        self.volumes = []\n",
    "        \n",
    "    def extract_volumes(self):\n",
    "        for ctp in self.ctps:\n",
    "            if ctp:\n",
    "                #List of 2 strings ex. ['0 mL', '146 mL']\n",
    "                search = self.volume_pattern.findall(ctp)\n",
    "                if search:                      \n",
    "                    #['0 mL', '146 mL'] -> (0, 146)\n",
    "                    vol_tup = tuple([int(vol_text.split()[0]) if len(vol_text.split()) > 1 else None\n",
    "                                     for vol_text in search])\n",
    "                    self.volumes.append(vol_tup)\n",
    "                else:\n",
    "                    self.volumes.append(None)\n",
    "            else:\n",
    "                self.volumes.append(None)\n",
    "\n",
    "    #List of tuples of format (ischemic core volume,hypoperfusion volume)\n",
    "    def get_volumes(self):\n",
    "        return self.volumes\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(177, 320)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctp_str = '''\n",
    "CT Perfusion:\n",
    "\n",
    "Estimated ischemic core volume (rCBF < 0.3): 177 mL\n",
    "Estimated hypoperfusion volume (Tmax > 6 sec): 320 mL\n",
    "\n",
    "\n",
    "IMPRESSION:'''\n",
    "test_ctp = [ctp_str]\n",
    "ctpve = ctp_volumes_extractor(test_ctp)\n",
    "ctpve.extract_volumes()\n",
    "ctpve.get_volumes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMINATION: Computed tomography angiography (CTA) of the head without and with contrast; Computed tomography angiography (CTA) of the neck with contrast._x000D_\n",
      "CT perfusion imaging of the head with contrast_x000D_\n",
      "_x000D_\n",
      "HISTORY: Left-sided weakness._x000D_\n",
      "_x000D_\n",
      "TECHNIQUE: Computed tomography of the head was performed without contrast according to standard protocol. Computed tomographic angiography was obtained from the level of the aortic arch to the vertex following the uneventful administration of intravenous contrast according to hyperacute stroke protocol. 3D images were generated on a dedicated workstation._x000D_\n",
      "CT perfusion of the brain was performed with intravenous contrast using a separate data acquisition. The data was transmitted to a separate workstation for processing by RAPID software (iSchemaView) to produce automated calculations of the estimated cerebral blood flow and Tmax._x000D_\n",
      "_x000D_\n",
      "Contrast information:_x000D_\n",
      "125 mL Optiray-350_x000D_\n",
      "_x000D_\n",
      "COMPARISON: 04/01/2019._x000D_\n",
      "_x000D_\n",
      "FINDINGS:_x000D_\n",
      "_x000D_\n",
      "HEAD CT FINDINGS:_x000D_\n",
      "_x000D_\n",
      "There is no acute intracranial hemorrhage.  A small chronic right paramedian parietal infarct is unchanged.  There is no noncontrast evidence of acute stroke. There is no vascular hyperdensity of the M1 segments or basilar artery. There are no periventricular and deep white matter hypodensities. There are no lacunar infarcts. Cerebral volume is typical for age. Ventricles are of normal size and morphology. There is no mass effect or midline shift._x000D_\n",
      "_x000D_\n",
      "ANGIOGRAPHIC FINDINGS:_x000D_\n",
      "_x000D_\n",
      "The visualized aortic arch appears normal with normal configuration of the great vessels. There is no significant stenosis of the origins of the great vessels._x000D_\n",
      "_x000D_\n",
      "There is no geographic area of vascular paucity in the brain._x000D_\n",
      "_x000D_\n",
      "Left anterior circulation:_x000D_\n",
      "L CCA: no occlusion or significant stenosis_x000D_\n",
      "L carotid bifurcation: no occlusion or significant stenosis_x000D_\n",
      "L ICA proximal: There is moderate stenosis of the left internal carotid artery immediately after the bifurcation._x000D_\n",
      "L ICA distal: There is mild atherosclerosis and narrowing of the left cavernous internal carotid artery_x000D_\n",
      "L ICA terminus: no occlusion or significant stenosis_x000D_\n",
      "L M1: no occlusion or significant stenosis_x000D_\n",
      "L M2 branches: no occlusion or significant stenosis_x000D_\n",
      "L A2: no occlusion or significant stenosis_x000D_\n",
      "_x000D_\n",
      "Right anterior circulation:_x000D_\n",
      "R CCA: no occlusion or significant stenosis_x000D_\n",
      "R carotid bifurcation: no occlusion or significant stenosis_x000D_\n",
      "R ICA proximal: no occlusion or significant stenosis_x000D_\n",
      "R ICA distal: There is mild atherosclerosis and narrowing within the right cavernous carotid artery_x000D_\n",
      "R ICA terminus: no occlusion or significant stenosis_x000D_\n",
      "R M1: And occlusion of the right distal M1 artery is present with reconstitution of the vessels distally._x000D_\n",
      "R M2 branches: no occlusion or significant stenosis_x000D_\n",
      "R A2: no occlusion or significant stenosis_x000D_\n",
      " _x000D_\n",
      "Posterior circulation:_x000D_\n",
      "L Vertebral Artery: Dominant left vertebral artery without occlusion or stenosis_x000D_\n",
      "R Vertebral Artery: Diminutive right vertebral artery_x000D_\n",
      "Basilar Artery: no occlusion or significant stenosis_x000D_\n",
      "L PCA: no occlusion or significant stenosis_x000D_\n",
      "R PCA: no occlusion or significant stenosis_x000D_\n",
      "_x000D_\n",
      "No cerebral aneurysm is seen. There is no evidence for an arteriovenous malformation. There is no suspicious cervical lymphadenopathy. There is no significant cervical spondylosis.  A cavitary lesion within the right lung apex is present measuring approximately 1.5 cm and new compared to 03/07/2019._x000D_\n",
      "_x000D_\n",
      "CT Perfusion:_x000D_\n",
      "_x000D_\n",
      "Estimated ischemic core volume (rCBF < 0.3): 0 mL_x000D_\n",
      "Estimated hypoperfusion volume (Tmax > 6 sec): 8 mL_x000D_\n",
      "_x000D_\n",
      "_x000D_\n",
      "IMPRESSION:_x000D_\n",
      "1.\tDistal right M1 MCA occlusion with distal reconstitution._x000D_\n",
      "2.\tModerate left proximal internal carotid artery stenosis._x000D_\n",
      "3.\tIncidental right lung apex cavitary lesion, new since March 2018.  Further evaluation with chest CT is recommended._x000D_\n",
      "_x000D_\n",
      "These findings were communicated with Dr. Tang by Dr. Shelton at the time of the scan.  These were also discussed with Dr. Liu by Dr. Shelton on 07/12/2019 at 7:58 PM._x000D_\n",
      "_x000D_\n",
      "Dictated by: Peter Quinn Shelton, M.D._x000D_\n",
      "_x000D_\n",
      "The radiology attending physician has personally reviewed this study, and had reviewed and/or edited this written report and agrees with it._x000D_\n",
      "_x000D_\n",
      "Electronically signed by: Aseem Sharma, M.D.\n"
     ]
    }
   ],
   "source": [
    "# print(filtered_df[\"CTA CTP Report Text\"].iloc[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L CCA': 0,\n",
       " 'L carotid bifurcation': None,\n",
       " 'L ICA proximal': 0,\n",
       " 'L ICA distal': 0,\n",
       " 'L ICA terminus': 0,\n",
       " 'L M1': 0,\n",
       " 'L M2 branches': 0,\n",
       " 'L A2': 0,\n",
       " 'R CCA': 0,\n",
       " 'R carotid bifurcation': None,\n",
       " 'R ICA proximal': None,\n",
       " 'R ICA distal': None,\n",
       " 'R ICA terminus': None,\n",
       " 'R M1': None,\n",
       " 'R M2 branches': None,\n",
       " 'R A2': 0,\n",
       " 'L Vertebral Artery': 0,\n",
       " 'R Vertebral Artery': 0,\n",
       " 'Basilar Artery': 0,\n",
       " 'L PCA': 0,\n",
       " 'R PCA': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ctr.read_ca_template(templates[8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L CCA': 0,\n",
       " 'L carotid bifurcation': 0,\n",
       " 'L ICA proximal': 0,\n",
       " 'L ICA distal': 0,\n",
       " 'L ICA terminus': 0,\n",
       " 'L M1': 0,\n",
       " 'L M2 branches': 0,\n",
       " 'L A2': 0,\n",
       " 'R CCA': 0,\n",
       " 'R carotid bifurcation': 0,\n",
       " 'R ICA proximal': 0,\n",
       " 'R ICA distal': 0,\n",
       " 'R ICA terminus': 0,\n",
       " 'R M1': 0,\n",
       " 'R M2 branches': 0,\n",
       " 'R A2': 0,\n",
       " 'L Vertebral Artery': None,\n",
       " 'R Vertebral Artery': None,\n",
       " 'Basilar Artery': 0,\n",
       " 'L PCA': 0,\n",
       " 'R PCA': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ca_templates_dicts = ctr.read_templates()\n",
    "# ca_templates_dicts[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For empty values in the ca_dicts, need to check impression\n",
    "#NLP needs to be used here ex.'Occlusion of the proximal inferior left M2 branch'\n",
    "#NLP needed on midline shift ex. There is no mass effect or midline shift.\n",
    "import networkx as nx\n",
    "import pyConTextNLP.pyConText as pyConText\n",
    "import pyConTextNLP.itemData as itemData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/pyConTextNLP/itemData.py:40: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  context_items =  [contextItem((d[\"Lex\"],\n"
     ]
    }
   ],
   "source": [
    "modifiers = itemData.get_items(\"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/modifiers.yml\")\n",
    "targets = itemData.get_items(\"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/targets.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyConTextNLP uses yml files to store entities and their regex pattern information\n",
    "#This function is to be used for quick lookup of an entity for short term use\n",
    "#Or to add new entities to a yml file\n",
    "\n",
    "#\"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/modifiers.yml\"\n",
    "#\"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/targets.yml\"\n",
    "\n",
    "class pyConText_itemData_initializer:\n",
    "    \n",
    "    def __init__(self, modifiers_path=None, targets_path=None):\n",
    "        if modifiers_path is None:\n",
    "            self.modifiers_path = \"https://raw.githubusercontent.com/chapmanbe/pyConTextNLP/master/KB/lexical_kb_05042016.yml\"\n",
    "        else:\n",
    "            self.modifiers_path = modifiers_path\n",
    "            \n",
    "        self.modifiers = itemData.get_items(self.modifiers_path)\n",
    "        self.targets = []\n",
    "            \n",
    "        if targets_path is not None:\n",
    "            self.targets_path = targets_path\n",
    "            self.targets = itemData.get_items(self.targets_path)\n",
    "    \n",
    "    def add_target(self,entity,category='', regex='', direction=''):\n",
    "        self.targets.append(itemData.contextItem([entity,category,regex,direction]))\n",
    "        \n",
    "        #Code from pyConTextNLP itemData.py\n",
    "        #def get_items(_file): context_items =  [contextItem((d[\"Lex\"], d[\"Type\"],r\"%s\"%d[\"Regex\"],d[\"Direction\"])) for d in yaml.load_all(f0)]\n",
    "    \n",
    "    def add_entity_to_yml(self, yml_path, entity, category=\"''\", regex=\"''\", direction=\"''\"):\n",
    "        yml_entity = \"Direction: \"+ direction + \"\\n\" + \\\n",
    "        \"Lex: \"+ entity + \"\\n\" + \\\n",
    "        \"Regex: \"+ regex + \"\\n\" + \\\n",
    "        \"Type: \" + category + \"\\n\" + \\\n",
    "        \"---\\n\" \n",
    "        \n",
    "        with open(yml_path, \"r+\") as f:\n",
    "            old = f.read()\n",
    "            f.seek(0)\n",
    "            #Prepending\n",
    "            f.write(yml_entity + old)\n",
    "            \n",
    "        f.close()\n",
    "        \n",
    "    def get_itemData(self):\n",
    "        return self.modifiers,self.targets\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "literal<<\\bl\\b>>; category<<['left_side']>>; re<<\\bl\\b>>; rule<<forward>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_path = \"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/modifiers.yml\"\n",
    "targ_path = \"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/targets.yml\"\n",
    "itemData_init = pyConText_itemData_initializer(mod_path,targ_path)\n",
    "itemData_init.modifiers[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyConTextNLP.itemData.contextItem"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(itemData_init.targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "literal<<occlusion>>; category<<['occlusion']>>; re<<(occlusion|occlusive|occluded)>>; rule<<bidirectional>>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# itemData_init.add_target('occlusion')\n",
    "itemData_init.targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itemData_init.add_entity_to_yml(\"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/targets.yml\",'P1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to determine existence or rule out given entity\n",
    "#Ex. Midline shift/No midline shift\n",
    "#Ex. Occlusion/No occlusion\n",
    "\n",
    "#Input is string entity that you want to rule in/out\n",
    "#***Since words have DIFFERENT forms, consider using a PREFIX INSTEAD\n",
    "#Ex. Entity: occlusion; sentence contains 'occluded thrombus'\n",
    "# Using the prefix 'occlu' would capture both in a rudimentary fashion\n",
    "\n",
    "class modifier_target_finder:\n",
    "    def __init__(self, sentence, mod_itemData, targ_itemData):\n",
    "        self.modifiers = mod_itemData\n",
    "        self.targets = targ_itemData\n",
    "        self.sentence = sentence\n",
    "        \n",
    "        self.markup = pyConText.ConTextMarkup()\n",
    "        self.markup.setRawText(sentence.lower())\n",
    "        self.markup.cleanText()\n",
    "        \n",
    "        self.markup.markItems(self.modifiers,mode=\"modifier\")\n",
    "        self.markup.markItems(self.targets,mode=\"target\")\n",
    "        self.markup.applyModifiers()\n",
    "    \n",
    "    #New addition\n",
    "    def log(self):\n",
    "        print(self.markup.__str__())\n",
    "        \n",
    "    \n",
    "    def get_markup(self):\n",
    "        return list(self.markup.edges())\n",
    "            \n",
    "    def find_mod_target_pair(self,desired_modifier,modifier_attrib,desired_target,target_attrib):\n",
    "        #set removes duplicates\n",
    "        return list(set([(modifier.getLiteral(),target.getLiteral()) \n",
    "                for (modifier,target) in self.markup.edges() \n",
    "                # getattr() will run modifier/target . getLiteral()/categoryString()/getTagID()/getConTextCategory()\n",
    "                if (desired_modifier in getattr(modifier,modifier_attrib)() \n",
    "                    and desired_target in getattr(target,target_attrib)())]))\n",
    "    \n",
    "    \n",
    "    #Returns 0 if negated\n",
    "    #Returns 1 if present and not negated\n",
    "    #Returns 2 if confirmed existence\n",
    "    #Returns None if not in sentence\n",
    "    def confirm_existence(self, entity, entity_type):\n",
    "        entities = []\n",
    "        for modifier,target in self.markup.edges():\n",
    "            etype_func = {\"getLiteral\":[modifier.getLiteral,target.getLiteral],\n",
    "                          \"categoryString\":[modifier.categoryString,target.categoryString]}\n",
    "            for func in etype_func[entity_type]:\n",
    "                entities.append(func())\n",
    "        if entity in entities:\n",
    "            neg_pairs_list = self.find_mod_target_pair('negated_existence','categoryString',entity,entity_type)\n",
    "            if neg_pairs_list:\n",
    "                return 0\n",
    "            def_exis_pairs_list = self.find_mod_target_pair(('definite_existence'),\n",
    "                                                    'categoryString', entity,entity_type)\n",
    "            prob_exis_pairs_list = self.find_mod_target_pair(('definite_existence'),\n",
    "                                                            'categoryString', entity,entity_type)\n",
    "            if def_exis_pairs_list or prob_exis_pairs_list:\n",
    "                return 2\n",
    "            return 1\n",
    "        else:\n",
    "            return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<id> 110817854851893856320707927580509592686 </id> <phrase> left </phrase> <category> ['left_side'] </category> ,\n",
       "  <id> 110842944826398873551137051078701997166 </id> <phrase> frontal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110817854851893856320707927580509592686 </id> <phrase> left </phrase> <category> ['left_side'] </category> ,\n",
       "  <id> 110842955918341625548144314174855044206 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110817854851893856320707927580509592686 </id> <phrase> left </phrase> <category> ['left_side'] </category> ,\n",
       "  <id> 110842791123763595878322119603438345326 </id> <phrase> mca </phrase> <category> ['mca_circulation'] </category> ),\n",
       " (<id> 110817854851893856320707927580509592686 </id> <phrase> left </phrase> <category> ['left_side'] </category> ,\n",
       "  <id> 110842968594847627830438329141887097966 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110817854851893856320707927580509592686 </id> <phrase> left </phrase> <category> ['left_side'] </category> ,\n",
       "  <id> 110842664358703573055381969933117807726 </id> <phrase> infarct </phrase> <category> ['infarct'] </category> ),\n",
       " (<id> 110818387265145952177056556195855850606 </id> <phrase> mca </phrase> <category> ['mca_circulation'] </category> ,\n",
       "  <id> 110842944826398873551137051078701997166 </id> <phrase> frontal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110818387265145952177056556195855850606 </id> <phrase> mca </phrase> <category> ['mca_circulation'] </category> ,\n",
       "  <id> 110842955918341625548144314174855044206 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110818387265145952177056556195855850606 </id> <phrase> mca </phrase> <category> ['mca_circulation'] </category> ,\n",
       "  <id> 110842791123763595878322119603438345326 </id> <phrase> mca </phrase> <category> ['mca_circulation'] </category> ),\n",
       " (<id> 110818387265145952177056556195855850606 </id> <phrase> mca </phrase> <category> ['mca_circulation'] </category> ,\n",
       "  <id> 110842968594847627830438329141887097966 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110818387265145952177056556195855850606 </id> <phrase> mca </phrase> <category> ['mca_circulation'] </category> ,\n",
       "  <id> 110842664358703573055381969933117807726 </id> <phrase> infarct </phrase> <category> ['infarct'] </category> ),\n",
       " (<id> 110820326770564301368040846151760075886 </id> <phrase> frontal </phrase> <category> ['brain_anatomy'] </category> ,\n",
       "  <id> 110842944826398873551137051078701997166 </id> <phrase> frontal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110820326770564301368040846151760075886 </id> <phrase> frontal </phrase> <category> ['brain_anatomy'] </category> ,\n",
       "  <id> 110842955918341625548144314174855044206 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110820341823915179078264988925110639726 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ,\n",
       "  <id> 110842944826398873551137051078701997166 </id> <phrase> frontal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110820341823915179078264988925110639726 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ,\n",
       "  <id> 110842955918341625548144314174855044206 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110820341823915179078264988925110639726 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ,\n",
       "  <id> 110842791123763595878322119603438345326 </id> <phrase> mca </phrase> <category> ['mca_circulation'] </category> ),\n",
       " (<id> 110820341823915179078264988925110639726 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ,\n",
       "  <id> 110842968594847627830438329141887097966 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110820353708139556217915627956703190126 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ,\n",
       "  <id> 110842944826398873551137051078701997166 </id> <phrase> frontal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110820353708139556217915627956703190126 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ,\n",
       "  <id> 110842955918341625548144314174855044206 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110820353708139556217915627956703190126 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ,\n",
       "  <id> 110842791123763595878322119603438345326 </id> <phrase> mca </phrase> <category> ['mca_circulation'] </category> ),\n",
       " (<id> 110820353708139556217915627956703190126 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ,\n",
       "  <id> 110842968594847627830438329141887097966 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110820353708139556217915627956703190126 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ,\n",
       "  <id> 110842664358703573055381969933117807726 </id> <phrase> infarct </phrase> <category> ['infarct'] </category> ),\n",
       " (<id> 110823964135505331243779765754520001646 </id> <phrase> chronic </phrase> <category> ['historical'] </category> ,\n",
       "  <id> 110842944826398873551137051078701997166 </id> <phrase> frontal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110823964135505331243779765754520001646 </id> <phrase> chronic </phrase> <category> ['historical'] </category> ,\n",
       "  <id> 110842955918341625548144314174855044206 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110823964135505331243779765754520001646 </id> <phrase> chronic </phrase> <category> ['historical'] </category> ,\n",
       "  <id> 110842791123763595878322119603438345326 </id> <phrase> mca </phrase> <category> ['mca_circulation'] </category> ),\n",
       " (<id> 110823986319390835237794291946826095726 </id> <phrase> chronic </phrase> <category> ['historical'] </category> ,\n",
       "  <id> 110842944826398873551137051078701997166 </id> <phrase> frontal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110823986319390835237794291946826095726 </id> <phrase> chronic </phrase> <category> ['historical'] </category> ,\n",
       "  <id> 110842955918341625548144314174855044206 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110823986319390835237794291946826095726 </id> <phrase> chronic </phrase> <category> ['historical'] </category> ,\n",
       "  <id> 110842791123763595878322119603438345326 </id> <phrase> mca </phrase> <category> ['mca_circulation'] </category> ),\n",
       " (<id> 110823986319390835237794291946826095726 </id> <phrase> chronic </phrase> <category> ['historical'] </category> ,\n",
       "  <id> 110842968594847627830438329141887097966 </id> <phrase> parietal </phrase> <category> ['brain_anatomy'] </category> ),\n",
       " (<id> 110823986319390835237794291946826095726 </id> <phrase> chronic </phrase> <category> ['historical'] </category> ,\n",
       "  <id> 110842664358703573055381969933117807726 </id> <phrase> infarct </phrase> <category> ['infarct'] </category> )]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_str = \"Chronic left frontal and parietal infarcts with paucity of vessels within the distal MCA vessels at the site of chronic parietal infarct. \"\n",
    "# mtf = modifier_target_finder(test_str,\n",
    "#                              itemData_init.get_itemData()[0],itemData_init.get_itemData()[1])\n",
    "# mtf.get_markup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if mtf.find_mod_target_pair(\"negated_existence\",\"categoryString\",\"uncertain_flow\",\"categoryString\"):\n",
    "#     print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtf.confirm_existence(\"uncertain_occlusion\",\"categoryString\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract midline shift yes/no\n",
    "#Searches entire report for midline shift sentence,\n",
    "#as it is discussed at beginning of Findings section\n",
    "#exclusively\n",
    "class midline_shift_extractor:\n",
    "    def __init__(self,df_column,modifiers,targets):\n",
    "        self.df_column = df_column\n",
    "        self.modifiers = modifiers\n",
    "        self.targets = targets\n",
    "        self.midline_shift_pattern = re.compile('([^.]*?midline shift[^.]*\\.)')\n",
    "        self.midline_shift_statuses = []\n",
    "        \n",
    "    def extract_midline_shifts(self):\n",
    "        for report in self.df_column:\n",
    "            search = self.midline_shift_pattern.search(report)\n",
    "            if search:\n",
    "                sent = search.group()\n",
    "                mtf = modifier_target_finder(sent,self.modifiers,self.targets)\n",
    "                status = mtf.confirm_existence(\"midline shift\",\"getLiteral\")\n",
    "                if status == 0:\n",
    "                    self.midline_shift_statuses.append(0)\n",
    "                elif status > 0:\n",
    "                    self.midline_shift_statuses.append(1)\n",
    "                else:\n",
    "                    self.midline_shift_statuses.append(None)\n",
    "    \n",
    "    def get_midline_shifts(self):\n",
    "        return self.midline_shift_statuses\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse = midline_shift_extractor(filtered_df[\"CTA CTP Report Text\"],itemData_init.get_itemData()[0],itemData_init.get_itemData()[1])\n",
    "# mse.extract_midline_shifts()\n",
    "# midline_shifts = mse.get_midline_shifts()\n",
    "# midline_shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i, value in enumerate(midline_shifts) if value == 1] \n",
    "#Only 2 out of 500 have midline shift, will not run in final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df['CTA CTP Report Text'][211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ca = cerebral arteries\n",
    "#expects ca_templates_reader dictionary as input\n",
    "#which acts as a filter.\n",
    "#For sites not ruled out (value=None), read the impression\n",
    "#for further clarification\n",
    "\n",
    "class possible_occlusions_generator:    \n",
    "    def __init__(self, ca_filter_dict):\n",
    "        \n",
    "        self.new_dict_keys = [\"L CCA\", \"L ICA\", \"L ACA\", \"L A1\", \"L A2\", \"L A3\", \"L A4\", \"L A5\",\n",
    "                        \"L MCA\", \"L M1\", \"L M2\", \"L M3\", \"L M4\",\n",
    "                        \"R CCA\", \"R ICA\", \"R ACA\", \"R A1\", \"R A2\", \"R A3\", \"R A4\", \"R A5\",\n",
    "                        \"R MCA\", \"R M1\", \"R M2\", \"R M3\", \"R M4\",\n",
    "                        \"L vertebral\", \"L V1\", \"L V2\", \"L V3\", \"L V4\",\n",
    "                        \"R vertebral\", \"R V1\", \"R V2\", \"R V3\", \"R V4\",\n",
    "                        \"basilar\",\n",
    "                        \"L PCA\", \"L P1\", \"L P2\", \"L P3\", \"L P4\",\n",
    "                        \"R PCA\", \"R P1\", \"R P2\", \"R P3\", \"R P4\"]\n",
    "    \n",
    "        \n",
    "        self.filter_dict = self.consolidate_dict(ca_filter_dict)\n",
    "        self.filled_dict = self.fill_in_dict(self.filter_dict)\n",
    "        self.ca_to_investigate = [key for key,value in self.filled_dict.items() if value < 0]\n",
    "        \n",
    "    def negate_nones(self,dictionary):\n",
    "        negated_dict = {}\n",
    "        for key,value in dictionary.items():\n",
    "            if value is None:\n",
    "                negated_dict[key] = -1\n",
    "            else:\n",
    "                negated_dict[key] = value\n",
    "        return negated_dict\n",
    "            \n",
    "    def consolidate_keys_for_dict(self,dictionary,list_of_keys,consol_key):\n",
    "        new_dict = {}\n",
    "        consolidated_value = 0\n",
    "        found_key = 0\n",
    "        for key,value in dictionary.items():\n",
    "            if found_key == 1:\n",
    "                #Add consol_key to new_dict at spot of first key in list_of_keys \n",
    "                new_dict[consol_key] = consolidated_value\n",
    "            if key in list_of_keys:\n",
    "                found_key+=1                \n",
    "                #Value is negative, indicating further investigation\n",
    "                consolidated_value += value                \n",
    "            else:\n",
    "                new_dict[key] = value\n",
    "        new_dict[consol_key] = consolidated_value\n",
    "        return new_dict\n",
    "    \n",
    "    #Calls consolidate_keys_for_dict to consolidate CCAs and ICAs\n",
    "    def consolidate_dict(self,filter_dict):\n",
    "        consol_dict = self.negate_nones(filter_dict)\n",
    "        keys_dict = {\n",
    "            \"L CCA\": [\"L CCA\", \"L carotid bifurcation\"],\n",
    "            \"R CCA\": [\"R CCA\", \"R carotid bifurcation\"],\n",
    "            \"L ICA\": [\"L ICA proximal\",\"L ICA distal\",\"L ICA terminus\"],\n",
    "            \"R ICA\": [\"R ICA proximal\",\"R ICA distal\",\"R ICA terminus\"],\n",
    "            \"L M2\": [\"L M2 branches\"],\n",
    "            \"R M2\": [\"R M2 branches\"],\n",
    "            \"R vertebral\": [\"R Vertebral Artery\"],\n",
    "            \"L vertebral\": [\"L Vertebral Artery\"],\n",
    "            \"basilar\": [\"Basilar Artery\"]\n",
    "        }\n",
    "        for consol_key,list_of_keys in keys_dict.items():\n",
    "            consol_dict = self.consolidate_keys_for_dict(consol_dict,list_of_keys,consol_key)\n",
    "        \n",
    "        return consol_dict\n",
    "    \n",
    "    \n",
    "    #Adds AX, MX, PX, and VX segments, where X is segment location (ex. V4, X=4)\n",
    "    def fill_in_dict(self,consol_dict):\n",
    "        \n",
    "        new_dict = {}\n",
    "        \n",
    "        #Populate keys-vals from previous consolidated dictionary\n",
    "        for key in self.new_dict_keys:\n",
    "            if key in consol_dict:\n",
    "                new_dict[key] = consol_dict[key]\n",
    "            else:\n",
    "                #Temp value that will be filled by transfer_key_values\n",
    "                new_dict[key] = None\n",
    "        \n",
    "        #The template serves as a reference for which cerebral arteries to look at\n",
    "        #This function transfers the values from the template to the corresponding\n",
    "        #AX, MX, PX, VX arteries\n",
    "        #Ex. L Vertebral Artery Occlusion indicates occlusion in one/more of L V1-4\n",
    "        def transfer_key_values(source_key,list_receiving):\n",
    "            for key in list_receiving:\n",
    "                if key == \"L MCA\":\n",
    "                    new_dict[key] = consol_dict[\"L M1\"]+consol_dict[\"L M2\"]\n",
    "                elif key == \"R MCA\":\n",
    "                    new_dict[key] = consol_dict[\"R M1\"]+consol_dict[\"R M2\"]\n",
    "                else:\n",
    "                    new_dict[key] = consol_dict[source_key]\n",
    "        refs = {\n",
    "            \"L M1/M2\": [\"L MCA\"],\n",
    "            \"R M1/M2\": [\"R MCA\"],\n",
    "            \"L M2\": [\"L M3\", \"L M4\"],\n",
    "            \"R M2\": [\"R M3\", \"R M4\"],\n",
    "            \"L A2\": [\"L ACA\",\"L A1\", \"L A3\", \"L A4\", \"L A5\"],\n",
    "            \"R A2\": [\"R ACA\",\"R A1\", \"R A3\", \"R A4\", \"R A5\"],\n",
    "            \"L vertebral\": [\"L V1\", \"L V2\", \"L V3\", \"L V4\"],\n",
    "            \"R vertebral\": [\"R V1\", \"R V2\", \"R V3\", \"R V4\"],\n",
    "            \"L PCA\": [\"L P1\", \"L P2\", \"L P3\", \"L P4\"],\n",
    "            \"R PCA\": [\"R P1\", \"R P2\", \"R P3\", \"R P4\"]\n",
    "        }\n",
    "        \n",
    "        for source_key,list_receiving in refs.items():\n",
    "            transfer_key_values(source_key,list_receiving)\n",
    "            \n",
    "        return new_dict\n",
    "    \n",
    "    def get_possibilities(self):\n",
    "        return self.ca_to_investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "class occlusions_investigator:\n",
    "    \n",
    "    def __init__(self,ca_to_investigate,impressions_txt,modData,targData):\n",
    "        self.modData = modData\n",
    "        self.targData = targData\n",
    "        self.impressions_txt = impressions_txt\n",
    "        self.ca_to_investigate = ca_to_investigate\n",
    "        \n",
    "        self.extra_chars = re.compile('IMPRESSION:|\\n[0-9]\\.|\\r|\\n|\\t')\n",
    "        self.impressions = re.sub(self.extra_chars,'',self.impressions_txt)\n",
    "        self.txt_sents = self.impressions.split(\".\")\n",
    "        #List of 0,1,2 or Nones based on negated occ, occ mentioned, occ confirmed, or no occ mentioned\n",
    "        self.sent_has_occlusion = [self.confirm_occlusion(sent) for sent in self.txt_sents]\n",
    "        self.sent_maybe_occlusion = [self.confirm_uncertain_occlusion(sent) for sent in self.txt_sents]\n",
    "        self.occlusions = []\n",
    "        self.bool_none = lambda x: False if not x else True\n",
    "        self.filt_sent_has_occlusion = list(filter(self.bool_none,self.sent_has_occlusion))\n",
    "        self.filt_txt_sents = list(compress(self.txt_sents,map(self.bool_none,self.sent_has_occlusion)))\n",
    "        self.filt_maybe_sents = list(compress(self.txt_sents,map(self.bool_none,self.sent_maybe_occlusion)))\n",
    "        #0 for negation\n",
    "        #1 for occlusion present and not negated\n",
    "        #2 for occlusion indicated\n",
    "        #None for occlusion not found\n",
    "        self.occlusion_ratings = []\n",
    "        \n",
    "        #This list keeps track of occlusion sentences that have and have not\n",
    "        #been matched with an occlusion in the template\n",
    "        #Those unmatched with template will be flagged for the user to review\n",
    "        self.unmatched_occlusion_sents = [True for sent in self.filt_txt_sents]\n",
    "        \n",
    "        #Find if sentence refers to cerebral artery with indeterminate terms\n",
    "        #Will be flagged for user review\n",
    "        self.uncertain_occlusions = []\n",
    "        \n",
    "        self.markups = []\n",
    "\n",
    "    def get_occlusions(self):\n",
    "        \n",
    "        ca_to_investigate = []\n",
    "        for potential in self.ca_to_investigate:\n",
    "            side = None\n",
    "            ca = None\n",
    "            if \" \" in potential:\n",
    "                potential_list = potential.split(\" \")\n",
    "                side = potential_list[0]\n",
    "                ca = potential_list[1]\n",
    "            else:\n",
    "                ca = potential\n",
    "            \n",
    "            ca_to_investigate.append((potential,side,ca))\n",
    "        \n",
    "        #Check for uncertain occlusions\n",
    "        if self.sent_maybe_occlusion:\n",
    "            #print(self.sent_maybe_occlusion)\n",
    "            for sent in self.filt_maybe_sents:\n",
    "                for potential,side,ca in ca_to_investigate:\n",
    "                    lim = self.confirm_limitation(sent, ca)\n",
    "                    if lim:\n",
    "                        self.uncertain_occlusions.append(lim)\n",
    "        \n",
    "        #There is no occlusion in any sentence\n",
    "        #Return empty occlusions list\n",
    "        if not self.filt_sent_has_occlusion:\n",
    "            self.occlusion_ratings = [None for pos in self.ca_to_investigate]\n",
    "            return self.occlusions\n",
    "\n",
    "        #Investigate potenial occlusions by side and cerebral artery\n",
    "        for potential,side,ca in ca_to_investigate:\n",
    "            #for sent,occl_label in zip(self.filt_txt_sents,self.filt_sent_has_occlusion):\n",
    "            for i in range(len(self.filt_txt_sents)):\n",
    "                sent = self.filt_txt_sents[i]\n",
    "                occl_label = self.filt_sent_has_occlusion[i]\n",
    "                #If site/side of occlusion verified\n",
    "                #Add the potential occlusion to the final occlusions list\n",
    "                is_occl, flag = self.confirm_site_occlusion(sent,side,ca)\n",
    "                #Excludes both None and 0, runs on 1 or 2\n",
    "                if is_occl:\n",
    "                    self.unmatched_occlusion_sents[i] = False\n",
    "                    if flag == -1:\n",
    "                        #side of the site could not be verified, flag -1\n",
    "                        #in occlusion_ratings list\n",
    "                        self.occlusion_ratings.append((sent,side,ca,flag))\n",
    "                        #Marked as non-occlusion for now\n",
    "                        occl_label = 0\n",
    "                    else:\n",
    "                        #Can be corellated to filt_sent_has_occlusion list with labels 0-2\n",
    "                        self.occlusion_ratings.append((sent,side,ca,occl_label))\n",
    "            \n",
    "                    if occl_label > 0:\n",
    "                        self.occlusions.append(potential)\n",
    "                else:\n",
    "                    #is_occl\n",
    "                    #None: Means potential occlusion not found in Findings\n",
    "                    #0: Occlusion negated\n",
    "                    self.occlusion_ratings.append((sent,side,ca,None))\n",
    "                        \n",
    "            \n",
    "        self.unmatched_occlusion_sents = list(compress(self.filt_txt_sents,self.unmatched_occlusion_sents))\n",
    "        \n",
    "\n",
    "        #If other side has no findings and occlusion of that artery present\n",
    "        #Then add to occlusions list\n",
    "        def check_other_side_occlusion(side,ca,occl_label):\n",
    "            \n",
    "            vs_dict = {\"ACA\": [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\"],\n",
    "                       \"MCA\": [\"M1\",\"M2\",\"M3\",\"M4\"],\n",
    "                       \"PCA\": [\"P1\",\"P2\",\"P3\",\"P4\"],\n",
    "                       \"vertebral\": [\"V1\",\"V2\",\"V3\",\"V4\"]\n",
    "                      }\n",
    "            \n",
    "            other_side = None\n",
    "            if side == \"L\":\n",
    "                other_side = \"R\"\n",
    "            elif side == \"R\":\n",
    "                other_side = \"L\"\n",
    "            for o_sent, o_side, o_ca, o_occl_label in self.occlusion_ratings:\n",
    "                #Most cases will be a list of one element\n",
    "                #Ex. M2\n",
    "                ca_list = [ca]\n",
    "                #Need to check other side segment equivalents\n",
    "                #If general large vessel a possible occlusion\n",
    "                #Ex. L MCA -> Check R M1, R M2, R M3, R M4\n",
    "                if ca in vs_dict:\n",
    "                    for c in vs_dict[ca]:\n",
    "                        ca_list.append(c)\n",
    "                for ca_equiv in ca_list:\n",
    "                    #Check if other side, same cerebral artery and occlusion label is 0,1,2\n",
    "                    #If so this occlusion is not likely\n",
    "                    #Otherwise, add this occlusion\n",
    "                    if o_occl_label is not None:\n",
    "                        if o_side == other_side and o_ca == ca_equiv and o_occl_label > -1:\n",
    "                            return\n",
    "            self.occlusions.append(side + \" \" + ca)\n",
    "\n",
    "        #Check flagged occlusions where side and cerebral artery didn't match\n",
    "        for sent,side,ca,occl_label in self.occlusion_ratings:\n",
    "            if occl_label == -1:             \n",
    "                check_other_side_occlusion(side,ca,occl_label)\n",
    "                \n",
    "\n",
    "        return self.occlusions\n",
    "    \n",
    "    def get_possibility_ratings(self):\n",
    "        return self.occlusion_ratings\n",
    "    \n",
    "    def get_markups(self):\n",
    "        #(index,sents with occlusion, markup of those sents)\n",
    "        return list(zip(self.filt_txt_sents,self.markups))\n",
    "    \n",
    "    def get_unmatched_occlusion_sents(self):\n",
    "        return self.unmatched_occlusion_sents\n",
    "    \n",
    "    def get_uncertain_occlusions(self):\n",
    "        return self.uncertain_occlusions\n",
    "\n",
    "\n",
    "    def confirm_occlusion(self,sent):\n",
    "        mtf = modifier_target_finder(sent,self.modData,self.targData)\n",
    "        #Looking for occlusion in acute context, not chronic occlusion\n",
    "        if not mtf.find_mod_target_pair('chronic','getLiteral','occlusion','getLiteral'):\n",
    "            #either 0, 1, 2, or None\n",
    "            return mtf.confirm_existence('occlusion','getLiteral')\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def confirm_uncertain_occlusion(self,sent):\n",
    "        mtf = modifier_target_finder(sent,self.modData,self.targData)\n",
    "        uo = mtf.confirm_existence('uncertain_occlusion','categoryString')\n",
    "        if uo:\n",
    "            return uo\n",
    "        uf = mtf.confirm_existence('uncertain_flow','categoryString')\n",
    "        if uf:\n",
    "            return uf\n",
    "        return None\n",
    "\n",
    "\n",
    "    def confirm_site_occlusion(self,sent,side,site):\n",
    "        mtf = modifier_target_finder(sent,self.modData,self.targData)\n",
    "        self.markups.append(mtf.get_markup())\n",
    "        full_side = side\n",
    "        if side == \"L\":\n",
    "            full_side = \"left_side\"\n",
    "        elif side == \"R\":\n",
    "            full_side = \"right_side\"\n",
    "\n",
    "        #Verify occlusion of specific cerebral artery\n",
    "        sites = mtf.find_mod_target_pair(site,'getLiteral','occlusion','getLiteral')\n",
    "        \n",
    "        #Verify side of specific cerebral artery\n",
    "        #None gets overwritten later, flag -1 important\n",
    "        if sites:\n",
    "            if side is None:\n",
    "                return (True,None)\n",
    "            sides_sites = mtf.find_mod_target_pair(full_side,'categoryString',site,'getLiteral')\n",
    "            if sides_sites:\n",
    "                return (True,None)\n",
    "            else:\n",
    "                #could not verify the side corresponds to site in findings\n",
    "                return (True,-1)\n",
    "        return (False,None)\n",
    "    \n",
    "    #Sentences with terms that may or may not suggest acute occlusion\n",
    "    #Are flagged for user review\n",
    "    def confirm_limitation(self, sent, site):\n",
    "        mtf = modifier_target_finder(sent,self.modData,self.targData)\n",
    "        if mtf.find_mod_target_pair('uncertain_occlusion','categoryString',site,'getLiteral'):\n",
    "            return ('uncertain occlusion',site,sent)\n",
    "        if mtf.find_mod_target_pair('uncertain_flow','categoryString',site,'getLiteral'):\n",
    "            if mtf.find_mod_target_pair('negated_existence','categoryString','uncertain_flow','categoryString'):\n",
    "                return ('uncertain negated flow',site,sent)\n",
    "        return None\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f435f468a00>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = ('t1','t2')\n",
    "p = ('t3')\n",
    "zip('t1','t2','t3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occlusions_list = filtered_edges(('anterior_circulation' or 'posterior_circulation'),'categoryString','occlusion','categoryString')\n",
    "# side_circ_list = []\n",
    "# #occlusions_list = [('M1', 'occlusion'), ('M2', 'occlusion')]\n",
    "# for (modifier,target) in occlusions_list:\n",
    "#     #side_circ_list = [('right', 'M1'), ('right', 'M2')]\n",
    "#     side_circ_list += filtered_edges('side','categoryString',modifier,'getLiteral')\n",
    "# for side,circ in side_circ_list:\n",
    "#     #list evaluates to [('no', 'occlusion')], or is empty\n",
    "#     no_occlusion = filtered_edges('negated_existence','categoryString','occlusion','categoryString')\n",
    "#     if no_occlusion:\n",
    "#         print(\"No occlusion at \"+ side + \" \" + circ)\n",
    "#     else:\n",
    "#         print(\"Occlusion at \"+ side + \" \" + circ)\n",
    "\n",
    "# # else:\n",
    "# #     for (modifier,target) in occlusions_list:\n",
    "# #             print(\"occlusion at \" + occlusions_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L CCA': 0,\n",
       " 'L carotid bifurcation': 0,\n",
       " 'L ICA proximal': 0,\n",
       " 'L ICA distal': 0,\n",
       " 'L ICA terminus': None,\n",
       " 'L M1': 0,\n",
       " 'L M2 branches': 0,\n",
       " 'L A2': 0,\n",
       " 'R CCA': 0,\n",
       " 'R carotid bifurcation': 0,\n",
       " 'R ICA proximal': 0,\n",
       " 'R ICA distal': None,\n",
       " 'R ICA terminus': 0,\n",
       " 'R M1': 0,\n",
       " 'R M2 branches': 0,\n",
       " 'R A2': 0,\n",
       " 'L Vertebral Artery': None,\n",
       " 'R Vertebral Artery': 0,\n",
       " 'Basilar Artery': 0,\n",
       " 'L PCA': 0,\n",
       " 'R PCA': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dict = ca_templates_dicts[163]\n",
    "# test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L CCA': 0,\n",
       " 'L ICA': -1,\n",
       " 'L M1': 0,\n",
       " 'L M2': 0,\n",
       " 'L A2': 0,\n",
       " 'R CCA': 0,\n",
       " 'R ICA': -1,\n",
       " 'R M1': 0,\n",
       " 'R M2': 0,\n",
       " 'R A2': 0,\n",
       " 'L vertebral': -1,\n",
       " 'R vertebral': 0,\n",
       " 'basilar': 0,\n",
       " 'L PCA': 0,\n",
       " 'R PCA': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pog = possible_occlusions_generator(test_dict)\n",
    "# pog.filter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L CCA': 0,\n",
       " 'L ICA': -1,\n",
       " 'L ACA': 0,\n",
       " 'L A1': 0,\n",
       " 'L A2': 0,\n",
       " 'L A3': 0,\n",
       " 'L A4': 0,\n",
       " 'L A5': 0,\n",
       " 'L MCA': 0,\n",
       " 'L M1': 0,\n",
       " 'L M2': 0,\n",
       " 'L M3': 0,\n",
       " 'L M4': 0,\n",
       " 'R CCA': 0,\n",
       " 'R ICA': -1,\n",
       " 'R ACA': 0,\n",
       " 'R A1': 0,\n",
       " 'R A2': 0,\n",
       " 'R A3': 0,\n",
       " 'R A4': 0,\n",
       " 'R A5': 0,\n",
       " 'R MCA': 0,\n",
       " 'R M1': 0,\n",
       " 'R M2': 0,\n",
       " 'R M3': 0,\n",
       " 'R M4': 0,\n",
       " 'L vertebral': -1,\n",
       " 'L V1': -1,\n",
       " 'L V2': -1,\n",
       " 'L V3': -1,\n",
       " 'L V4': -1,\n",
       " 'R vertebral': 0,\n",
       " 'R V1': 0,\n",
       " 'R V2': 0,\n",
       " 'R V3': 0,\n",
       " 'R V4': 0,\n",
       " 'basilar': 0,\n",
       " 'L PCA': 0,\n",
       " 'L P1': 0,\n",
       " 'L P2': 0,\n",
       " 'L P3': 0,\n",
       " 'L P4': 0,\n",
       " 'R PCA': 0,\n",
       " 'R P1': 0,\n",
       " 'R P2': 0,\n",
       " 'R P3': 0,\n",
       " 'R P4': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pog.filled_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L ICA', 'R ICA', 'L vertebral', 'L V1', 'L V2', 'L V3', 'L V4']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# possibilities = pog.get_possibilities()\n",
    "# possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMINATION: Computed tomography angiography (CTA) of the head with contrast; Computed tomography angiography (CTA) of the neck with contrast._x000D_\n",
      "CT perfusion imaging of the head with contrast_x000D_\n",
      "_x000D_\n",
      "HISTORY: Left hemiparesis and right conjugate gaze deviation, suspected hyperacute stroke._x000D_\n",
      "_x000D_\n",
      "TECHNIQUE: Computed tomography of the head was performed without contrast according to standard protocol. Computed tomographic angiography was obtained from the level of the aortic arch to the vertex following the uneventful administration of intravenous contrast according to hyperacute stroke protocol. 3D images were generated on a dedicated workstation._x000D_\n",
      "CT perfusion of the brain was performed with intravenous contrast using a separate data acquisition. The data was transmitted to a separate workstation for processing by RAPID software (iSchemaView) to produce automated calculations of the estimated cerebral blood flow and Tmax._x000D_\n",
      "_x000D_\n",
      "Contrast information:_x000D_\n",
      "125 mL Optiray-350_x000D_\n",
      "_x000D_\n",
      "COMPARISON: None available._x000D_\n",
      "_x000D_\n",
      "FINDINGS:_x000D_\n",
      "_x000D_\n",
      "HEAD CT FINDINGS:_x000D_\n",
      "_x000D_\n",
      "There is no acute intracranial hemorrhage. There is no noncontrast evidence of acute stroke. There is no vascular hyperdensity of the M1 segments or basilar artery. There are no periventricular and deep white matter hypodensities. There are no lacunar infarcts. Cerebral volume is typical for age. Ventricles are of normal size and morphology. There is no mass effect or midline shift._x000D_\n",
      "_x000D_\n",
      "ANGIOGRAPHIC FINDINGS:_x000D_\n",
      "_x000D_\n",
      "The visualized aortic arch appears normal with normal configuration of the great vessels. There is no significant stenosis of the origins of the great vessels._x000D_\n",
      "_x000D_\n",
      "There is no geographic area of vascular paucity in the brain._x000D_\n",
      "_x000D_\n",
      "Left anterior circulation:_x000D_\n",
      "L CCA: no occlusion or significant stenosis_x000D_\n",
      "L carotid bifurcation: no occlusion or significant stenosis_x000D_\n",
      "L ICA proximal: no occlusion or significant stenosis_x000D_\n",
      "L ICA distal: no occlusion or significant stenosis_x000D_\n",
      "L ICA terminus: no occlusion or significant stenosis_x000D_\n",
      "L M1: no occlusion or significant stenosis_x000D_\n",
      "L M2 branches: no occlusion or significant stenosis_x000D_\n",
      "L A2: no occlusion or significant stenosis_x000D_\n",
      "_x000D_\n",
      "Right anterior circulation:_x000D_\n",
      "R CCA: no occlusion or significant stenosis_x000D_\n",
      "R carotid bifurcation: no occlusion or significant stenosis_x000D_\n",
      "R ICA proximal: no occlusion or significant stenosis_x000D_\n",
      "R ICA distal: no occlusion or significant stenosis_x000D_\n",
      "R ICA terminus: no occlusion or significant stenosis_x000D_\n",
      "R M1: no occlusion or significant stenosis_x000D_\n",
      "R M2 branches: no occlusion or significant stenosis_x000D_\n",
      "R A2: no occlusion or significant stenosis_x000D_\n",
      " _x000D_\n",
      "Posterior circulation:_x000D_\n",
      "L Vertebral Artery: no occlusion or significant stenosis_x000D_\n",
      "R Vertebral Artery: Near-complete, long segment occlusion extending from the right subclavian artery origin to the C4 vertebral level with subsequent diminutive reconstitution._x000D_\n",
      "Basilar Artery: no occlusion or significant stenosis_x000D_\n",
      "L PCA: no occlusion or significant stenosis_x000D_\n",
      "R PCA: Partial occlusion of the mid-proximal P1 segment._x000D_\n",
      "_x000D_\n",
      "No cerebral aneurysm is seen. There is no evidence for an arteriovenous malformation. There is no suspicious cervical lymphadenopathy. There is multilevel degenerative changes of the cervical spine, greatest and moderate at C5-C7. Limited views of the lung apices are normal._x000D_\n",
      "_x000D_\n",
      "CT Perfusion:_x000D_\n",
      "_x000D_\n",
      "Estimated ischemic core volume (rCBF < 0.3): 5 mL_x000D_\n",
      "Estimated hypoperfusion volume (Tmax > 6 sec): 16 mL_x000D_\n",
      "_x000D_\n",
      "_x000D_\n",
      "IMPRESSION:_x000D_\n",
      "_x000D_\n",
      "1.  Acute ischemic stroke characterized by partial occlusion of the right posterior cerebral artery mid-proximal P1 segment._x000D_\n",
      "_x000D_\n",
      "2.  Long segment occlusion of the right vertebral artery extending from the right subclavian artery origin to the C4 vertebral level with distal diminutive reconstitution._x000D_\n",
      "_x000D_\n",
      "The Critical results were discussed at the scanner with Dr. Sarah from the Neurology team by Dr. Austin Cail on 04/08/2019 at 2:00 PM _x000D_\n",
      "_x000D_\n",
      "Dictated by: Philip Andrew Velez_x000D_\n",
      "_x000D_\n",
      "The radiology attending physician has personally reviewed this study, and had reviewed and/or edited this written report and agrees with it._x000D_\n",
      "_x000D_\n",
      "Electronically signed by: Cyrus A Raji, M.D, PHD\n"
     ]
    }
   ],
   "source": [
    "# print(filtered_df[\"CTA CTP Report Text\"].iloc[325])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L vertebral']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oi = occlusions_investigator(possibilities,impressions[325],itemData_init.get_itemData()[0],itemData_init.get_itemData()[1])\n",
    "# predicted_occlusions = oi.get_occlusions()\n",
    "# predicted_occlusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #predicted_occlusions vs possibilities\n",
    "# extra_poss = list(set(predicted_occlusions) - set(possibilities))\n",
    "# extra_poss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_x000D__x000D_  Acute ischemic stroke characterized by partial occlusion of the right posterior cerebral artery mid-proximal P1 segment']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oi.unmatched_occlusion_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oi.uncertain_occlusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oi.confirm_uncertain_occlusion('cut-off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('uncertain occlusion',\n",
       " 'M2',\n",
       " 'Abrupt cut off of distal right M2/ proximal M3 with geographic paucity of vasculature compared to contralateral side. ')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oi.confirm_limitation('Abrupt cut off of distal right M2/ proximal M3 with geographic paucity of vasculature compared to contralateral side. ','M2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_x000D__x000D_  Acute ischemic stroke characterized by partial occlusion of the right posterior cerebral artery mid-proximal P1 segment',\n",
       " '_x000D__x000D_  Long segment occlusion of the right vertebral artery extending from the right subclavian artery origin to the C4 vertebral level with distal diminutive reconstitution']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oi.filt_txt_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_x000D__x000D_  Acute ischemic stroke characterized by partial occlusion of the right posterior cerebral artery mid-proximal P1 segment',\n",
       "  'L',\n",
       "  'ICA',\n",
       "  None),\n",
       " ('_x000D__x000D_  Long segment occlusion of the right vertebral artery extending from the right subclavian artery origin to the C4 vertebral level with distal diminutive reconstitution',\n",
       "  'L',\n",
       "  'ICA',\n",
       "  None),\n",
       " ('_x000D__x000D_  Acute ischemic stroke characterized by partial occlusion of the right posterior cerebral artery mid-proximal P1 segment',\n",
       "  'R',\n",
       "  'ICA',\n",
       "  None),\n",
       " ('_x000D__x000D_  Long segment occlusion of the right vertebral artery extending from the right subclavian artery origin to the C4 vertebral level with distal diminutive reconstitution',\n",
       "  'R',\n",
       "  'ICA',\n",
       "  None),\n",
       " ('_x000D__x000D_  Acute ischemic stroke characterized by partial occlusion of the right posterior cerebral artery mid-proximal P1 segment',\n",
       "  'L',\n",
       "  'vertebral',\n",
       "  None),\n",
       " ('_x000D__x000D_  Long segment occlusion of the right vertebral artery extending from the right subclavian artery origin to the C4 vertebral level with distal diminutive reconstitution',\n",
       "  'L',\n",
       "  'vertebral',\n",
       "  -1),\n",
       " ('_x000D__x000D_  Acute ischemic stroke characterized by partial occlusion of the right posterior cerebral artery mid-proximal P1 segment',\n",
       "  'L',\n",
       "  'V1',\n",
       "  None),\n",
       " ('_x000D__x000D_  Long segment occlusion of the right vertebral artery extending from the right subclavian artery origin to the C4 vertebral level with distal diminutive reconstitution',\n",
       "  'L',\n",
       "  'V1',\n",
       "  None),\n",
       " ('_x000D__x000D_  Acute ischemic stroke characterized by partial occlusion of the right posterior cerebral artery mid-proximal P1 segment',\n",
       "  'L',\n",
       "  'V2',\n",
       "  None),\n",
       " ('_x000D__x000D_  Long segment occlusion of the right vertebral artery extending from the right subclavian artery origin to the C4 vertebral level with distal diminutive reconstitution',\n",
       "  'L',\n",
       "  'V2',\n",
       "  None),\n",
       " ('_x000D__x000D_  Acute ischemic stroke characterized by partial occlusion of the right posterior cerebral artery mid-proximal P1 segment',\n",
       "  'L',\n",
       "  'V3',\n",
       "  None),\n",
       " ('_x000D__x000D_  Long segment occlusion of the right vertebral artery extending from the right subclavian artery origin to the C4 vertebral level with distal diminutive reconstitution',\n",
       "  'L',\n",
       "  'V3',\n",
       "  None),\n",
       " ('_x000D__x000D_  Acute ischemic stroke characterized by partial occlusion of the right posterior cerebral artery mid-proximal P1 segment',\n",
       "  'L',\n",
       "  'V4',\n",
       "  None),\n",
       " ('_x000D__x000D_  Long segment occlusion of the right vertebral artery extending from the right subclavian artery origin to the C4 vertebral level with distal diminutive reconstitution',\n",
       "  'L',\n",
       "  'V4',\n",
       "  None)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oi.occlusion_ratings\n",
    "# oi.get_possibility_ratings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oi.filt_sent_has_occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, None, None, None, None, None]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oi.sent_has_occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oi.sent_maybe_occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_x000D__x000D_  Acute ischemic stroke characterized by partial occlusion of the right posterior cerebral artery mid-proximal P1 segment',\n",
       " '_x000D__x000D_  Long segment occlusion of the right vertebral artery extending from the right subclavian artery origin to the C4 vertebral level with distal diminutive reconstitution']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oi.filt_txt_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vessel_site_classifier:\n",
    "    \n",
    "    def __init__(self,occl_list):\n",
    "        self.occl_list = occl_list\n",
    "        #Large vessels: CCA, ICA, ACA, MCA, PCA, Vertebrals, Basilar\n",
    "        #Segments (of large vessels):\n",
    "        #A1-5, M1-4, V1-4, P1-4\n",
    "        self.vessels = []\n",
    "        self.sites = []\n",
    "        self.vs_dict = {\"CCA\": [],\n",
    "                       \"ICA\": [],\n",
    "                       \"ACA\": [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\"],\n",
    "                       \"MCA\": [\"M1\",\"M2\",\"M3\",\"M4\"],\n",
    "                       \"PCA\": [\"P1\",\"P2\",\"P3\",\"P4\"],\n",
    "                       \"vertebral\": [\"V1\",\"V2\",\"V3\",\"V4\"],\n",
    "                       \"basilar\": []}\n",
    "    def classify(self):\n",
    "        for occlusions in self.occl_list:\n",
    "            vessel_occlusions = []\n",
    "            site_occlusions = []\n",
    "            for occlusion in occlusions:\n",
    "                if len(occlusion.split()) > 1:\n",
    "                    side = occlusion.split()[0]\n",
    "                    occl = occlusion.split()[1]\n",
    "                else:\n",
    "                    side = ''\n",
    "                    occl = occlusion\n",
    "                if occl in self.vs_dict.keys():\n",
    "                    vessel_occlusions.append(side + \" \" + occl)\n",
    "                else:\n",
    "                    for vessel,segments in self.vs_dict.items():\n",
    "                        if occl in segments:\n",
    "                            vessel_occlusions.append(side + \" \" + vessel)\n",
    "                            #Ex ('MCA', 'L')\n",
    "                            site_occlusions.append(side + \" \" + occl)\n",
    "            #list(set()) removes duplicates (ex. multiple MCA occlucions -> ['MCA'])\n",
    "            self.vessels.append(list(set(vessel_occlusions)))\n",
    "            self.sites.append(site_occlusions)\n",
    "        return self.vessels,self.sites\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test vessel_site_classifier\n",
    "# test_vsc = vessel_site_classifier([predicted_occlusions])\n",
    "# predicted_vessel_occ, predicted_segments_occ = test_vsc.classify()\n",
    "# lower = lambda x:x.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L vertebral']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted_vessel_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted_segments_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l vertebral']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(set(map(lower,predicted_vessel_occ[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ICA', 'vertebral', 'MCA']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = lambda q:q.split()[-1]\n",
    "f = [\" MCA\",\"R ICA\", \"L vertebral\",\"L ICA\"]\n",
    "list(set(map(l,f)))\n",
    "#s.split()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "class stats_generator:\n",
    "    def __init__(self, X, y, check_side=None):\n",
    "        self.oneHotEncoder = MultiLabelBinarizer()\n",
    "        \n",
    "        if check_side:\n",
    "            #Get the 'L', or 'R' if there is a side, or put \"None\"\n",
    "            self.splitter = lambda w:w.split()[0] if ('L' in w) or ('R' in w) else 'None'\n",
    "        else:\n",
    "            #Get rid of side labels\n",
    "            #[\"L MCA\"] -> [\"MCA\"]\n",
    "            #***Merges both side occlusions ex. [\"R MCA\",\"L MCA\"] -> [\"MCA\"]\n",
    "            self.splitter = lambda l:l.split()[-1]\n",
    "        self.X = [list(set(map(self.splitter,i))) for i in X]\n",
    "        self.y = [list(set(map(self.splitter,i))) for i in y]\n",
    "        #If only self.y, classes in self.X not in self.y would\n",
    "        #cause an error when classification_report(...) generated\n",
    "        #Case where predictions vary from y labels \n",
    "        self.oneHotEncoder.fit(self.y+self.X)\n",
    "        self.encoded_X = self.oneHotEncoder.transform(self.X)\n",
    "        self.encoded_y = self.oneHotEncoder.transform(self.y)\n",
    "    def getStats(self):\n",
    "        print(classification_report(self.encoded_y, self.encoded_X, target_names=self.oneHotEncoder.classes_))\n",
    "        print(multilabel_confusion_matrix(self.encoded_y, self.encoded_X))\n",
    "        print(accuracy_score(self.encoded_y, self.encoded_X))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.vessels = [\"CCA\",\"ICA\",\"ACA\",\"MCA\",\"PCA\",\"vertebral\",\"basilar\"]\n",
    "#         self.sites = [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"M1\",\"M2\",\"M3\",\"M4\",\"P1\",\"P2\",\"P3\",\"P4\",\"V1\",\"V2\",\"V3\",\"V4\"]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 1 0 0]]\n",
      "[[0 0 1 1]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['ICA', 'MCA', 'basilar', 'vertebral'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testX = [['R basilar'],['L vertebral'],['L MCA','L ICA']]\n",
    "# testy = [['L vertebral', ' basilar', 'R vertebral'],['L vertebral'],['L ICA']]\n",
    "# sg = stats_generator(testX, testy)\n",
    "# print(sg.encoded_X)\n",
    "# print(sg.encoded_y)\n",
    "# sg.oneHotEncoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ICA       1.00      1.00      1.00         1\n",
      "         MCA       0.00      0.00      0.00         0\n",
      "     basilar       1.00      1.00      1.00         1\n",
      "   vertebral       1.00      0.50      0.67         2\n",
      "\n",
      "   micro avg       0.75      0.75      0.75         4\n",
      "   macro avg       0.75      0.62      0.67         4\n",
      "weighted avg       1.00      0.75      0.83         4\n",
      " samples avg       0.83      0.83      0.78         4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# print(classification_report(sg.encoded_y, sg.encoded_X, target_names=sg.oneHotEncoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "[[1 1 1]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['L', 'None', 'R'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # testX = [['R basilar'],['L vertebral'],[]]\n",
    "# # testy = [['L vertebral', ' basilar', 'R vertebral'],['L vertebral'],['R MCA']]\n",
    "# sg = stats_generator(testX, testy, \"sides\")\n",
    "# print(sg.encoded_X)\n",
    "# print(sg.encoded_y)\n",
    "# sg.oneHotEncoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           L       1.00      0.67      0.80         3\n",
      "        None       0.00      0.00      0.00         1\n",
      "           R       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      0.60      0.75         5\n",
      "   macro avg       0.67      0.56      0.60         5\n",
      "weighted avg       0.80      0.60      0.68         5\n",
      " samples avg       1.00      0.78      0.83         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# print(classification_report(sg.encoded_y, sg.encoded_X, target_names=sg.oneHotEncoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R', 'R', 'L']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class labels_extractor:\n",
    "    \n",
    "    def __init__(self,side_column,occlusion_column):\n",
    "        self.side_labels = self.extract_occlusion_labels(side_column)\n",
    "        self.occl_labels = self.extract_occlusion_labels(occlusion_column)\n",
    "    \n",
    "    def extract_occlusion_labels(self,df):\n",
    "        #side = (df['Side'])\n",
    "        cleaned_df = list(df.fillna(''))\n",
    "\n",
    "        #df_normalize_both = [df.lower() if \"both\" in df.lower() else df for df in cleaned_df]\n",
    "\n",
    "        df_no_spaces = [df.replace(\" \",\"\") for df in cleaned_df]\n",
    "\n",
    "        df_no_seps = [re.split(\"\\/|&\",df) if (\"/\" in df or \"&\" in df) else df for df in df_no_spaces]\n",
    "\n",
    "\n",
    "        return df_no_seps\n",
    "    \n",
    "    def return_labels(self):\n",
    "        labels = []\n",
    "        for side, occl in zip(self.side_labels,self.occl_labels):\n",
    "            #Occl is '', i.e. no occlusion\n",
    "            if not occl:\n",
    "                labels.append([])\n",
    "            else:\n",
    "                #Lowercase vertebral and basilar to match predictions\n",
    "                if occl in [\"Vertebral\", \"Basilar\"]:\n",
    "                    occl = occl.lower()\n",
    "                #Ex. [\"M2\", \"M1\"]\n",
    "                if isinstance(occl,list):\n",
    "                    #Ex. [\"L\", \"R\"]\n",
    "                    if isinstance(side,list):\n",
    "                        labels.append([\" \".join(tup) for tup in zip(side,occl)])\n",
    "#                         for s, o in zip(side,occl):\n",
    "#                             labels.append(s + \" \" + o)\n",
    "                    else:\n",
    "                        if side:\n",
    "                            side += \" \"\n",
    "                        l = []\n",
    "                        for o in occl:\n",
    "                            if \"both\" in side.lower():\n",
    "                                l.append(\"R \"+ o)\n",
    "                                l.append(\"L \"+ o)\n",
    "                            else:\n",
    "                                l.append(side + o)\n",
    "                        labels.append(l)\n",
    "                else:\n",
    "                    if side:\n",
    "                        side += \" \"\n",
    "                    if \"both\" in side.lower():\n",
    "                        labels.append([\"R \"+ occl, \"L \"+ occl])\n",
    "#                         l.append(\"R \"+ occl)\n",
    "#                         l.append(\"L \"+ occl)\n",
    "                    else:\n",
    "                        labels.append([side + occl])\n",
    "        return labels\n",
    "                        \n",
    "                    \n",
    "\n",
    "    \n",
    "# side_labels = extract_occlusion_labels(filtered_df['Side'])\n",
    "# side_labels\n",
    "# le = labels_extractor(filtered_df['Side'],filtered_df['site occlusion'])\n",
    "# le.side_labels[421]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vertebral', 'Basilar', 'ICA']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# occl_labels = extract_occlusion_labels(filtered_df['site occlusion'])\n",
    "# occl_labels\n",
    "# le.occl_labels[452]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Both Vertebral', ' Basilar', 'R ICA']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels = le.return_labels()\n",
    "# labels[452]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Both Vertebral', ' Basilar', 'R ICA']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels[452]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L vertebral']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted_occlusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted_occlusions == labels[129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class test_occlusions:\n",
    "    \n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        self.df_report_column = self.df[\"CTA CTP Report Text\"]\n",
    "        \n",
    "        #For output excel file\n",
    "        self.output_dir = \"\"\n",
    "        \n",
    "        #Class to extract desired portions of reports\n",
    "        self.report_extractor = report_text_extractor(self.df_report_column)\n",
    "        #Cerebral artery templates\n",
    "        #NOT USED\n",
    "        self.report_ca_templates = self.report_extractor.get_report_templates()\n",
    "        #End 'Impression:' section\n",
    "        self.impressions = self.report_extractor.get_impressions()\n",
    "        #CTP Perfusion section\n",
    "        self.ctp_sections = self.report_extractor.get_ctps()\n",
    "        \n",
    "        self.ctpve = ctp_volumes_extractor(self.ctp_sections)\n",
    "        self.ctpve.extract_volumes()\n",
    "        ##List of tuples of format (ischemic core volume,hypoperfusion volume)\n",
    "        self.ctp_volumes = self.ctpve.get_volumes()\n",
    "        \n",
    "        #Goes reads through all templates and populates a dictionary\n",
    "        self.ca_templates_reader = ca_templates_reader(self.df_report_column)\n",
    "        self.ca_templates_reader.compile_regex()\n",
    "        self.ca_templates_dicts = self.ca_templates_reader.read_templates()\n",
    "        \n",
    "        #Setting up pyConText\n",
    "        self.mod_path = \"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/modifiers.yml\"\n",
    "        self.targ_path = \"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/targets.yml\"\n",
    "        self.itemData_init = pyConText_itemData_initializer(self.mod_path,self.targ_path)\n",
    "        self.modifiers = self.itemData_init.get_itemData()[0]\n",
    "        self.targets = self.itemData_init.get_itemData()[1]\n",
    "        \n",
    "        #Finding possible occlusions based on cerebral arteries templates\n",
    "        self.ca_to_investigate = []\n",
    "        self.pogs = []\n",
    "        for template in self.ca_templates_dicts:\n",
    "            pog = possible_occlusions_generator(template)\n",
    "            self.pogs.append(pog)\n",
    "            self.ca_to_investigate.append(pog.get_possibilities())\n",
    "        \n",
    "        #Investigating the possible occlusions of the impressions of each report\n",
    "        #Using pyConText (used by occlusions_investigator)\n",
    "        \n",
    "        \n",
    "        #self.predicted_occlusions -> Raw occlusions found, used to populate self.occlusion_labels,\n",
    "        # self.predicted_vessels, and self.predicted_segments\n",
    "        self.predicted_occlusions = []\n",
    "        #For debugging, -1 represents flag of uncertainty\n",
    "        self.possibility_ratings = []\n",
    "        self.markups = []\n",
    "        self.unmatched_occlusion_sents = []\n",
    "        self.uncertain_occlusions = []\n",
    "        \n",
    "        for possibilities,impression in zip(self.ca_to_investigate,self.impressions):\n",
    "            \n",
    "            \n",
    "            oi = occlusions_investigator(possibilities,impression,self.modifiers,self.targets)\n",
    "            self.predicted_occlusions.append(oi.get_occlusions())\n",
    "            self.possibility_ratings.append(oi.get_possibility_ratings())\n",
    "            self.markups.append(oi.get_markups())\n",
    "            self.unmatched_occlusion_sents.append(oi.get_unmatched_occlusion_sents())\n",
    "            self.uncertain_occlusions.append(oi.get_uncertain_occlusions())\n",
    "            \n",
    "            \n",
    "        self.pred_occl_statuses = [0 if not occl else 1 for occl in self.predicted_occlusions]\n",
    "        self.occl_status_labels = [1 if val > 0 else val for val in self.df['LVO']]\n",
    "        \n",
    "        self.pred_classifier = vessel_site_classifier(self.predicted_occlusions)\n",
    "        self.predicted_vessels, self.predicted_segments = self.pred_classifier.classify()\n",
    "        \n",
    "        #Extracting labels to verify the predicted occlusions\n",
    "        self.labels_extractor = labels_extractor(self.df['Side'],self.df['site occlusion'])\n",
    "        self.labels = self.labels_extractor.return_labels()\n",
    "        \n",
    "        self.labels_classifier = vessel_site_classifier(self.labels)\n",
    "        self.labeled_vessels, self.labeled_segments = self.labels_classifier.classify()        \n",
    "                \n",
    "        #self.differences = [(prediction,label) if list(set(map(self.lower,prediction))^set(map(self.lower,label))) \n",
    "                       #else () for prediction,label in zip(self.predicted_occlusions,self.labels) ]\n",
    "        self.diffs = self.differences(self.predicted_occlusions,self.labels)\n",
    "        #Indices of differences of occlusion status\n",
    "        self.occl_status_diffs = np.where((abs(np.subtract(self.pred_occl_statuses,self.occl_status_labels)).astype(bool)))\n",
    "        self.vessels_diffs = self.differences(self.predicted_vessels,self.labeled_vessels)\n",
    "        self.segments_diffs = self.differences(self.predicted_segments,self.labeled_segments)\n",
    "    \n",
    "\n",
    "    \n",
    "    def differences(self,preds,labels):\n",
    "        #Compare lower case labels to lower case predictions\n",
    "        #Set ^ operation returns list with elements unique to either prediction or label but not both\n",
    "        lower = lambda x:x.lower()\n",
    "        differences = []\n",
    "        for prediction,label in zip(preds,labels):\n",
    "            if list(set(map(lower,prediction))^set(map(lower,label))):\n",
    "                differences.append((prediction,label))\n",
    "            else:\n",
    "                differences.append(())\n",
    "        return differences\n",
    "    \n",
    "     \n",
    "    #An uncertainty flag for user review\n",
    "    #may show a template error\n",
    "    #returns the sentences and report indices where occlusions are mentioned in the findings \n",
    "    #but the algo did not register any at those same cerebral arteries in template\n",
    "    def get_unmatched_occlusions(self):\n",
    "        return [(sent,i) for sent,i in zip(self.unmatched_occlusion_sents,range(len(self.unmatched_occlusion_sents))) if sent]\n",
    "    \n",
    "    #For user review of instances where occlusion is possible because of similar terms used, but occlusion not used\n",
    "    def get_uncertain_occlusions(self):\n",
    "        return [(uo,i) for uo,i in zip(self.uncertain_occlusions,range(len(self.uncertain_occlusions))) if uo] \n",
    "        #return self.uncertain_occlusions\n",
    "    \n",
    "    def get_occlusion_metrics(self):\n",
    "            #Precision/PPV = TP/TP+FP i.e. out of true calls, how many actually true\n",
    "            #Recall = TP/TP+FN i.e. out of all trues, how many algo called true\n",
    "            #F1-score = Accuracy taking into account precision and recall\n",
    "            print(classification_report(to.occl_status_labels,to.pred_occl_statuses))\n",
    "            print(multilabel_confusion_matrix(to.occl_status_labels,to.pred_occl_statuses))\n",
    "    \n",
    "    def get_vessel_metrics(self):\n",
    "        vessel_sg = stats_generator(self.predicted_vessels,self.labeled_vessels)\n",
    "        vessel_sg.getStats()\n",
    "        \n",
    "    def get_segment_metrics(self):\n",
    "        segment_sg = stats_generator(self.predicted_segments,self.labeled_segments)\n",
    "        segment_sg.getStats()\n",
    "        \n",
    "    def get_side_metrics(self):\n",
    "        side_sg = stats_generator(self.predicted_segments,self.labeled_segments,\"sides\")\n",
    "        side_sg.getStats()\n",
    "    \n",
    "    def get_occlusion_accuracy(self):\n",
    "        #Subtracts the 1's and 0's of the predicted and labeled occl statuses and turns them positive, sums, divides\n",
    "        #by the total to get the error rate, then yields accuracy by 1-error_rate\n",
    "        return(1-(sum(abs(np.subtract(self.pred_occl_statuses,self.occl_status_labels)))/len(self.occl_status_labels)))\n",
    "    \n",
    "    def get_vessel_accuracy(self):\n",
    "        return self.get_accuracy(self.vessels_diffs)\n",
    "    \n",
    "    def get_segments_accuracy(self):\n",
    "        return self.get_accuracy(self.segments_diffs)\n",
    "    \n",
    "    def get_raw_accuracy(self):\n",
    "        return self.get_accuracy(self.diffs)\n",
    "        \n",
    "        \n",
    "    def get_accuracy(self,diffs_list):\n",
    "        score = 0\n",
    "        for tup in diffs_list:\n",
    "            if not tup:\n",
    "                score += 1\n",
    "        return(score/len(diffs_list))\n",
    "    \n",
    "    def get_volumes(self):\n",
    "        return self.ctp_volumes\n",
    "      \n",
    "    #Writes to an excel file\n",
    "    #(Takes input excel file, adds columns, and outputs it)\n",
    "    #Input: Input excel file dataframe, input excel file path\n",
    "    def write_to_excel(self,raw_df, raw_dir):\n",
    "        output_df = raw_df\n",
    "        self.output_dir = raw_dir.split(\".\")[0] + \"_AUTOMATED_OCCLUSION_PREDICTIONS.xlsx\"\n",
    "        outputs = {\"pred_occlusion\": self.pred_occl_statuses,\n",
    "                   \"pred_vessels\": self.predicted_vessels,\n",
    "                   \"pred_segments\": self.predicted_segments,\n",
    "                   \"true_vessels\": self.labeled_vessels,\n",
    "                   \"true_segments\": self.labeled_segments,\n",
    "                   \"volumes\": self.get_volumes(),\n",
    "                   \"unmatched_occlusions\": self.unmatched_occlusion_sents,\n",
    "                   \"uncertain_occlusions\": self.uncertain_occlusions\n",
    "                  }\n",
    "        for col_name, col_data in outputs.items():\n",
    "            output_df[col_name] = col_data\n",
    "        output_df.to_excel(self.output_dir,index=False)\n",
    "    \n",
    "    def get_output_excel_path(self):\n",
    "        return self.output_dir\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/pyConTextNLP/itemData.py:40: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  context_items =  [contextItem((d[\"Lex\"],\n"
     ]
    }
   ],
   "source": [
    "to = test_occlusions(filtered_df)\n",
    "#print(to.report_ca_templates[90])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to.pred_occl_statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to.occl_status_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       422\n",
      "           1       0.89      0.94      0.91       155\n",
      "\n",
      "    accuracy                           0.95       577\n",
      "   macro avg       0.93      0.95      0.94       577\n",
      "weighted avg       0.95      0.95      0.95       577\n",
      "\n",
      "[[[145  10]\n",
      "  [ 18 404]]\n",
      "\n",
      " [[404  18]\n",
      "  [ 10 145]]]\n"
     ]
    }
   ],
   "source": [
    "to.get_occlusion_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACA       0.83      1.00      0.91         5\n",
      "         CCA       0.33      0.25      0.29         4\n",
      "         ICA       0.78      0.78      0.78        32\n",
      "         MCA       0.94      0.96      0.95       112\n",
      "         PCA       0.86      0.92      0.89        13\n",
      "     basilar       0.33      1.00      0.50         3\n",
      "   vertebral       0.60      0.92      0.73        13\n",
      "\n",
      "   micro avg       0.83      0.91      0.87       182\n",
      "   macro avg       0.67      0.83      0.72       182\n",
      "weighted avg       0.85      0.91      0.88       182\n",
      " samples avg       0.24      0.25      0.24       182\n",
      "\n",
      "[[[571   1]\n",
      "  [  0   5]]\n",
      "\n",
      " [[571   2]\n",
      "  [  3   1]]\n",
      "\n",
      " [[538   7]\n",
      "  [  7  25]]\n",
      "\n",
      " [[458   7]\n",
      "  [  5 107]]\n",
      "\n",
      " [[562   2]\n",
      "  [  1  12]]\n",
      "\n",
      " [[568   6]\n",
      "  [  0   3]]\n",
      "\n",
      " [[556   8]\n",
      "  [  1  12]]]\n",
      "0.925476603119584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "to.get_vessel_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       1.00      1.00      1.00         1\n",
      "          A2       1.00      1.00      1.00         2\n",
      "          A3       1.00      1.00      1.00         1\n",
      "          M1       0.88      0.91      0.89        64\n",
      "          M2       0.94      0.97      0.95        61\n",
      "          M3       0.60      1.00      0.75         3\n",
      "          P1       0.75      1.00      0.86         3\n",
      "          P2       1.00      1.00      1.00         7\n",
      "          V2       0.50      1.00      0.67         1\n",
      "          V3       0.50      1.00      0.67         2\n",
      "          V4       0.38      0.75      0.50         4\n",
      "\n",
      "   micro avg       0.86      0.94      0.90       149\n",
      "   macro avg       0.78      0.97      0.84       149\n",
      "weighted avg       0.88      0.94      0.91       149\n",
      " samples avg       0.20      0.21      0.20       149\n",
      "\n",
      "[[[576   0]\n",
      "  [  0   1]]\n",
      "\n",
      " [[575   0]\n",
      "  [  0   2]]\n",
      "\n",
      " [[576   0]\n",
      "  [  0   1]]\n",
      "\n",
      " [[505   8]\n",
      "  [  6  58]]\n",
      "\n",
      " [[512   4]\n",
      "  [  2  59]]\n",
      "\n",
      " [[572   2]\n",
      "  [  0   3]]\n",
      "\n",
      " [[573   1]\n",
      "  [  0   3]]\n",
      "\n",
      " [[570   0]\n",
      "  [  0   7]]\n",
      "\n",
      " [[575   1]\n",
      "  [  0   1]]\n",
      "\n",
      " [[573   2]\n",
      "  [  0   2]]\n",
      "\n",
      " [[568   5]\n",
      "  [  1   3]]]\n",
      "0.9549393414211439\n"
     ]
    }
   ],
   "source": [
    "to.get_segment_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           L       0.89      0.99      0.94        69\n",
      "           R       0.90      0.93      0.92        58\n",
      "\n",
      "   micro avg       0.90      0.96      0.93       127\n",
      "   macro avg       0.90      0.96      0.93       127\n",
      "weighted avg       0.90      0.96      0.93       127\n",
      " samples avg       0.21      0.21      0.21       127\n",
      "\n",
      "[[[500   8]\n",
      "  [  1  68]]\n",
      "\n",
      " [[513   6]\n",
      "  [  4  54]]]\n",
      "0.9688041594454073\n"
     ]
    }
   ],
   "source": [
    "to.get_side_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 18, 10, 145)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# tn, fp, fn, tp = confusion_matrix(to.occl_status_labels,to.pred_occl_statuses).ravel()\n",
    "# (tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " (['L V3', 'L V4'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R P2'], ['R P2']),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L V3', 'L V4'], ['L V3', 'L V4']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R V4'], ['R V4']),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], []),\n",
       " ([], ['R V4', 'L V4']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1', 'L M2'], ['L M1', 'L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1', 'L M2'], ['L M1', 'L M2']),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], ['R M1', 'R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L V4'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2', 'R M3'], ['R M2', 'R M3']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L V4', 'R V4'], ['R V4', 'L V4']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M2'], ['R M1', 'R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1', 'R M2', 'R M3'], ['R M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['L M1'], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L P2'], ['L P2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1'], []),\n",
       " (['L M2', 'L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2', 'R M3'], ['R M2', 'R M3']),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R P1'], ['R P1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R A2', 'R M1'], ['R M1', 'R A2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L P1'], ['L P1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L P2'], ['L P2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " (['L M1', 'L P2'], ['L M1', 'L P2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R P1'], []),\n",
       " ([], []),\n",
       " (['R P2'], ['R P2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1', 'R M1'], ['L M1']),\n",
       " ([], []),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1', 'L M2'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1', 'R M2', 'R M3'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L V2'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1', 'R M1', 'R M2', 'R M2'], ['R M1', 'R M2']),\n",
       " (['R M1', 'R M1', 'R M2', 'R M2'], ['R M1', 'R M2']),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R V3', 'R V4'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2', 'R M1'], ['R M1', 'R M2']),\n",
       " (['R M2', 'R M1'], ['R M1', 'R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1', 'R M1'], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M3'], ['L M3']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R A2'], ['R A2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R P1'], ['R P1']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1', 'L M2'], ['L M2']),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1', 'R M2', 'R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M1', 'L M2']),\n",
       " ([], []),\n",
       " (['L V4', 'L V2', 'L V3', 'R V2', 'R V3'], ['L V2', 'L V3']),\n",
       " ([], []),\n",
       " (['R M2', 'R M2'], ['R M1', 'R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], ['R M1']),\n",
       " (['L P2'], ['L P2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L A3'], ['L A3']),\n",
       " (['L M2'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M1', 'R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1', 'L M2'], ['L M1', 'L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L A1', 'L M1', 'L M2'], ['L M1', 'L M2', 'L A1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " (['R V4', 'R P2'], ['R P2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], [])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segs = list(zip(to.predicted_segments, to.labeled_segments))\n",
    "# segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o = ['L M1', ' basilar', 'M']\n",
    "# seg_m1 = lambda x: int(not not sum([\"M1\" in l for l in x]))\n",
    "# seg_m2 = lambda x: int(not not sum([\"M2\" in l for l in x]))\n",
    "# seg_m2(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['L M2'], ['L M2']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['L M1', 'L M2'], ['L M1', 'L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M1', 'L M2'], ['L M1', 'L M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], ['R M1', 'R M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2', 'R M3'], ['R M2', 'R M3']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M2'], ['R M1', 'R M2']),\n",
       " (['R M1', 'R M2', 'R M3'], ['R M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['L M2', 'L M2'], ['L M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (['R M2', 'R M3'], ['R M2', 'R M3']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R A2', 'R M1'], ['R M1', 'R A2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M1', 'L P2'], ['L M1', 'L P2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M1', 'R M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['L M1', 'L M2'], ['L M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1', 'R M2', 'R M3'], ['R M2']),\n",
       " (['L V2'], ['L M1']),\n",
       " (['R M1', 'R M1', 'R M2', 'R M2'], ['R M1', 'R M2']),\n",
       " (['R M1', 'R M1', 'R M2', 'R M2'], ['R M1', 'R M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M2', 'R M1'], ['R M1', 'R M2']),\n",
       " (['R M2', 'R M1'], ['R M1', 'R M2']),\n",
       " (['L M1', 'R M1'], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M1', 'L M2'], ['L M2']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1', 'R M2', 'R M2'], ['R M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M1', 'L M2']),\n",
       " (['R M2', 'R M2'], ['R M1', 'R M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], ['R M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M1', 'R M1'], ['R M1']),\n",
       " (['L M1', 'L M2'], ['L M1', 'L M2']),\n",
       " ([], ['R M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L A1', 'L M1', 'L M2'], ['L M1', 'L M2', 'L A1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M2'], ['R M2'])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Only M1 or M2s in segs\n",
    "# segs = [(segs[i][0],segs[i][1]) for i in range(len(segs)) if (seg_m1(segs[i][1]) or seg_m2(segs[i][1]))]\n",
    "# segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted  labels\n",
       "0            0       0\n",
       "1            1       1\n",
       "2            1       1\n",
       "3            0       0\n",
       "4            0       0\n",
       "..         ...     ...\n",
       "103          0       0\n",
       "104          1       1\n",
       "105          1       1\n",
       "106          0       0\n",
       "107          0       0\n",
       "\n",
       "[108 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seg_m1_status_df = pd.DataFrame([(seg_m1(p),seg_m1(l)) for (p,l) in segs])\n",
    "# seg_m1_status_df.columns = [\"predicted\",\"labels\"]\n",
    "# seg_m1_status_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m1_df = pd.DataFrame([(seg_m1(p),seg_m1(l)) for (p,l) in segs])\n",
    "# seg_m1_df.columns = [\"predicted\",\"labels\"]\n",
    "# seg_m2_df = pd.DataFrame([(seg_m2(p),seg_m2(l)) for (p,l) in segs])\n",
    "# seg_m2_df.columns = [\"predicted\",\"labels\"]\n",
    "# seg_m1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m1_df.labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m1_df.predicted.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m2_df.labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m2_df.predicted.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90        44\n",
      "           1       0.95      0.91      0.93        64\n",
      "\n",
      "    accuracy                           0.92       108\n",
      "   macro avg       0.91      0.92      0.91       108\n",
      "weighted avg       0.92      0.92      0.92       108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(classification_report(seg_m1_status_df.labels, seg_m1_status_df.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 3, 6, 58)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# tn, fp, fn, tp = confusion_matrix(seg_m1_status_df.labels, seg_m1_status_df.predicted).ravel()\n",
    "# (tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted  labels\n",
       "0            1       1\n",
       "1            1       1\n",
       "3            1       1\n",
       "4            1       1\n",
       "5            1       1\n",
       "..         ...     ...\n",
       "102          1       1\n",
       "103          0       1\n",
       "105          1       1\n",
       "106          1       1\n",
       "107          1       1\n",
       "\n",
       "[61 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seg_m2_df = pd.DataFrame([(seg_m2(p),seg_m2(l)) for (p,l) in segs])\n",
    "# seg_m2_df.columns = [\"predicted\",\"labels\"]\n",
    "# seg_m2_df[seg_m2_df[\"labels\"]>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        47\n",
      "           1       0.97      0.97      0.97        61\n",
      "\n",
      "    accuracy                           0.96       108\n",
      "   macro avg       0.96      0.96      0.96       108\n",
      "weighted avg       0.96      0.96      0.96       108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(classification_report(seg_m2_df.labels, seg_m2_df.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 2, 2, 59)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# tn, fp, fn, tp = confusion_matrix(seg_m2_df.labels,seg_m2_df.predicted).ravel()\n",
    "# (tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L ICA']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to.predicted_vessels[163]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L ICA']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to.labeled_vessels[163]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_df = raw_df\n",
    "# output_dir = raw_dir.split(\".\")[0] + \"_AUTOMATED_OCCLUSION_PREDICTIONS.xlsx\"\n",
    "# outputs = {\"pred_occlusion\":to.pred_occl_statuses,\n",
    "#            \"pred_vessels\":to.predicted_vessels,\n",
    "#            \"pred_segments\":to.predicted_segments,\n",
    "#            \"volumes\":to.get_volumes()\n",
    "#           }\n",
    "# for col_name, col_data in outputs.items():\n",
    "#     output_df[col_name] = col_data\n",
    "\n",
    "# output_df.to_excel(output_dir,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.write_to_excel(raw_df,raw_dir)\n",
    "print(\"save complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Organization</th>\n",
       "      <th>Point of Care</th>\n",
       "      <th>Accession Number</th>\n",
       "      <th>Exam Code</th>\n",
       "      <th>Exam Description</th>\n",
       "      <th>LVO</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Side</th>\n",
       "      <th>Site</th>\n",
       "      <th>Carotid stenosis</th>\n",
       "      <th>...</th>\n",
       "      <th>Report Created to Report Finalized (minutes)</th>\n",
       "      <th>Preliminary Report to Report Finalized (minutes)</th>\n",
       "      <th>pred_occlusion</th>\n",
       "      <th>pred_vessels</th>\n",
       "      <th>pred_segments</th>\n",
       "      <th>true_vessels</th>\n",
       "      <th>true_segments</th>\n",
       "      <th>volumes</th>\n",
       "      <th>unmatched_occlusions</th>\n",
       "      <th>uncertain_occlusions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 0092</td>\n",
       "      <td>53737770</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH ED</td>\n",
       "      <td>53736230</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>(13, 5)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[('uncertain occlusion', 'vertebral', '  _x000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH ED</td>\n",
       "      <td>53728765</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>164.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>(0, 8)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH ED</td>\n",
       "      <td>53728726</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>407.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 0105</td>\n",
       "      <td>53718900</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>1724.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>(0, 5)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Organization Point of Care  Accession Number Exam Code  \\\n",
       "0          BJH      BJH 0092          53737770    IMG575   \n",
       "1          BJH        BJH ED          53736230    IMG575   \n",
       "2          BJH        BJH ED          53728765    IMG575   \n",
       "3          BJH        BJH ED          53728726    IMG575   \n",
       "4          BJH      BJH 0105          53718900    IMG575   \n",
       "\n",
       "           Exam Description  LVO Comments Side Site Carotid stenosis  ...  \\\n",
       "0  CTA/CTP RAPID STROKE (C)    0      NaN  NaN  NaN              NaN  ...   \n",
       "1  CTA/CTP RAPID STROKE (C)    0      NaN  NaN  NaN              NaN  ...   \n",
       "2  CTA/CTP RAPID STROKE (C)    0      NaN  NaN  NaN              NaN  ...   \n",
       "3  CTA/CTP RAPID STROKE (C)    0      NaN  NaN  NaN              NaN  ...   \n",
       "4  CTA/CTP RAPID STROKE (C)    0      NaN  NaN  NaN              NaN  ...   \n",
       "\n",
       "   Report Created to Report Finalized (minutes)  \\\n",
       "0                                          12.0   \n",
       "1                                          70.0   \n",
       "2                                         164.0   \n",
       "3                                         407.0   \n",
       "4                                        1731.0   \n",
       "\n",
       "   Preliminary Report to Report Finalized (minutes)  pred_occlusion  \\\n",
       "0                                               3.0               0   \n",
       "1                                              10.0               0   \n",
       "2                                             126.0               0   \n",
       "3                                             370.0               0   \n",
       "4                                            1724.0               0   \n",
       "\n",
       "  pred_vessels pred_segments true_vessels  true_segments  volumes  \\\n",
       "0           []            []           []             []   (0, 0)   \n",
       "1           []            []           []             []  (13, 5)   \n",
       "2           []            []           []             []   (0, 8)   \n",
       "3           []            []           []             []   (0, 0)   \n",
       "4           []            []           []             []   (0, 5)   \n",
       "\n",
       "  unmatched_occlusions                               uncertain_occlusions  \n",
       "0                   []                                                 []  \n",
       "1                   []  [('uncertain occlusion', 'vertebral', '  _x000...  \n",
       "2                   []                                                 []  \n",
       "3                   []                                                 []  \n",
       "4                   []                                                 []  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.read_excel(to.get_output_excel_path()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_pred_vessels = mlb_fit.transform(to.predicted_vessels)\n",
    "# bin_labeled_vessels = mlb_fit.transform(to.labeled_vessels)\n",
    "# metrics.confusion_matrix(bin_labeled_vessels.argmax(axis=1), bin_pred_vessels.argmax(axis=1), labels=mlb_fit.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 71,  72,  73,  87, 109, 114, 125, 137, 203, 210, 235, 241, 283,\n",
       "        301, 306, 334, 345, 359, 387, 403, 404, 418, 419, 452, 516, 528,\n",
       "        537, 568]),)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.occl_status_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L MCA', 'L ICA']),\n",
       " (['L MCA', 'L ICA'], ['L ICA']),\n",
       " ([], ['L vertebral', 'R vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'L ICA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([' basilar'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([' basilar', 'L vertebral', 'R vertebral'], [' vertebral', ' basilar']),\n",
       " (['R ICA', 'R MCA'], ['R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', ' basilar', 'R vertebral'], ['L vertebral', 'R vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L ICA'], ['L CCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'R vertebral'], ['L MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'R MCA'], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA'], []),\n",
       " (['L PCA', ' basilar', 'L vertebral'], ['L vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'R MCA', 'R ACA'], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([' basilar'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R PCA', 'R vertebral'], ['R vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L CCA']),\n",
       " (['L MCA', 'R MCA'], ['L MCA']),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'R vertebral'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'R CCA', 'L vertebral'], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L PCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral'], ['L MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R vertebral'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'L MCA', 'R MCA'], ['R ICA', 'R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([' basilar'], []),\n",
       " (['L MCA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L MCA']),\n",
       " ([], ['R ICA']),\n",
       " (),\n",
       " (['R ICA', 'R CCA', 'L ICA'], ['R ICA', 'R CCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([' basilar'], ['L basilar', 'R basilar']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R PCA', ' basilar'], ['R PCA', 'R basilar']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R PCA', ' basilar'], ['R PCA']),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'R vertebral'], ['L vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R CCA', 'L vertebral'], ['L vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R vertebral'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L ICA', 'L ACA'], ['L ICA', 'L MCA', 'L CCA', 'L ACA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L ICA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R PCA', 'R vertebral'], ['R PCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ()]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to.vessels_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['R ICA', 'R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA', 'L ICA', 'L ACA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['L vertebral'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R PCA'],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['L ICA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L vertebral'],\n",
       " ['R MCA'],\n",
       " ['R vertebral'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA', 'L ICA'],\n",
       " ['L ICA'],\n",
       " ['L vertebral', 'R vertebral'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA', 'L ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R vertebral'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['R ICA', 'R MCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [' vertebral', ' basilar'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L vertebral', 'R vertebral'],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " ['R MCA'],\n",
       " ['L CCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L PCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " ['R ICA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L vertebral'],\n",
       " ['R MCA'],\n",
       " ['R MCA'],\n",
       " ['R ICA', 'R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L vertebral', 'R vertebral'],\n",
       " [],\n",
       " ['R MCA'],\n",
       " ['L MCA', 'L ICA'],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " ['R ICA'],\n",
       " [],\n",
       " ['R PCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R vertebral'],\n",
       " ['R ICA'],\n",
       " [],\n",
       " [],\n",
       " ['R ICA', 'R MCA', 'R ACA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L PCA'],\n",
       " [],\n",
       " ['R ICA'],\n",
       " ['R ICA'],\n",
       " ['L PCA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['L MCA', 'L PCA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['R vertebral'],\n",
       " [],\n",
       " ['R PCA'],\n",
       " [],\n",
       " [],\n",
       " ['L CCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA', 'L ICA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R ICA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L ICA'],\n",
       " [],\n",
       " [],\n",
       " ['L ICA'],\n",
       " ['L PCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R ICA', 'R MCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['R ICA'],\n",
       " [],\n",
       " ['R ICA', 'R CCA'],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R ACA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L basilar', 'R basilar'],\n",
       " ['L ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R PCA', 'R basilar'],\n",
       " ['R MCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R PCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['L vertebral'],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['L MCA', 'L ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " ['L PCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['L vertebral'],\n",
       " ['L MCA'],\n",
       " ['L ICA'],\n",
       " ['L MCA'],\n",
       " ['L ACA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " ['R ICA', 'R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['L PCA'],\n",
       " ['L ICA', 'L MCA', 'L CCA', 'L ACA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " ['R PCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to.labeled_vessels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8492201039861352"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.get_raw_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951473136915078"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.get_occlusion_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9133448873483535"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.get_vessel_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951473136915078"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.get_segments_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'R MCA', 'R M1', 'R M2'], ['R ICA', 'R M1', 'R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (['L vertebral', 'L V3', 'L V4'], ['L vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R PCA', 'R P2'], ['R P2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1'], ['R M1', 'R Vetebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M1'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'L V3', 'L V4'], ['L V3', 'L V4']),\n",
       " (['R MCA', 'R M1'], ['R M1']),\n",
       " (['R vertebral', 'R V4'], ['R V4']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L ICA', 'L MCA']),\n",
       " (['L ICA', 'L M1'], ['L ICA']),\n",
       " ([], ['R V4', 'L V4']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L ICA', 'R ICA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M1', 'R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'L V4'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (['basilar'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M1'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M1'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'R vertebral', 'basilar'], ['basilar', 'vertebral']),\n",
       " (['R ICA', 'R M2', 'R M3'], ['R M2', 'R M3']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R MCA', 'R M1'], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['basilar', 'L vertebral', 'L V4', 'R vertebral', 'R V4'],\n",
       "  ['R PICA', 'L PICA', 'R V4', 'L V4']),\n",
       " (),\n",
       " (['R M2'], ['R M1', 'R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1', 'R M2', 'R M3'], ['R M1']),\n",
       " (),\n",
       " (['L ICA', 'L MCA', 'L M1'], ['L CCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M1', 'R vertebral'], ['L M1', 'R Vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L PCA', 'L PCA', 'L P2'], ['L P2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'R M1'], ['R ICA']),\n",
       " (['L MCA', 'L M2', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M2'], []),\n",
       " (['L vertebral', 'basilar', 'L PCA'], [' Basilar', 'L vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R MCA', 'R M2', 'R M3'], ['R M2', 'R M3']),\n",
       " (),\n",
       " (),\n",
       " (['R MCA', 'R M1'], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M1'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R ICA']),\n",
       " (),\n",
       " (['R PCA', 'R P1'], ['R P1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'R ACA', 'R M1'], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'R ACA', 'R A2', 'R MCA', 'R M1'], ['R ICA', 'R M1', 'R A2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R MCA', 'R M1'], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['basilar'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R vertebral', 'R PCA', 'R P1'], ['R vertebral']),\n",
       " (),\n",
       " (['R PCA', 'R P2'], ['R P2']),\n",
       " (),\n",
       " (),\n",
       " ([], ['L CCA']),\n",
       " (['L M1', 'R M1'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'L vertebral', 'R vertebral', 'R vertebral'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M1', 'L M2'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R CCA', 'R ICA', 'L vertebral'], ['R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L PCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R MCA', 'R M1', 'R M2', 'R M3'], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'L V2'], ['L M1', 'L Vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (['R vertebral', 'R V3', 'R V4'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'L M1', 'R M1'], ['R ICA', 'R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['basilar'], []),\n",
       " (['L M1'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L MCA']),\n",
       " ([], ['R ICA']),\n",
       " (),\n",
       " (['L ICA', 'R CCA', 'R ICA'], ['R CCA', 'R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['Both Vertebral', ' Basilar', 'R ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['basilar'], ['R Vertebral', 'L Vertebral', 'R basilar', 'L basilar']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['basilar', 'R P1'], ['R P1', 'R basilar']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M1', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1', 'R M2', 'R M2'], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['basilar', 'R PCA'], ['R Basilar', 'R PCA']),\n",
       " (['L MCA', 'L M2'], ['L M1', 'L M2']),\n",
       " (),\n",
       " (['L vertebral', 'L V4', 'L V2', 'L V3', 'R V2', 'R V3'], ['L V2', 'L V3']),\n",
       " (),\n",
       " (['R MCA', 'R M2', 'R M2'], ['R M1', 'R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (['R CCA', 'L vertebral'], ['L vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L ACA', 'L A3'], ['L A3']),\n",
       " (['L M2'], []),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M1'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R vertebral'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R MCA'], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L ICA', 'L A1', 'L M1', 'L M2'],\n",
       "  ['L CCA', 'L ICA', 'L M1', 'L M2', 'L A1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L ICA'], []),\n",
       " (),\n",
       " (['R MCA', 'R M2'], ['R M2']),\n",
       " (),\n",
       " (['R vertebral', 'R V4', 'R P2'], ['R Vertebral', 'R P2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ()]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = to.diffs\n",
    "# diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L V3', 'L V4'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M1'], []),\n",
       " ([], ['R V4', 'L V4']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M1', 'R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L V4'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M2'], ['R M1', 'R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1', 'R M2', 'R M3'], ['R M1']),\n",
       " (),\n",
       " (['L M1'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M2'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R P1'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M1', 'R M1'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M1', 'L M2'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1', 'R M2', 'R M3'], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (['L V2'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R V3', 'R V4'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M1', 'R M1'], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M1'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M1', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1', 'R M2', 'R M2'], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M2'], ['L M1', 'L M2']),\n",
       " (),\n",
       " (['L V4', 'L V2', 'L V3', 'R V2', 'R V3'], ['L V2', 'L V3']),\n",
       " (),\n",
       " (['R M2', 'R M2'], ['R M1', 'R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M2'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R V4', 'R P2'], ['R P2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ()]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to.segments_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(([], ['L MCA', 'L ICA']), 71),\n",
       " ((['L MCA', 'L ICA'], ['L ICA']), 72),\n",
       " (([], ['L vertebral', 'R vertebral']), 73),\n",
       " ((['R ICA', 'L ICA'], []), 87),\n",
       " (([], ['R MCA']), 109),\n",
       " ((['L vertebral'], []), 114),\n",
       " (([], ['R ICA']), 125),\n",
       " (([' basilar'], []), 137),\n",
       " (([' basilar', 'L vertebral', 'R vertebral'], [' vertebral', ' basilar']),\n",
       "  154),\n",
       " ((['R ICA', 'R MCA'], ['R MCA']), 155),\n",
       " ((['L vertebral', ' basilar', 'R vertebral'], ['L vertebral', 'R vertebral']),\n",
       "  183),\n",
       " ((['L MCA', 'L ICA'], ['L CCA']), 203),\n",
       " ((['R ICA'], []), 210),\n",
       " ((['L MCA', 'R vertebral'], ['L MCA']), 218),\n",
       " ((['R ICA', 'R MCA'], ['R ICA']), 231),\n",
       " ((['L MCA'], []), 241),\n",
       " ((['L PCA', ' basilar', 'L vertebral'], ['L vertebral']), 242),\n",
       " (([], ['R ICA']), 273),\n",
       " ((['R ICA', 'R MCA', 'R ACA'], ['R ICA']), 280),\n",
       " (([' basilar'], []), 301),\n",
       " (([], ['R ICA']), 306),\n",
       " (([], ['R ICA']), 315),\n",
       " ((['R PCA', 'R vertebral'], ['R vertebral']), 325),\n",
       " (([], ['L CCA']), 330),\n",
       " ((['L MCA', 'R MCA'], ['L MCA']), 331),\n",
       " ((['L vertebral', 'R vertebral'], []), 334),\n",
       " ((['R ICA'], []), 345),\n",
       " ((['R ICA', 'R CCA', 'L vertebral'], ['R ICA']), 351),\n",
       " (([], ['L PCA']), 359),\n",
       " ((['L vertebral'], ['L MCA']), 367),\n",
       " ((['R vertebral'], []), 387),\n",
       " ((['R ICA', 'L MCA', 'R MCA'], ['R ICA', 'R MCA']), 397),\n",
       " (([' basilar'], []), 403),\n",
       " ((['L MCA'], []), 404),\n",
       " (([], ['L MCA']), 418),\n",
       " (([], ['R ICA']), 419),\n",
       " ((['R ICA', 'R CCA', 'L ICA'], ['R ICA', 'R CCA']), 421),\n",
       " (([], ['R ICA']), 452),\n",
       " (([' basilar'], ['L basilar', 'R basilar']), 462),\n",
       " ((['R PCA', ' basilar'], ['R PCA', 'R basilar']), 470),\n",
       " ((['R PCA', ' basilar'], ['R PCA']), 490),\n",
       " ((['L vertebral', 'R vertebral'], ['L vertebral']), 493),\n",
       " (([], ['R MCA']), 516),\n",
       " ((['R CCA', 'L vertebral'], ['L vertebral']), 523),\n",
       " ((['L MCA'], []), 528),\n",
       " ((['R ICA'], []), 537),\n",
       " ((['R vertebral'], []), 545),\n",
       " ((['L MCA', 'L ICA', 'L ACA'], ['L ICA', 'L MCA', 'L CCA', 'L ACA']), 561),\n",
       " ((['L ICA'], []), 568),\n",
       " ((['R PCA', 'R vertebral'], ['R PCA']), 572)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_vessels_diffs = list(filter(lambda xy:True if xy[0] else False,(zip(to.vessels_diffs,range(len(to.vessels_diffs))))))\n",
    "ind_vessels_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([\"_x000D__x000D_Of note, the perfusion data is likely inaccurate due to patient's poor cardiac output, however, the decreased perfusion within the right anterior watershed zone between the ACA and MCA is likely secondary to decreased systemic perfusion rather than a focal occlusion\"],\n",
       "  31),\n",
       " (['_x000D_No intracranial large vessel occlusion nor ischemic core on CT perfusion'],\n",
       "  58),\n",
       " (['_x000D__x000D_No intracranial large vessel occlusion or stenosis'], 79),\n",
       " (['  Occlusion of a left superior division M3 on CTA with associated 11 mm brain at risk, but no infarct core'],\n",
       "  97),\n",
       " ([' The left posterior inferior cerebellar artery takes off just distal to this occlusion and appears patent'],\n",
       "  114),\n",
       " (['  This is likely the site of occlusion'], 125),\n",
       " (['  There is also 90% occlusion of the left subclavian artery origin which reconstitutes distally'],\n",
       "  137),\n",
       " ([' Distal left MCA branches paucity/occlusion with reconstitution distally (axial series 10 image 180-194 and 145-165)'],\n",
       "  168),\n",
       " (['_x000D__x000D_  Left MCA territory posterior M3 branches become occluded/nonopacified distally matching with an area of geographic posterior vasculature  and the infarct core on perfusion images'],\n",
       "  193),\n",
       " (['ADDENDUM:Upon further review, there is occlusion of the proximal portion of the right internal carotid artery that extends from C3 to the distal cavernous segment'],\n",
       "  238),\n",
       " (['_x000D__x000D_  Nondiagnostic angiographic study to evaluate for large vessel occlusion due to failure of the contrast bolus'],\n",
       "  293),\n",
       " (['_x000D_ Nonocclusive thrombosis of the proximal left M3 with distal reconstitution',\n",
       "   '  There is 1 or possibly 2 areas of stenosis/focal partial occlusion in the left M2 branch near the area of prior infarct with distal reconstitution'],\n",
       "  299),\n",
       " ([' These were both occluded on the prior CTA'], 334),\n",
       " (['_x000D_No large intracranial artery occlusion'], 368),\n",
       " ([' Correlation with the prior study demonstrates likely corresponds to a distal M2 stenosis secondary to long-term sequela of previous seen occlusion from 05/09/2016_x000D__x000D_Dictated by: Kristy Lea Ratkowski, M'],\n",
       "  415),\n",
       " (['_x000D_  Severely diminished flow at the petrous portion of the right internal carotid artery, which becomes completely occluded at the cavernous segment without evidence of opacification distally, with non-opacification of the right anterior and middle cerebral artery distribution vessels'],\n",
       "  419),\n",
       " (['_x000D__x000D_Hypoperfusion of the left parietal lobe without ischemic core, which may correlate with an M3 segment occlusion'],\n",
       "  452),\n",
       " ([' _x000D__x000D_  Left superior cerebellar artery originates proximal to the occlusion, and is opacified'],\n",
       "  470),\n",
       " (['_x000D_  Left V4 segment occlusion'], 477),\n",
       " (['_x000D__x000D_There is an occlusion in the left posterior parietal or angular artery_x000D__x000D_This report was telephoned by Dr'],\n",
       "  484)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmatched_occlusion_sents = to.get_unmatched_occlusions()\n",
    "unmatched_occlusion_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    '  _x000D_Non-opacification of the right vertebral artery at its origin with reconstitution distally')],\n",
       "  1),\n",
       " ([('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    '  _x000D__x000D_Abrupt cut off of the left vertebral artery slightly before junction with the right at the level of V3-V4 consistent with post surgical ligation or dissection with backfilling and retrograde filling of left PICA from the basilar confluence'),\n",
       "   ('uncertain occlusion',\n",
       "    'V3',\n",
       "    '  _x000D__x000D_Abrupt cut off of the left vertebral artery slightly before junction with the right at the level of V3-V4 consistent with post surgical ligation or dissection with backfilling and retrograde filling of left PICA from the basilar confluence'),\n",
       "   ('uncertain occlusion',\n",
       "    'V4',\n",
       "    '  _x000D__x000D_Abrupt cut off of the left vertebral artery slightly before junction with the right at the level of V3-V4 consistent with post surgical ligation or dissection with backfilling and retrograde filling of left PICA from the basilar confluence')],\n",
       "  12),\n",
       " ([('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    '_x000D_  Nonopacification of the left vertebral artery at its origin and within the left V3 segment with distal reconstitution related to retrograde filling from the right vertebral artery'),\n",
       "   ('uncertain occlusion',\n",
       "    'V3',\n",
       "    '_x000D_  Nonopacification of the left vertebral artery at its origin and within the left V3 segment with distal reconstitution related to retrograde filling from the right vertebral artery')],\n",
       "  26),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D__x000D_Nonopacification of the distal right common carotid artery and right internal carotid artery with filling of the right external carotid artery via reversal flow in the right internal maxillary artery'),\n",
       "   ('uncertain occlusion',\n",
       "    'CCA',\n",
       "    '_x000D__x000D_Nonopacification of the distal right common carotid artery and right internal carotid artery with filling of the right external carotid artery via reversal flow in the right internal maxillary artery'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D__x000D_Nonopacification of the distal right common carotid artery and right internal carotid artery with filling of the right external carotid artery via reversal flow in the right internal maxillary artery')],\n",
       "  31),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D__x000D_Non-opacification of the right internal carotid artery extending to the supraclinoid segment with collateral filling of the right MCA and ACA vessels'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D__x000D_Non-opacification of the right internal carotid artery extending to the supraclinoid segment with collateral filling of the right MCA and ACA vessels'),\n",
       "   ('uncertain occlusion',\n",
       "    'MCA',\n",
       "    '_x000D__x000D_Non-opacification of the right internal carotid artery extending to the supraclinoid segment with collateral filling of the right MCA and ACA vessels')],\n",
       "  40),\n",
       " ([('uncertain occlusion',\n",
       "    'M1',\n",
       "    '_x000D__x000D_  Focal nonopacification or near-total occlusion of the right M1 segment with immediate reconstitution')],\n",
       "  56),\n",
       " ([('uncertain occlusion',\n",
       "    'basilar',\n",
       "    '_x000D_ Small eccentric filling defect in the mid basilar artery may represent a web or eccentric thrombus')],\n",
       "  101),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D_ Significant stenosis of the right internal carotid artery just above the bifurcation with filling defects seen in the right carotid artery slightly above the bifurcation extending to the level of petrous section with nonopacification of the petrous and cavernous section of the right internal carotid artery and distal reconstitution from collaterals'),\n",
       "   ('uncertain occlusion',\n",
       "    'MCA',\n",
       "    ' _x000D_ Attenuated right MCA with nonopacification of the distal M1 and M2 segments'),\n",
       "   ('uncertain occlusion',\n",
       "    'M1',\n",
       "    ' _x000D_ Attenuated right MCA with nonopacification of the distal M1 and M2 segments'),\n",
       "   ('uncertain occlusion',\n",
       "    'M2',\n",
       "    ' _x000D_ Attenuated right MCA with nonopacification of the distal M1 and M2 segments')],\n",
       "  109),\n",
       " ([('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    '_x000D__x000D_Non-opacification of the right vertebral artery from the level of the 2')],\n",
       "  119),\n",
       " ([('uncertain occlusion',\n",
       "    'MCA',\n",
       "    '_x000D_Hyperdense left MCA (calcified thrombus) with abrupt cut off of proximal left M2 just beyond the MCA bifurcation seen on angiography compatible with large vessel occlusion'),\n",
       "   ('uncertain occlusion',\n",
       "    'M2',\n",
       "    '_x000D_Hyperdense left MCA (calcified thrombus) with abrupt cut off of proximal left M2 just beyond the MCA bifurcation seen on angiography compatible with large vessel occlusion')],\n",
       "  135),\n",
       " ([('uncertain occlusion',\n",
       "    'basilar',\n",
       "    '_x000D__x000D_  Small filling defect in the tip of the basilar artery, concerning for a nonocclusive thrombus')],\n",
       "  137),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D_ Nonopacification of the right internal carotid artery from the bifurcation into the distal cavernous branch with multifocal high-grade stenosis/occlusion in the anterior branch of the M2 and M3 consistent with tandem lesions'),\n",
       "   ('uncertain occlusion',\n",
       "    'M2',\n",
       "    '_x000D_ Nonopacification of the right internal carotid artery from the bifurcation into the distal cavernous branch with multifocal high-grade stenosis/occlusion in the anterior branch of the M2 and M3 consistent with tandem lesions'),\n",
       "   ('uncertain occlusion',\n",
       "    'M3',\n",
       "    '_x000D_ Nonopacification of the right internal carotid artery from the bifurcation into the distal cavernous branch with multifocal high-grade stenosis/occlusion in the anterior branch of the M2 and M3 consistent with tandem lesions'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D_ Nonopacification of the right internal carotid artery from the bifurcation into the distal cavernous branch with multifocal high-grade stenosis/occlusion in the anterior branch of the M2 and M3 consistent with tandem lesions'),\n",
       "   ('uncertain occlusion',\n",
       "    'M2',\n",
       "    '_x000D_ Nonopacification of the right internal carotid artery from the bifurcation into the distal cavernous branch with multifocal high-grade stenosis/occlusion in the anterior branch of the M2 and M3 consistent with tandem lesions'),\n",
       "   ('uncertain occlusion',\n",
       "    'M3',\n",
       "    '_x000D_ Nonopacification of the right internal carotid artery from the bifurcation into the distal cavernous branch with multifocal high-grade stenosis/occlusion in the anterior branch of the M2 and M3 consistent with tandem lesions'),\n",
       "   ('uncertain occlusion',\n",
       "    'CCA',\n",
       "    ' _x000D_  Nonopacification of the left common carotid artery from its origin to the bifurcation, likely related to chronic occlusion'),\n",
       "   ('uncertain occlusion',\n",
       "    'CCA',\n",
       "    ' _x000D_  Nonopacification of the left common carotid artery from its origin to the bifurcation, likely related to chronic occlusion')],\n",
       "  155),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '   Nonopacification of left carotid artery shortly after its origin extending throughout the left internal carotid artery course')],\n",
       "  244),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  _x000D_Abrupt cut off of the left internal carotid artery shortly after its take off from the bifurcation with good reconstitution beginning at the cavernous segment, good contrast opacification of the distal left ICA terminus, and good contrast opacification of the left MCA and ACA arteries'),\n",
       "   ('uncertain occlusion',\n",
       "    'MCA',\n",
       "    '  _x000D_Abrupt cut off of the left internal carotid artery shortly after its take off from the bifurcation with good reconstitution beginning at the cavernous segment, good contrast opacification of the distal left ICA terminus, and good contrast opacification of the left MCA and ACA arteries')],\n",
       "  249),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D__x000D_  High-grade stenosis of the proximal right internal carotid artery shortly after its origin secondary to a dense calcified atherosclerotic plaque with l non-opacification of the cervical portion of the right internal carotid artery shortly after the plaque to the level of the cavernous segment at which time opacification of the ICA and its ACA and MCA branches is noted secondary to collateral flow'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D__x000D_  High-grade stenosis of the proximal right internal carotid artery shortly after its origin secondary to a dense calcified atherosclerotic plaque with l non-opacification of the cervical portion of the right internal carotid artery shortly after the plaque to the level of the cavernous segment at which time opacification of the ICA and its ACA and MCA branches is noted secondary to collateral flow')],\n",
       "  273),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  Please refer to the subsequent MRA evaluation for detailed discussion of the nonopacification of the right ICA'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  Please refer to the subsequent MRA evaluation for detailed discussion of the nonopacification of the right ICA'),\n",
       "   ('uncertain occlusion',\n",
       "    'CCA',\n",
       "    '  Of note the digital suboptimal images demonstrate nonopacification of the visualized CCA and ICA on the right'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  Of note the digital suboptimal images demonstrate nonopacification of the visualized CCA and ICA on the right'),\n",
       "   ('uncertain occlusion',\n",
       "    'CCA',\n",
       "    '  Of note the digital suboptimal images demonstrate nonopacification of the visualized CCA and ICA on the right'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  Of note the digital suboptimal images demonstrate nonopacification of the visualized CCA and ICA on the right')],\n",
       "  293),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    ' Filling defect within the proximal right internal carotid artery just beyond the bifurcation likely represents thrombus'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  The distal right ACA branches are not opacified, and there is non-opacification of portions of the distal right middle cerebral artery suggestive of distal arterial embolic occlusions in the setting of the right internal carotid artery clot'),\n",
       "   ('uncertain occlusion',\n",
       "    'ACA',\n",
       "    '  The distal right ACA branches are not opacified, and there is non-opacification of portions of the distal right middle cerebral artery suggestive of distal arterial embolic occlusions in the setting of the right internal carotid artery clot')],\n",
       "  315),\n",
       " ([('uncertain occlusion',\n",
       "    'M1',\n",
       "    '  There is nonopacification of the left M1 and proximal M2 branches'),\n",
       "   ('uncertain occlusion',\n",
       "    'M2',\n",
       "    '  There is nonopacification of the left M1 and proximal M2 branches')],\n",
       "  358),\n",
       " ([('uncertain occlusion',\n",
       "    'MCA',\n",
       "    '_x000D_Type A aortic dissection with nonopacification of the right carotid artery and early findings of right MCA distribution stroke on non contrast CT')],\n",
       "  363),\n",
       " ([('uncertain occlusion',\n",
       "    'PCA',\n",
       "    \"_x000D_Nonopacification of both PCAs distal to the P1 segments in this patient's with old bilateral PCA distribution infarcts\"),\n",
       "   ('uncertain occlusion',\n",
       "    'P1',\n",
       "    \"_x000D_Nonopacification of both PCAs distal to the P1 segments in this patient's with old bilateral PCA distribution infarcts\"),\n",
       "   ('uncertain occlusion',\n",
       "    'PCA',\n",
       "    \"_x000D_Nonopacification of both PCAs distal to the P1 segments in this patient's with old bilateral PCA distribution infarcts\"),\n",
       "   ('uncertain occlusion',\n",
       "    'P1',\n",
       "    \"_x000D_Nonopacification of both PCAs distal to the P1 segments in this patient's with old bilateral PCA distribution infarcts\")],\n",
       "  408),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D_  Severely diminished flow at the petrous portion of the right internal carotid artery, which becomes completely occluded at the cavernous segment without evidence of opacification distally, with non-opacification of the right anterior and middle cerebral artery distribution vessels')],\n",
       "  419),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D__x000D_  Nonopacification of the the left internal carotid artery from the distal cervical segment extending to the terminus consistent with occlusion of the internal carotid artery terminus'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  Filling defect in the petrous segment of the left internal carotid artery could represent thrombus or may be related to slow flow')],\n",
       "  463),\n",
       " ([('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    ' _x000D__x000D_ Nonopacification of the proximal and distal right vertebral artery, and moderate narrowing of the basilar artery and distal P2 segments, likely represents multifocal atherosclerotic narrowing'),\n",
       "   ('uncertain occlusion',\n",
       "    'basilar',\n",
       "    ' _x000D__x000D_ Nonopacification of the proximal and distal right vertebral artery, and moderate narrowing of the basilar artery and distal P2 segments, likely represents multifocal atherosclerotic narrowing'),\n",
       "   ('uncertain occlusion',\n",
       "    'P2',\n",
       "    ' _x000D__x000D_ Nonopacification of the proximal and distal right vertebral artery, and moderate narrowing of the basilar artery and distal P2 segments, likely represents multifocal atherosclerotic narrowing'),\n",
       "   ('uncertain occlusion',\n",
       "    'P2',\n",
       "    ' _x000D__x000D_ Nonopacification of the proximal and distal right vertebral artery, and moderate narrowing of the basilar artery and distal P2 segments, likely represents multifocal atherosclerotic narrowing')],\n",
       "  485),\n",
       " ([('uncertain occlusion',\n",
       "    'M1',\n",
       "    '_x000D__x000D_  Abrupt cut off of the right M1 bifurcation with evidence of decreased perfusion of the right middle cerebral artery distribution')],\n",
       "  516),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D__x000D_ Linear, intraluminal filling defect within the proximal left internal carotid artery is suspicious for a thrombus'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D__x000D_ Linear, intraluminal filling defect within the proximal left internal carotid artery is suspicious for a thrombus')],\n",
       "  521),\n",
       " ([('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    '_x000D__x000D_  Occlusion of the left vertebral artery at its origin with reconstitution at the C2 level via collaterals then with nonopacification prior to the formation of the basilar artery')],\n",
       "  523),\n",
       " ([('uncertain occlusion',\n",
       "    'M2',\n",
       "    '_x000D_ Nonopacification of one of the left proximal M2 branches suggestive with occlusion'),\n",
       "   ('uncertain occlusion',\n",
       "    'M2',\n",
       "    '  There are irregularities of several M2 branches and nonopacification of some left M3 segments, primarily affecting the anterior division, are noted'),\n",
       "   ('uncertain occlusion',\n",
       "    'M3',\n",
       "    '  There are irregularities of several M2 branches and nonopacification of some left M3 segments, primarily affecting the anterior division, are noted')],\n",
       "  528),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '_x000D__x000D_  Occlusion of the right internal carotid artery from just past the bifurcation to the ICA terminus, and nonopacification of the right M1 segment'),\n",
       "   ('uncertain occlusion',\n",
       "    'M1',\n",
       "    '_x000D__x000D_  Occlusion of the right internal carotid artery from just past the bifurcation to the ICA terminus, and nonopacification of the right M1 segment')],\n",
       "  533),\n",
       " ([('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    '_x000D__x000D_Nonopacification of the left vertebral artery origin with retrograde filling distally')],\n",
       "  560)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertain_occlusions = to.get_uncertain_occlusions()\n",
    "#uncertain_occlusions= [x for x in uncertain_occlusions if x]\n",
    "uncertain_occlusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[419, 452, 137, 334, 114, 125]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_diffs = [ind for sent,ind in ind_vessels_diffs]\n",
    "ind_unmatched = [ind for sent,ind in unmatched_occlusion_sents]\n",
    "list(set(np.ndarray.tolist(to.occl_status_diffs[0]))&set(ind_unmatched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(([], ['L MCA', 'L ICA']), 71),\n",
       " ((['L MCA', 'L ICA'], ['L ICA']), 72),\n",
       " (([], ['L vertebral', 'R vertebral']), 73),\n",
       " ((['R ICA', 'L ICA'], []), 87),\n",
       " (([], ['R MCA']), 109),\n",
       " ((['L vertebral'], []), 114),\n",
       " (([], ['R ICA']), 125),\n",
       " (([' basilar'], []), 137),\n",
       " (([' basilar', 'L vertebral', 'R vertebral'], [' vertebral', ' basilar']),\n",
       "  154),\n",
       " ((['R ICA', 'R MCA'], ['R MCA']), 155),\n",
       " ((['L vertebral', ' basilar', 'R vertebral'], ['L vertebral', 'R vertebral']),\n",
       "  183),\n",
       " ((['L MCA', 'L ICA'], ['L CCA']), 203),\n",
       " ((['R ICA'], []), 210),\n",
       " ((['L MCA', 'R vertebral'], ['L MCA']), 218),\n",
       " ((['R ICA', 'R MCA'], ['R ICA']), 231),\n",
       " ((['L MCA'], []), 241),\n",
       " ((['L PCA', ' basilar', 'L vertebral'], ['L vertebral']), 242),\n",
       " (([], ['R ICA']), 273),\n",
       " ((['R ICA', 'R MCA', 'R ACA'], ['R ICA']), 280),\n",
       " (([' basilar'], []), 301),\n",
       " (([], ['R ICA']), 306),\n",
       " (([], ['R ICA']), 315),\n",
       " ((['R PCA', 'R vertebral'], ['R vertebral']), 325),\n",
       " (([], ['L CCA']), 330),\n",
       " ((['L MCA', 'R MCA'], ['L MCA']), 331),\n",
       " ((['L vertebral', 'R vertebral'], []), 334),\n",
       " ((['R ICA'], []), 345),\n",
       " ((['R ICA', 'R CCA', 'L vertebral'], ['R ICA']), 351),\n",
       " (([], ['L PCA']), 359),\n",
       " ((['L vertebral'], ['L MCA']), 367),\n",
       " ((['R vertebral'], []), 387),\n",
       " ((['R ICA', 'L MCA', 'R MCA'], ['R ICA', 'R MCA']), 397),\n",
       " (([' basilar'], []), 403),\n",
       " ((['L MCA'], []), 404),\n",
       " (([], ['L MCA']), 418),\n",
       " (([], ['R ICA']), 419),\n",
       " ((['R ICA', 'R CCA', 'L ICA'], ['R ICA', 'R CCA']), 421),\n",
       " (([], ['R ICA']), 452),\n",
       " (([' basilar'], ['L basilar', 'R basilar']), 462),\n",
       " ((['R PCA', ' basilar'], ['R PCA', 'R basilar']), 470),\n",
       " ((['R PCA', ' basilar'], ['R PCA']), 490),\n",
       " ((['L vertebral', 'R vertebral'], ['L vertebral']), 493),\n",
       " (([], ['R MCA']), 516),\n",
       " ((['R CCA', 'L vertebral'], ['L vertebral']), 523),\n",
       " ((['L MCA'], []), 528),\n",
       " ((['R ICA'], []), 537),\n",
       " ((['R vertebral'], []), 545),\n",
       " ((['L MCA', 'L ICA', 'L ACA'], ['L ICA', 'L MCA', 'L CCA', 'L ACA']), 561),\n",
       " ((['L ICA'], []), 568),\n",
       " ((['R PCA', 'R vertebral'], ['R PCA']), 572)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_vessels_diffs\n",
    "#5 missed occls 419,452, 125, 516, 109\n",
    "#4 falsely called occl 137, 334, 114, 528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unmatched_occlusion_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ind_vessels_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[419, 516, 137, 109, 528]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_diffs = [ind for sent,ind in ind_vessels_diffs]\n",
    "ind_uncertain = [ind for sent,ind in uncertain_occlusions]\n",
    "list(set(np.ndarray.tolist(to.occl_status_diffs[0]))&set(ind_uncertain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncertain_occlusions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
