{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Organization</th>\n",
       "      <th>Point of Care</th>\n",
       "      <th>Accession Number</th>\n",
       "      <th>Modality</th>\n",
       "      <th>Exam Code</th>\n",
       "      <th>Exam Description</th>\n",
       "      <th>CPT Code</th>\n",
       "      <th>Report Text</th>\n",
       "      <th>LVO</th>\n",
       "      <th>Side</th>\n",
       "      <th>...</th>\n",
       "      <th>EVT</th>\n",
       "      <th>EVT Occlusion</th>\n",
       "      <th>EVT Report</th>\n",
       "      <th>TICI Score</th>\n",
       "      <th>Ordered By</th>\n",
       "      <th>Ordered Date</th>\n",
       "      <th>Exam Completed Date</th>\n",
       "      <th>Report Created Date</th>\n",
       "      <th>Report Finalized By</th>\n",
       "      <th>Report Finalized Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 0105</td>\n",
       "      <td>55427372</td>\n",
       "      <td>CTA</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>70496</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LEVINE, MARK D.</td>\n",
       "      <td>2020-09-20 16:51:13</td>\n",
       "      <td>2020-09-20 16:25:00</td>\n",
       "      <td>2020-09-20 17:24:56</td>\n",
       "      <td>Eldaya, Rami W</td>\n",
       "      <td>2020-09-20 17:51:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 0082</td>\n",
       "      <td>55427150</td>\n",
       "      <td>CTA</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>70496</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KOLLEF, MARIN H.</td>\n",
       "      <td>2020-09-20 15:24:17</td>\n",
       "      <td>2020-09-20 15:15:00</td>\n",
       "      <td>2020-09-20 15:28:59</td>\n",
       "      <td>Goyal, Manu Shri</td>\n",
       "      <td>2020-09-20 19:32:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 94ICU</td>\n",
       "      <td>55427201</td>\n",
       "      <td>CTA</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>70496</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GRUBB, LAURIE</td>\n",
       "      <td>2020-09-20 14:59:48</td>\n",
       "      <td>2020-09-20 14:50:00</td>\n",
       "      <td>2020-09-20 15:10:57</td>\n",
       "      <td>Goyal, Manu Shri</td>\n",
       "      <td>2020-09-20 20:04:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 94ICU</td>\n",
       "      <td>55426263</td>\n",
       "      <td>CTA</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>70496</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>1</td>\n",
       "      <td>R/L</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FREER, JAMES MATTHEW</td>\n",
       "      <td>2020-09-20 02:53:03</td>\n",
       "      <td>2020-09-20 02:35:00</td>\n",
       "      <td>2020-09-20 03:05:39</td>\n",
       "      <td>Goyal, Manu Shri</td>\n",
       "      <td>2020-09-20 09:56:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 0082</td>\n",
       "      <td>55408274</td>\n",
       "      <td>CTA</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>70496</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LOVELESS, REBECCA ANNE</td>\n",
       "      <td>2020-09-17 03:05:10</td>\n",
       "      <td>2020-09-17 02:20:00</td>\n",
       "      <td>2020-09-17 03:06:56</td>\n",
       "      <td>Vo, Katie D.</td>\n",
       "      <td>2020-09-17 07:33:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Organization Point of Care  Accession Number Modality Exam Code  \\\n",
       "0          BJH      BJH 0105          55427372      CTA    IMG575   \n",
       "1          BJH      BJH 0082          55427150      CTA    IMG575   \n",
       "2          BJH     BJH 94ICU          55427201      CTA    IMG575   \n",
       "3          BJH     BJH 94ICU          55426263      CTA    IMG575   \n",
       "4          BJH      BJH 0082          55408274      CTA    IMG575   \n",
       "\n",
       "           Exam Description  CPT Code  \\\n",
       "0  CTA/CTP RAPID STROKE (C)     70496   \n",
       "1  CTA/CTP RAPID STROKE (C)     70496   \n",
       "2  CTA/CTP RAPID STROKE (C)     70496   \n",
       "3  CTA/CTP RAPID STROKE (C)     70496   \n",
       "4  CTA/CTP RAPID STROKE (C)     70496   \n",
       "\n",
       "                                         Report Text  LVO Side  ...    EVT  \\\n",
       "0  EXAMINATION: Computed tomography angiography (...    0  NaN  ...  False   \n",
       "1  EXAMINATION: Computed tomography angiography (...    0  NaN  ...  False   \n",
       "2  EXAMINATION: Computed tomography angiography (...    0  NaN  ...  False   \n",
       "3  EXAMINATION: Computed tomography angiography (...    1  R/L  ...  False   \n",
       "4  EXAMINATION: Computed tomography angiography (...    0  NaN  ...  False   \n",
       "\n",
       "   EVT Occlusion  EVT Report TICI Score              Ordered By  \\\n",
       "0            NaN         NaN        NaN         LEVINE, MARK D.   \n",
       "1            NaN         NaN        NaN        KOLLEF, MARIN H.   \n",
       "2            NaN         NaN        NaN           GRUBB, LAURIE   \n",
       "3            NaN         NaN        NaN    FREER, JAMES MATTHEW   \n",
       "4            NaN         NaN        NaN  LOVELESS, REBECCA ANNE   \n",
       "\n",
       "         Ordered Date Exam Completed Date Report Created Date  \\\n",
       "0 2020-09-20 16:51:13 2020-09-20 16:25:00 2020-09-20 17:24:56   \n",
       "1 2020-09-20 15:24:17 2020-09-20 15:15:00 2020-09-20 15:28:59   \n",
       "2 2020-09-20 14:59:48 2020-09-20 14:50:00 2020-09-20 15:10:57   \n",
       "3 2020-09-20 02:53:03 2020-09-20 02:35:00 2020-09-20 03:05:39   \n",
       "4 2020-09-17 03:05:10 2020-09-17 02:20:00 2020-09-17 03:06:56   \n",
       "\n",
       "   Report Finalized By  Report Finalized Date  \n",
       "0       Eldaya, Rami W    2020-09-20 17:51:50  \n",
       "1     Goyal, Manu Shri    2020-09-20 19:32:25  \n",
       "2     Goyal, Manu Shri    2020-09-20 20:04:06  \n",
       "3     Goyal, Manu Shri    2020-09-20 09:56:07  \n",
       "4         Vo, Katie D.    2020-09-17 07:33:09  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_dir = \"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/LVOs_19_20_Zohair_Aaron/CTAP_19_20_LVO_Test_Data_Final.xls\"\n",
    "raw_df=pd.read_excel(io=raw_dir, sheet_name=\"Completed\")\n",
    "# print(raw_df.columns)\n",
    "raw_df.head()\n",
    "#Excel sheet - 2\n",
    "#raw_df[\"RAPID Core\"][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim_df = raw_df[[\"LVO\",\"Side\",\"site occlusion\",\"CTA CTP Report Text\",\"RAPID Core\",\"RAPID Penumbra\"]]\n",
    "# trim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LVO</th>\n",
       "      <th>Side</th>\n",
       "      <th>site occlusion</th>\n",
       "      <th>Report Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>R/L</td>\n",
       "      <td>M1/Vertebral</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LVO Side site occlusion                                        Report Text\n",
       "0    0  NaN            NaN  EXAMINATION: Computed tomography angiography (...\n",
       "1    0  NaN            NaN  EXAMINATION: Computed tomography angiography (...\n",
       "2    0  NaN            NaN  EXAMINATION: Computed tomography angiography (...\n",
       "3    1  R/L   M1/Vertebral  EXAMINATION: Computed tomography angiography (...\n",
       "4    0  NaN            NaN  EXAMINATION: Computed tomography angiography (..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_df = raw_df[[\"LVO\",\"Side\",\"site occlusion\",\"Report Text\"]]\n",
    "trim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncertain LVO 1?\n",
    "# trim_df.iloc[504]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter if LVO uncertain\n",
    "# filtered_df = trim_df[~trim_df[\"LVO\"].str.contains(\"?\",regex=False).fillna(False)]\n",
    "# filtered_df.iloc[50]\n",
    "filtered_df = trim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that reads the angio report cerebral artery table\n",
    "#from pandas dataframe\n",
    "#and outputs a python dictionary\n",
    "#\n",
    "#If \"no occlusion\" is present for a given artery, value is marked as 0\n",
    "#Otherwise, the program looks to free-body text for clarification\n",
    "#These values are marked with 'None'\n",
    "import copy\n",
    "import re\n",
    "\n",
    "class ca_templates_reader:\n",
    "    \n",
    "    def __init__(self,df_column):\n",
    "        \n",
    "        self.df_column = df_column\n",
    "        \n",
    "        #cerebral arteries dictionary\n",
    "        self.main_dict = {\n",
    "            \"L CCA\": None,\n",
    "            \"L carotid bifurcation\": None,\n",
    "            \"L ICA proximal\": None,\n",
    "            \"L ICA distal\": None,\n",
    "            \"L ICA terminus\": None,\n",
    "            \"L M1\": None,\n",
    "            \"L M2 branches\": None,\n",
    "            \"L A2\": None,\n",
    "            \"R CCA\": None,\n",
    "            \"R carotid bifurcation\": None,\n",
    "            \"R ICA proximal\": None,\n",
    "            \"R ICA distal\": None,\n",
    "            \"R ICA terminus\": None,\n",
    "            \"R M1\": None,\n",
    "            \"R M2 branches\": None,\n",
    "            \"R A2\": None,\n",
    "            \"L Vertebral Artery\": None,\n",
    "            \"R Vertebral Artery\": None,\n",
    "            \"Basilar Artery\": None,\n",
    "            \"L PCA\": None,\n",
    "            \"R PCA\": None\n",
    "\n",
    "        }\n",
    "        \n",
    "        self.ca_pattern_dict = copy.deepcopy(self.main_dict)\n",
    "    \n",
    "    def compile_regex(self):\n",
    "        #Compiling regex patterns of the cerebral artery strings\n",
    "        #For optimal performance\n",
    "        \n",
    "        for ca in self.ca_pattern_dict:\n",
    "            self.ca_pattern_dict[ca]=re.compile(ca+':.*\\n')\n",
    "        \n",
    "    \n",
    "    def read_ca_template(self, text):\n",
    "        \n",
    "        #Example cerebral arteries template line:\n",
    "        #R CCA: No occlusion or significant stenosis\n",
    "\n",
    "        no_occlusion = re.compile('(n|N)o occlusion')\n",
    "    #     for file in raw_text_files:\n",
    "        curr_dict = copy.deepcopy(self.main_dict)\n",
    "        for ca in curr_dict:\n",
    "            #Searching for template text for each cerebral artery\n",
    "            #Some follow up reports may not have the template\n",
    "            \n",
    "            search = self.ca_pattern_dict[ca].search(text)\n",
    "            if search is not None:\n",
    "                #string of ca template line\n",
    "                sent = search.group()\n",
    "                #print(sent)\n",
    "                #Expect match most cases, change dict value to 0 (no occlusion)\n",
    "                #If 'no occlusion' not in cerebral a. desc, leave with None value (default flag)\n",
    "                #Will determine later\n",
    "                if no_occlusion.search(sent) is not None:\n",
    "                    curr_dict[ca] = 0\n",
    "        return curr_dict\n",
    "    #         ca_dicts.append(curr_dict)\n",
    "    \n",
    "    def read_templates(self):\n",
    "        #List to store the cerebral artery dictionaries for report\n",
    "        ca_dicts = []\n",
    "        for report in self.df_column:\n",
    "            ca_dicts.append(self.read_ca_template(report))\n",
    "        return ca_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = ca_templates_reader(filtered_df[\"Report Text\"])\n",
    "ctr.compile_regex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class report_text_extractor:\n",
    "    \n",
    "    def __init__(self, df_column):\n",
    "        self.df_column = df_column\n",
    "        self.report_template_pattern = re.compile('Left anterior circulation:[\\s\\S]*IMPRESSION', re.I)\n",
    "        #List of all cerebral artery templates from the reports\n",
    "        self.report_templates = self.extract_from_reports(self.report_template_pattern)\n",
    "\n",
    "\n",
    "        self.impression_pattern = re.compile('IMPRESSION:[\\s\\S]*', re.I)\n",
    "        #List of all impressions from the reports\n",
    "        self.impressions = self.extract_from_reports(self.impression_pattern)\n",
    "        \n",
    "        #List of all CT Perfusion sections from reports\n",
    "        self.ctp_pattern = re.compile('CT Perfusion:[\\s\\S]*IMPRESSION', re.I)\n",
    "        self.ctp_sections = self.extract_from_reports(self.ctp_pattern)\n",
    "        \n",
    "        \n",
    "    def get_report_templates(self):\n",
    "        return self.report_templates\n",
    "    \n",
    "    def get_impressions(self):\n",
    "        return self.impressions\n",
    "\n",
    "    def get_ctps(self):\n",
    "        return self.ctp_sections\n",
    "    \n",
    "    def extract_from_reports(self,pattern):\n",
    "        report_extractions = []\n",
    "        for report in self.df_column:\n",
    "            search = pattern.search(report)\n",
    "            if search:\n",
    "                report_extractions.append(search.group())\n",
    "            else:\n",
    "                report_extractions.append(None)\n",
    "        return report_extractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left anterior circulation:\r\n",
      "L CCA: no occlusion or significant stenosis\r\n",
      "L carotid bifurcation: no occlusion or significant stenosis\r\n",
      "L ICA proximal: no occlusion or significant stenosis\r\n",
      "L ICA distal: no occlusion or significant stenosis\r\n",
      "L ICA terminus: no occlusion or significant stenosis\r\n",
      "L M1: no occlusion or significant stenosis\r\n",
      "L M2 branches: no occlusion or significant stenosis\r\n",
      "L A2: no occlusion or significant stenosis\r\n",
      "\r\n",
      "Right anterior circulation:\r\n",
      "R CCA: no occlusion or significant stenosis\r\n",
      "R carotid bifurcation: no occlusion or significant stenosis\r\n",
      "R ICA proximal: no occlusion or significant stenosis\r\n",
      "R ICA distal: no occlusion or significant stenosis\r\n",
      "R ICA terminus: no occlusion or significant stenosis\r\n",
      "R M1: no occlusion or significant stenosis\r\n",
      "R M2 branches: no occlusion or significant stenosis\r\n",
      "R A2: no occlusion or significant stenosis\r\n",
      " \r\n",
      "Posterior circulation:\r\n",
      "L Vertebral Artery: no occlusion or significant stenosis\r\n",
      "R Vertebral Artery: no occlusion or significant stenosis\r\n",
      "Basilar Artery: no occlusion or significant stenosis\r\n",
      "L PCA: no occlusion or significant stenosis\r\n",
      "R PCA: no occlusion or significant stenosis\r\n",
      "\r\n",
      "No cerebral aneurysm is seen. There is no evidence for an arteriovenous malformation. There is no suspicious cervical lymphadenopathy. There is no significant cervical spondylosis. Limited views of the lung apices are normal.\r\n",
      "\r\n",
      "CT Perfusion:\r\n",
      "\r\n",
      "Estimated ischemic core volume (rCBF < 0.3): 0 mL\r\n",
      "Estimated hypoperfusion volume (Tmax > 6 sec): 0 mL\r\n",
      "\r\n",
      "IMPRESSION\n",
      "//////////////////////////////////////////////////\n",
      "IMPRESSION:\r\n",
      "1.  No CT evidence of stroke.\r\n",
      "2.  No significant carotid artery stenosis.\r\n",
      "\r\n",
      "The Non Critical results were discussed with Dr. Farkas by Dr. Glasser on 01/30/2020 at at 9:50 PM.  \r\n",
      "\r\n",
      "Dictated by: Matthew F Glasser, M.D. Ph.D.\r\n",
      "\r\n",
      "The radiology attending physician has personally reviewed this study, and had reviewed and/or edited this written report and agrees with it.\r\n",
      "\r\n",
      "Electronically signed by: Cyrus A Raji, M.D, PHD\n",
      "//////////////////////////////////////////////////\n"
     ]
    }
   ],
   "source": [
    "rte = report_text_extractor(filtered_df[\"Report Text\"])\n",
    "templates = rte.get_report_templates()\n",
    "impressions = rte.get_impressions()\n",
    "ctps = rte.get_ctps()\n",
    "print(templates[300])\n",
    "print('/'*50)\n",
    "print(impressions[300])\n",
    "print('/'*50)\n",
    "#Extract ischemic core volume and hypoperfusion values in mLprint(ctps[522])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract ischemic core and hypoperfusion volumes in mL\n",
    "class ctp_volumes_extractor:\n",
    "    def __init__(self,ctp_sections_list):\n",
    "        self.ctps = ctp_sections_list\n",
    "        self.volume_pattern = re.compile('[0-9]* *m[Ll]')\n",
    "        self.volumes = []\n",
    "        \n",
    "    def extract_volumes(self):\n",
    "        for ctp in self.ctps:\n",
    "            if ctp:\n",
    "                #List of 2 strings ex. ['0 mL', '146 mL']\n",
    "                search = self.volume_pattern.findall(ctp)\n",
    "                if search:                      \n",
    "                    #['0 mL', '146 mL'] -> (0, 146)\n",
    "#                     vol_tup = tuple([int(vol_text.split()[0]) if len(vol_text.split()) > 1 else None\n",
    "#                                      for vol_text in search])\n",
    "                    vol_tup = tuple([int(vol) for vol_text in search for vol in re.findall(r'\\d+',vol_text)])\n",
    "                    self.volumes.append(vol_tup)\n",
    "                else:\n",
    "                    self.volumes.append(None)\n",
    "            else:\n",
    "                self.volumes.append(None)\n",
    "\n",
    "    #List of tuples of format (ischemic core volume,hypoperfusion volume)\n",
    "    def get_volumes(self):\n",
    "        return self.volumes\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(177, 320)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctp_str = '''\n",
    "CT Perfusion:\n",
    "\n",
    "Estimated ischemic core volume (rCBF < 0.3): 177ml\n",
    "Estimated hypoperfusion volume (Tmax > 6 sec): 320 mL\n",
    "\n",
    "\n",
    "IMPRESSION:'''\n",
    "test_ctp = [ctp_str]\n",
    "ctpve = ctp_volumes_extractor(test_ctp)\n",
    "ctpve.extract_volumes()\n",
    "ctpve.get_volumes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMINATION: Computed tomography angiography (CTA) of the head without and with contrast; Computed tomography angiography (CTA) of the neck with contrast.\r\n",
      "CT perfusion imaging of the head with contrast\r\n",
      "\r\n",
      "HISTORY: Suspected hyperacute stroke.\r\n",
      "\r\n",
      "TECHNIQUE: Computed tomography of the head was performed without contrast according to standard protocol. Computed tomographic angiography was obtained from the level of the aortic arch to the vertex following the uneventful administration of intravenous contrast according to hyperacute stroke protocol. 3D images were generated on a dedicated workstation.\r\n",
      "CT perfusion of the brain was performed with intravenous contrast using a separate data acquisition. The data was transmitted to a separate workstation for processing by RAPID software (iSchemaView) to produce automated calculations of the estimated cerebral blood flow and Tmax.\r\n",
      "\r\n",
      "Contrast information:\r\n",
      "125 mL Optiray-350\r\n",
      "\r\n",
      "COMPARISON: CT head dated 5/18/2020 and 2/5/2020.\r\n",
      "\r\n",
      "FINDINGS:\r\n",
      "\r\n",
      "HEAD CT FINDINGS:\r\n",
      "\r\n",
      "There is no acute intracranial hemorrhage.  There is diffuse cerebral volume loss.  Multiple areas of periventricular white matter hypodensities consistent with small vessel ischemic disease.  Areas of chronic lacunar infarction in the deep gray structures as well as cerebellar hemispheres and pons are similar to prior study.  A focal area of hypoattenuation in the left occipital lobe is similar to the prior head CT dated 05/18/2020 but increased when compared to head CT from 02/05/2020, consistent with age-indeterminate infarct. There is no noncontrast evidence of acute stroke. There is no vascular hyperdensity of the M1 segments or basilar artery. Ventricles are of normal size and morphology. There is no mass effect or midline shift.  Near complete opacification of the left maxillary sinus with inspissated mucus or chronic fungal elements.  There is a left mastoid effusion.  Atherosclerotic calcifications of the cavernous carotid arteries.  Bilateral lens replacements present.\r\n",
      "\r\n",
      "ANGIOGRAPHIC FINDINGS:\r\n",
      "\r\n",
      "The visualized aortic arch appears normal with normal configuration of the great vessels. There is no significant stenosis of the origins of the great vessels.\r\n",
      "\r\n",
      "There is no geographic area of vascular paucity in the brain.\r\n",
      "\r\n",
      "Left anterior circulation:\r\n",
      "L CCA: no occlusion or significant stenosis\r\n",
      "L carotid bifurcation: no occlusion or significant stenosis\r\n",
      "Occlusion of the left internal carotid artery immediately distal to the carotid bifurcation with distal reconstitution of the supraclinoid portion.\r\n",
      "L M1: no occlusion or significant stenosis\r\n",
      "L M2 branches: no occlusion or significant stenosis\r\n",
      "L A2: no occlusion or significant stenosis\r\n",
      "\r\n",
      "Right anterior circulation:\r\n",
      "R CCA: no occlusion or significant stenosis\r\n",
      "R carotid bifurcation: no occlusion or significant stenosis\r\n",
      "R ICA proximal: no occlusion or significant stenosis\r\n",
      "R ICA distal: Mild stenosis.\r\n",
      "R ICA terminus: no occlusion or significant stenosis\r\n",
      "R M1: no occlusion or significant stenosis\r\n",
      "R M2 branches: no occlusion or significant stenosis\r\n",
      "R A2: no occlusion or significant stenosis\r\n",
      " \r\n",
      "Posterior circulation:\r\n",
      "L Vertebral Artery: no occlusion or significant stenosis\r\n",
      "R Vertebral Artery: no occlusion or significant stenosis\r\n",
      "Basilar Artery: no occlusion or significant stenosis\r\n",
      "L PCA: no occlusion or significant stenosis\r\n",
      "R PCA: no occlusion or significant stenosis\r\n",
      "\r\n",
      "No cerebral aneurysm is seen. There is no evidence for an arteriovenous malformation. There is no suspicious cervical lymphadenopathy. There is no significant cervical spondylosis. Limited views of the lung apices show pulmonary fibrosis in the upper lobes left greater than right which is similar to the prior chest CT and partially evaluated on this study.\r\n",
      "\r\n",
      "CT Perfusion:\r\n",
      "\r\n",
      "Evaluation limited secondary to motion artifact present on perfusion imaging.\r\n",
      "\r\n",
      "Estimated ischemic core volume (rCBF < 0.3): 10 mL\r\n",
      "Estimated hypoperfusion volume (Tmax > 6 sec): 41 mL\r\n",
      "\r\n",
      "\r\n",
      "IMPRESSION:\r\n",
      "\r\n",
      "1.  No acute intracranial hemorrhage, significant mass effect, or midline shift.  Focal area of hypoattenuation in the left occipital region is consistent with age-indeterminate infarction.  Multiple scattered periventricular and subcortical white matter hypodensities likely represent small vessel ischemic disease, similar to the prior study.  Chronic lacunar infarctions noted in the deep gray structures and pons.\r\n",
      "\r\n",
      "2.  Occlusion of the proximal left internal carotid artery immediately distal to the bifurcation with distal reconstitution of its supraclinoid portion likely via the anterior or posterior communicating artery.  The left anterior and middle cerebral arteries appear patent.  Perfusion imaging demonstrates small infarct core with slightly larger area of penumbra in the left cerebral hemisphere.\r\n",
      "\r\n",
      "3.  Changes of pulmonary fibrosis partially evaluated.\r\n",
      "\r\n",
      "The Critical results were discussed with Dr. Baldeshwiler by Dr. Naylor on 05/20/2020 at 10:36 AM. \r\n",
      "\r\n",
      "Dictated by: Adam David Naylor, M.D.\r\n",
      "\r\n",
      "The radiology attending physician has personally reviewed this study, and had reviewed and/or edited this written report and agrees with it.\r\n",
      "\r\n",
      "Electronically signed by: Martin Nicholas Reis, M.D.\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df[\"Report Text\"].iloc[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L CCA': 0,\n",
       " 'L carotid bifurcation': None,\n",
       " 'L ICA proximal': 0,\n",
       " 'L ICA distal': None,\n",
       " 'L ICA terminus': 0,\n",
       " 'L M1': 0,\n",
       " 'L M2 branches': 0,\n",
       " 'L A2': 0,\n",
       " 'R CCA': 0,\n",
       " 'R carotid bifurcation': None,\n",
       " 'R ICA proximal': 0,\n",
       " 'R ICA distal': None,\n",
       " 'R ICA terminus': 0,\n",
       " 'R M1': 0,\n",
       " 'R M2 branches': 0,\n",
       " 'R A2': 0,\n",
       " 'L Vertebral Artery': 0,\n",
       " 'R Vertebral Artery': 0,\n",
       " 'Basilar Artery': 0,\n",
       " 'L PCA': 0,\n",
       " 'R PCA': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr.read_ca_template(templates[8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L CCA': 0,\n",
       " 'L carotid bifurcation': None,\n",
       " 'L ICA proximal': 0,\n",
       " 'L ICA distal': None,\n",
       " 'L ICA terminus': None,\n",
       " 'L M1': 0,\n",
       " 'L M2 branches': 0,\n",
       " 'L A2': 0,\n",
       " 'R CCA': 0,\n",
       " 'R carotid bifurcation': None,\n",
       " 'R ICA proximal': 0,\n",
       " 'R ICA distal': None,\n",
       " 'R ICA terminus': None,\n",
       " 'R M1': 0,\n",
       " 'R M2 branches': 0,\n",
       " 'R A2': 0,\n",
       " 'L Vertebral Artery': 0,\n",
       " 'R Vertebral Artery': 0,\n",
       " 'Basilar Artery': None,\n",
       " 'L PCA': 0,\n",
       " 'R PCA': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_templates_dicts = ctr.read_templates()\n",
    "ca_templates_dicts[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For empty values in the ca_dicts, need to check impression\n",
    "#NLP needs to be used here ex.'Occlusion of the proximal inferior left M2 branch'\n",
    "#NLP needed on midline shift ex. There is no mass effect or midline shift.\n",
    "import networkx as nx\n",
    "import pyConTextNLP.pyConText as pyConText\n",
    "import pyConTextNLP.itemData as itemData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/pyConTextNLP/itemData.py:40: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  context_items =  [contextItem((d[\"Lex\"],\n"
     ]
    }
   ],
   "source": [
    "modifiers = itemData.get_items(\"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/modifiers.yml\")\n",
    "targets = itemData.get_items(\"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/targets.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyConTextNLP uses yml files to store entities and their regex pattern information\n",
    "#This function is to be used for quick lookup of an entity for short term use\n",
    "#Or to add new entities to a yml file\n",
    "\n",
    "#\"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/modifiers.yml\"\n",
    "#\"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/targets.yml\"\n",
    "\n",
    "class pyConText_itemData_initializer:\n",
    "    \n",
    "    def __init__(self, modifiers_path=None, targets_path=None):\n",
    "        if modifiers_path is None:\n",
    "            self.modifiers_path = \"https://raw.githubusercontent.com/chapmanbe/pyConTextNLP/master/KB/lexical_kb_05042016.yml\"\n",
    "        else:\n",
    "            self.modifiers_path = modifiers_path\n",
    "            \n",
    "        self.modifiers = itemData.get_items(self.modifiers_path)\n",
    "        self.targets = []\n",
    "            \n",
    "        if targets_path is not None:\n",
    "            self.targets_path = targets_path\n",
    "            self.targets = itemData.get_items(self.targets_path)\n",
    "    \n",
    "    def add_target(self,entity,category='', regex='', direction=''):\n",
    "        self.targets.append(itemData.contextItem([entity,category,regex,direction]))\n",
    "        \n",
    "        #Code from pyConTextNLP itemData.py\n",
    "        #def get_items(_file): context_items =  [contextItem((d[\"Lex\"], d[\"Type\"],r\"%s\"%d[\"Regex\"],d[\"Direction\"])) for d in yaml.load_all(f0)]\n",
    "    \n",
    "    def add_entity_to_yml(self, yml_path, entity, category=\"''\", regex=\"''\", direction=\"''\"):\n",
    "        yml_entity = \"Direction: \"+ direction + \"\\n\" + \\\n",
    "        \"Lex: \"+ entity + \"\\n\" + \\\n",
    "        \"Regex: \"+ regex + \"\\n\" + \\\n",
    "        \"Type: \" + category + \"\\n\" + \\\n",
    "        \"---\\n\" \n",
    "        \n",
    "        with open(yml_path, \"r+\") as f:\n",
    "            old = f.read()\n",
    "            f.seek(0)\n",
    "            #Prepending\n",
    "            f.write(yml_entity + old)\n",
    "            \n",
    "        f.close()\n",
    "        \n",
    "    def get_itemData(self):\n",
    "        return self.modifiers,self.targets\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "literal<<\\bl\\b>>; category<<['left_side']>>; re<<\\bl\\b>>; rule<<forward>>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_path = \"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/modifiers.yml\"\n",
    "targ_path = \"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/targets.yml\"\n",
    "itemData_init = pyConText_itemData_initializer(mod_path,targ_path)\n",
    "itemData_init.modifiers[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyConTextNLP.itemData.contextItem"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(itemData_init.targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "literal<<occlusion>>; category<<['occlusion']>>; re<<(occlusion|occlusive|occluded)>>; rule<<bidirectional>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# itemData_init.add_target('occlusion')\n",
    "itemData_init.targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itemData_init.add_entity_to_yml(\"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/targets.yml\",'P1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to determine existence or rule out given entity\n",
    "#Ex. Midline shift/No midline shift\n",
    "#Ex. Occlusion/No occlusion\n",
    "\n",
    "#Input is string entity that you want to rule in/out\n",
    "#***Since words have DIFFERENT forms, consider using a PREFIX INSTEAD\n",
    "#Ex. Entity: occlusion; sentence contains 'occluded thrombus'\n",
    "# Using the prefix 'occlu' would capture both in a rudimentary fashion\n",
    "\n",
    "class modifier_target_finder:\n",
    "    def __init__(self, sentence, mod_itemData, targ_itemData):\n",
    "        self.modifiers = mod_itemData\n",
    "        self.targets = targ_itemData\n",
    "        self.sentence = sentence\n",
    "        \n",
    "        self.markup = pyConText.ConTextMarkup()\n",
    "        self.markup.setRawText(sentence.lower())\n",
    "        self.markup.cleanText()\n",
    "        \n",
    "        self.markup.markItems(self.modifiers,mode=\"modifier\")\n",
    "        self.markup.markItems(self.targets,mode=\"target\")\n",
    "        self.markup.applyModifiers()\n",
    "    \n",
    "    #New addition\n",
    "    def log(self):\n",
    "        print(self.markup.__str__())\n",
    "        \n",
    "    \n",
    "    def get_markup(self):\n",
    "        return list(self.markup.edges())\n",
    "            \n",
    "    def find_mod_target_pair(self,desired_modifier,modifier_attrib,desired_target,target_attrib):\n",
    "        #set removes duplicates\n",
    "        return list(set([(modifier.getLiteral(),target.getLiteral()) \n",
    "                for (modifier,target) in self.markup.edges() \n",
    "                # getattr() will run modifier/target . getLiteral()/categoryString()/getTagID()/getConTextCategory()\n",
    "                if (desired_modifier in getattr(modifier,modifier_attrib)() \n",
    "                    and desired_target in getattr(target,target_attrib)())]))\n",
    "    \n",
    "    \n",
    "    #Returns 0 if negated\n",
    "    #Returns 1 if present and not negated\n",
    "    #Returns 2 if confirmed existence\n",
    "    #Returns None if not in sentence\n",
    "    def confirm_existence(self, entity, entity_type):\n",
    "        entities = []\n",
    "        for modifier,target in self.markup.edges():\n",
    "            etype_func = {\"getLiteral\":[modifier.getLiteral,target.getLiteral],\n",
    "                          \"categoryString\":[modifier.categoryString,target.categoryString]}\n",
    "            for func in etype_func[entity_type]:\n",
    "                entities.append(func())\n",
    "        if entity in entities:\n",
    "            neg_pairs_list = self.find_mod_target_pair('negated_existence','categoryString',entity,entity_type)\n",
    "            if neg_pairs_list:\n",
    "                return 0\n",
    "            def_exis_pairs_list = self.find_mod_target_pair(('definite_existence'),\n",
    "                                                    'categoryString', entity,entity_type)\n",
    "            prob_exis_pairs_list = self.find_mod_target_pair(('definite_existence'),\n",
    "                                                            'categoryString', entity,entity_type)\n",
    "            if def_exis_pairs_list or prob_exis_pairs_list:\n",
    "                return 2\n",
    "            return 1\n",
    "        else:\n",
    "            return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<id> 43921701137426022956509316448840336511 </id> <phrase> right </phrase> <category> ['right_side'] </category> ,\n",
       "  <id> 43950117110193389003830616922067846271 </id> <phrase> m2 </phrase> <category> ['m2_circulation'] </category> ),\n",
       " (<id> 43921701137426022956509316448840336511 </id> <phrase> right </phrase> <category> ['right_side'] </category> ,\n",
       "  <id> 43950158308837896421286165564922020991 </id> <phrase> m3 </phrase> <category> ['m3_circulation'] </category> ),\n",
       " (<id> 43922801616603346088158490774310503551 </id> <phrase> m2 </phrase> <category> ['m2_circulation'] </category> ,\n",
       "  <id> 43950117110193389003830616922067846271 </id> <phrase> m2 </phrase> <category> ['m2_circulation'] </category> ),\n",
       " (<id> 43922801616603346088158490774310503551 </id> <phrase> m2 </phrase> <category> ['m2_circulation'] </category> ,\n",
       "  <id> 43950158308837896421286165564922020991 </id> <phrase> m3 </phrase> <category> ['m3_circulation'] </category> ),\n",
       " (<id> 43922887975300486636286467737216369791 </id> <phrase> m3 </phrase> <category> ['m3_circulation'] </category> ,\n",
       "  <id> 43950117110193389003830616922067846271 </id> <phrase> m2 </phrase> <category> ['m2_circulation'] </category> ),\n",
       " (<id> 43922887975300486636286467737216369791 </id> <phrase> m3 </phrase> <category> ['m3_circulation'] </category> ,\n",
       "  <id> 43950158308837896421286165564922020991 </id> <phrase> m3 </phrase> <category> ['m3_circulation'] </category> ),\n",
       " (<id> 43923761069651393829286748591549072511 </id> <phrase> cut off </phrase> <category> ['uncertain_occlusion'] </category> ,\n",
       "  <id> 43950117110193389003830616922067846271 </id> <phrase> m2 </phrase> <category> ['m2_circulation'] </category> ),\n",
       " (<id> 43923761069651393829286748591549072511 </id> <phrase> cut off </phrase> <category> ['uncertain_occlusion'] </category> ,\n",
       "  <id> 43950158308837896421286165564922020991 </id> <phrase> m3 </phrase> <category> ['m3_circulation'] </category> )]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtf = modifier_target_finder(\"Abrupt cut off of distal right M2/ proximal M3 with geographic paucity of vasculature compared to contralateral side. \",\n",
    "                             itemData_init.get_itemData()[0],itemData_init.get_itemData()[1])\n",
    "mtf.get_markup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mtf.find_mod_target_pair(\"negated_existence\",\"categoryString\",\"uncertain_flow\",\"categoryString\"):\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtf.confirm_existence(\"uncertain_occlusion\",\"categoryString\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract midline shift yes/no\n",
    "#Searches entire report for midline shift sentence,\n",
    "#as it is discussed at beginning of Findings section\n",
    "#exclusively\n",
    "class midline_shift_extractor:\n",
    "    def __init__(self,df_column,modifiers,targets):\n",
    "        self.df_column = df_column\n",
    "        self.modifiers = modifiers\n",
    "        self.targets = targets\n",
    "        self.midline_shift_pattern = re.compile('([^.]*?midline shift[^.]*\\.)')\n",
    "        self.midline_shift_statuses = []\n",
    "        \n",
    "    def extract_midline_shifts(self):\n",
    "        for report in self.df_column:\n",
    "            search = self.midline_shift_pattern.search(report)\n",
    "            if search:\n",
    "                sent = search.group()\n",
    "                mtf = modifier_target_finder(sent,self.modifiers,self.targets)\n",
    "                status = mtf.confirm_existence(\"midline shift\",\"getLiteral\")\n",
    "                if status == 0:\n",
    "                    self.midline_shift_statuses.append(0)\n",
    "                elif status > 0:\n",
    "                    self.midline_shift_statuses.append(1)\n",
    "                else:\n",
    "                    self.midline_shift_statuses.append(None)\n",
    "    \n",
    "    def get_midline_shifts(self):\n",
    "        return self.midline_shift_statuses\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse = midline_shift_extractor(filtered_df[\"CTA CTP Report Text\"],itemData_init.get_itemData()[0],itemData_init.get_itemData()[1])\n",
    "# mse.extract_midline_shifts()\n",
    "# midline_shifts = mse.get_midline_shifts()\n",
    "# midline_shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i, value in enumerate(midline_shifts) if value == 1] \n",
    "#Only 2 out of 500 have midline shift, will not run in final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df['CTA CTP Report Text'][211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ca = cerebral arteries\n",
    "#expects ca_templates_reader dictionary as input\n",
    "#which acts as a filter.\n",
    "#For sites not ruled out (value=None), read the impression\n",
    "#for further clarification\n",
    "\n",
    "class possible_occlusions_generator:    \n",
    "    def __init__(self, ca_filter_dict):\n",
    "        \n",
    "        self.new_dict_keys = [\"L CCA\", \"L ICA\", \"L ACA\", \"L A1\", \"L A2\", \"L A3\", \"L A4\", \"L A5\",\n",
    "                        \"L MCA\", \"L M1\", \"L M2\", \"L M3\", \"L M4\",\n",
    "                        \"R CCA\", \"R ICA\", \"R ACA\", \"R A1\", \"R A2\", \"R A3\", \"R A4\", \"R A5\",\n",
    "                        \"R MCA\", \"R M1\", \"R M2\", \"R M3\", \"R M4\",\n",
    "                        \"L vertebral\", \"L V1\", \"L V2\", \"L V3\", \"L V4\",\n",
    "                        \"R vertebral\", \"R V1\", \"R V2\", \"R V3\", \"R V4\",\n",
    "                        \"basilar\",\n",
    "                        \"L PCA\", \"L P1\", \"L P2\", \"L P3\", \"L P4\",\n",
    "                        \"R PCA\", \"R P1\", \"R P2\", \"R P3\", \"R P4\"]\n",
    "    \n",
    "        \n",
    "        self.filter_dict = self.consolidate_dict(ca_filter_dict)\n",
    "        self.filled_dict = self.fill_in_dict(self.filter_dict)\n",
    "        self.ca_to_investigate = [key for key,value in self.filled_dict.items() if value < 0]\n",
    "        \n",
    "    def negate_nones(self,dictionary):\n",
    "        negated_dict = {}\n",
    "        for key,value in dictionary.items():\n",
    "            if value is None:\n",
    "                negated_dict[key] = -1\n",
    "            else:\n",
    "                negated_dict[key] = value\n",
    "        return negated_dict\n",
    "            \n",
    "    def consolidate_keys_for_dict(self,dictionary,list_of_keys,consol_key):\n",
    "        new_dict = {}\n",
    "        consolidated_value = 0\n",
    "        found_key = 0\n",
    "        for key,value in dictionary.items():\n",
    "            if found_key == 1:\n",
    "                #Add consol_key to new_dict at spot of first key in list_of_keys \n",
    "                new_dict[consol_key] = consolidated_value\n",
    "            if key in list_of_keys:\n",
    "                found_key+=1                \n",
    "                #Value is negative, indicating further investigation\n",
    "                consolidated_value += value                \n",
    "            else:\n",
    "                new_dict[key] = value\n",
    "        new_dict[consol_key] = consolidated_value\n",
    "        return new_dict\n",
    "    \n",
    "    #Calls consolidate_keys_for_dict to consolidate CCAs and ICAs\n",
    "    def consolidate_dict(self,filter_dict):\n",
    "        consol_dict = self.negate_nones(filter_dict)\n",
    "        keys_dict = {\n",
    "            \"L CCA\": [\"L CCA\", \"L carotid bifurcation\"],\n",
    "            \"R CCA\": [\"R CCA\", \"R carotid bifurcation\"],\n",
    "            \"L ICA\": [\"L ICA proximal\",\"L ICA distal\",\"L ICA terminus\"],\n",
    "            \"R ICA\": [\"R ICA proximal\",\"R ICA distal\",\"R ICA terminus\"],\n",
    "            \"L M2\": [\"L M2 branches\"],\n",
    "            \"R M2\": [\"R M2 branches\"],\n",
    "            \"R vertebral\": [\"R Vertebral Artery\"],\n",
    "            \"L vertebral\": [\"L Vertebral Artery\"],\n",
    "            \"basilar\": [\"Basilar Artery\"]\n",
    "        }\n",
    "        for consol_key,list_of_keys in keys_dict.items():\n",
    "            consol_dict = self.consolidate_keys_for_dict(consol_dict,list_of_keys,consol_key)\n",
    "        \n",
    "        return consol_dict\n",
    "    \n",
    "    \n",
    "    #Adds AX, MX, PX, and VX segments, where X is segment location (ex. V4, X=4)\n",
    "    def fill_in_dict(self,consol_dict):\n",
    "        \n",
    "        new_dict = {}\n",
    "        \n",
    "        #Populate keys-vals from previous consolidated dictionary\n",
    "        for key in self.new_dict_keys:\n",
    "            if key in consol_dict:\n",
    "                new_dict[key] = consol_dict[key]\n",
    "            else:\n",
    "                #Temp value that will be filled by transfer_key_values\n",
    "                new_dict[key] = None\n",
    "        \n",
    "        #The template serves as a reference for which cerebral arteries to look at\n",
    "        #This function transfers the values from the template to the corresponding\n",
    "        #AX, MX, PX, VX arteries\n",
    "        #Ex. L Vertebral Artery Occlusion indicates occlusion in one/more of L V1-4\n",
    "        def transfer_key_values(source_key,list_receiving):\n",
    "            for key in list_receiving:\n",
    "                if key == \"L MCA\":\n",
    "                    new_dict[key] = consol_dict[\"L M1\"]+consol_dict[\"L M2\"]\n",
    "                elif key == \"R MCA\":\n",
    "                    new_dict[key] = consol_dict[\"R M1\"]+consol_dict[\"R M2\"]\n",
    "                else:\n",
    "                    new_dict[key] = consol_dict[source_key]\n",
    "        refs = {\n",
    "            \"L M1/M2\": [\"L MCA\"],\n",
    "            \"R M1/M2\": [\"R MCA\"],\n",
    "            \"L M2\": [\"L M3\", \"L M4\"],\n",
    "            \"R M2\": [\"R M3\", \"R M4\"],\n",
    "            \"L A2\": [\"L ACA\",\"L A1\", \"L A3\", \"L A4\", \"L A5\"],\n",
    "            \"R A2\": [\"R ACA\",\"R A1\", \"R A3\", \"R A4\", \"R A5\"],\n",
    "            \"L vertebral\": [\"L V1\", \"L V2\", \"L V3\", \"L V4\"],\n",
    "            \"R vertebral\": [\"R V1\", \"R V2\", \"R V3\", \"R V4\"],\n",
    "            \"L PCA\": [\"L P1\", \"L P2\", \"L P3\", \"L P4\"],\n",
    "            \"R PCA\": [\"R P1\", \"R P2\", \"R P3\", \"R P4\"]\n",
    "        }\n",
    "        \n",
    "        for source_key,list_receiving in refs.items():\n",
    "            transfer_key_values(source_key,list_receiving)\n",
    "            \n",
    "        return new_dict\n",
    "    \n",
    "    def get_possibilities(self):\n",
    "        return self.ca_to_investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "class occlusions_investigator:\n",
    "    \n",
    "    def __init__(self,ca_to_investigate,impressions_txt,modData,targData):\n",
    "        self.modData = modData\n",
    "        self.targData = targData\n",
    "        self.impressions_txt = impressions_txt\n",
    "        self.ca_to_investigate = ca_to_investigate\n",
    "        \n",
    "        self.extra_chars = re.compile('IMPRESSION:|\\n[0-9]\\.|\\r|\\n|\\t')\n",
    "        self.impressions = re.sub(self.extra_chars,'',self.impressions_txt)\n",
    "        self.txt_sents = self.impressions.split(\".\")\n",
    "        #List of 0,1,2 or Nones based on negated occ, occ mentioned, occ confirmed, or no occ mentioned\n",
    "        self.sent_has_occlusion = [self.confirm_occlusion(sent) for sent in self.txt_sents]\n",
    "        self.sent_maybe_occlusion = [self.confirm_uncertain_occlusion(sent) for sent in self.txt_sents]\n",
    "        self.occlusions = []\n",
    "        self.bool_none = lambda x: False if not x else True\n",
    "        self.filt_sent_has_occlusion = list(filter(self.bool_none,self.sent_has_occlusion))\n",
    "        self.filt_txt_sents = list(compress(self.txt_sents,map(self.bool_none,self.sent_has_occlusion)))\n",
    "        self.filt_maybe_sents = list(compress(self.txt_sents,map(self.bool_none,self.sent_maybe_occlusion)))\n",
    "        #0 for negation\n",
    "        #1 for occlusion present and not negated\n",
    "        #2 for occlusion indicated\n",
    "        #None for occlusion not found\n",
    "        self.occlusion_ratings = []\n",
    "        \n",
    "        #This list keeps track of occlusion sentences that have and have not\n",
    "        #been matched with an occlusion in the template\n",
    "        #Those unmatched with template will be flagged for the user to review\n",
    "        self.unmatched_occlusion_sents = [True for sent in self.filt_txt_sents]\n",
    "        \n",
    "        #Find if sentence refers to cerebral artery with indeterminate terms\n",
    "        #Will be flagged for user review\n",
    "        self.uncertain_occlusions = []\n",
    "        \n",
    "        self.markups = []\n",
    "\n",
    "    def get_occlusions(self):\n",
    "        \n",
    "        ca_to_investigate = []\n",
    "        for potential in self.ca_to_investigate:\n",
    "            side = None\n",
    "            ca = None\n",
    "            if \" \" in potential:\n",
    "                potential_list = potential.split(\" \")\n",
    "                side = potential_list[0]\n",
    "                ca = potential_list[1]\n",
    "            else:\n",
    "                ca = potential\n",
    "            \n",
    "            ca_to_investigate.append((potential,side,ca))\n",
    "        \n",
    "        #Check for uncertain occlusions\n",
    "        if self.sent_maybe_occlusion:\n",
    "            #print(self.sent_maybe_occlusion)\n",
    "            for sent in self.filt_maybe_sents:\n",
    "                for potential,side,ca in ca_to_investigate:\n",
    "                    lim = self.confirm_limitation(sent, ca)\n",
    "                    if lim:\n",
    "                        self.uncertain_occlusions.append(lim)\n",
    "        \n",
    "        #There is no occlusion in any sentence\n",
    "        #Return empty occlusions list\n",
    "        if not self.filt_sent_has_occlusion:\n",
    "            self.occlusion_ratings = [None for pos in self.ca_to_investigate]\n",
    "            return self.occlusions\n",
    "\n",
    "        #Investigate potenial occlusions by side and cerebral artery\n",
    "        for potential,side,ca in ca_to_investigate:\n",
    "            #for sent,occl_label in zip(self.filt_txt_sents,self.filt_sent_has_occlusion):\n",
    "            for i in range(len(self.filt_txt_sents)):\n",
    "                sent = self.filt_txt_sents[i]\n",
    "                occl_label = self.filt_sent_has_occlusion[i]\n",
    "                #If site/side of occlusion verified\n",
    "                #Add the potential occlusion to the final occlusions list\n",
    "                is_occl, flag = self.confirm_site_occlusion(sent,side,ca)\n",
    "                #Excludes both None and 0, runs on 1 or 2\n",
    "                if is_occl:\n",
    "                    self.unmatched_occlusion_sents[i] = False\n",
    "                    if flag == -1:\n",
    "                        #side of the site could not be verified, flag -1\n",
    "                        #in occlusion_ratings list\n",
    "                        self.occlusion_ratings.append((sent,side,ca,flag))\n",
    "                        #Marked as non-occlusion for now\n",
    "                        occl_label = 0\n",
    "                    else:\n",
    "                        #Can be corellated to filt_sent_has_occlusion list with labels 0-2\n",
    "                        self.occlusion_ratings.append((sent,side,ca,occl_label))\n",
    "            \n",
    "                    if occl_label > 0:\n",
    "                        self.occlusions.append(potential)\n",
    "                else:\n",
    "                    #is_occl\n",
    "                    #None: Means potential occlusion not found in Findings\n",
    "                    #0: Occlusion negated\n",
    "                    self.occlusion_ratings.append((sent,side,ca,None))\n",
    "                        \n",
    "            \n",
    "        self.unmatched_occlusion_sents = list(compress(self.filt_txt_sents,self.unmatched_occlusion_sents))\n",
    "        \n",
    "\n",
    "        #If other side has no findings and occlusion of that artery present\n",
    "        #Then add to occlusions list\n",
    "        def check_other_side_occlusion(side,ca,occl_label):\n",
    "            \n",
    "            vs_dict = {\"ACA\": [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\"],\n",
    "                       \"MCA\": [\"M1\",\"M2\",\"M3\",\"M4\"],\n",
    "                       \"PCA\": [\"P1\",\"P2\",\"P3\",\"P4\"],\n",
    "                       \"vertebral\": [\"V1\",\"V2\",\"V3\",\"V4\"]\n",
    "                      }\n",
    "            \n",
    "            other_side = None\n",
    "            if side == \"L\":\n",
    "                other_side = \"R\"\n",
    "            elif side == \"R\":\n",
    "                other_side = \"L\"\n",
    "            for o_sent, o_side, o_ca, o_occl_label in self.occlusion_ratings:\n",
    "                #Most cases will be a list of one element\n",
    "                #Ex. M2\n",
    "                ca_list = [ca]\n",
    "                #Need to check other side segment equivalents\n",
    "                #If general large vessel a possible occlusion\n",
    "                #Ex. L MCA -> Check R M1, R M2, R M3, R M4\n",
    "                if ca in vs_dict:\n",
    "                    for c in vs_dict[ca]:\n",
    "                        ca_list.append(c)\n",
    "                for ca_equiv in ca_list:\n",
    "                    #Check if other side, same cerebral artery and occlusion label is 0,1,2\n",
    "                    #If so this occlusion is not likely\n",
    "                    #Otherwise, add this occlusion\n",
    "                    if o_occl_label is not None:\n",
    "                        if o_side == other_side and o_ca == ca_equiv and o_occl_label > -1:\n",
    "                            return\n",
    "            self.occlusions.append(side + \" \" + ca)\n",
    "\n",
    "        #Check flagged occlusions where side and cerebral artery didn't match\n",
    "        for sent,side,ca,occl_label in self.occlusion_ratings:\n",
    "            if occl_label == -1:             \n",
    "                check_other_side_occlusion(side,ca,occl_label)\n",
    "                \n",
    "\n",
    "        return self.occlusions\n",
    "    \n",
    "    def get_possibility_ratings(self):\n",
    "        return self.occlusion_ratings\n",
    "    \n",
    "    def get_markups(self):\n",
    "        #(index,sents with occlusion, markup of those sents)\n",
    "        return list(zip(self.filt_txt_sents,self.markups))\n",
    "    \n",
    "    def get_unmatched_occlusion_sents(self):\n",
    "        return self.unmatched_occlusion_sents\n",
    "    \n",
    "    def get_uncertain_occlusions(self):\n",
    "        return self.uncertain_occlusions\n",
    "\n",
    "\n",
    "    def confirm_occlusion(self,sent):\n",
    "        mtf = modifier_target_finder(sent,self.modData,self.targData)\n",
    "        #Looking for occlusion in acute context, not chronic occlusion\n",
    "        if not mtf.find_mod_target_pair('chronic','getLiteral','occlusion','getLiteral'):\n",
    "            #either 0, 1, 2, or None\n",
    "            return mtf.confirm_existence('occlusion','getLiteral')\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def confirm_uncertain_occlusion(self,sent):\n",
    "        mtf = modifier_target_finder(sent,self.modData,self.targData)\n",
    "        uo = mtf.confirm_existence('uncertain_occlusion','categoryString')\n",
    "        if uo:\n",
    "            return uo\n",
    "        uf = mtf.confirm_existence('uncertain_flow','categoryString')\n",
    "        if uf:\n",
    "            return uf\n",
    "        return None\n",
    "\n",
    "\n",
    "    def confirm_site_occlusion(self,sent,side,site):\n",
    "        mtf = modifier_target_finder(sent,self.modData,self.targData)\n",
    "        self.markups.append(mtf.get_markup())\n",
    "        full_side = side\n",
    "        if side == \"L\":\n",
    "            full_side = \"left_side\"\n",
    "        elif side == \"R\":\n",
    "            full_side = \"right_side\"\n",
    "\n",
    "        #Verify occlusion of specific cerebral artery\n",
    "        sites = mtf.find_mod_target_pair(site,'getLiteral','occlusion','getLiteral')\n",
    "        \n",
    "        #Verify side of specific cerebral artery\n",
    "        #None gets overwritten later, flag -1 important\n",
    "        if sites:\n",
    "            if side is None:\n",
    "                return (True,None)\n",
    "            sides_sites = mtf.find_mod_target_pair(full_side,'categoryString',site,'getLiteral')\n",
    "            if sides_sites:\n",
    "                return (True,None)\n",
    "            else:\n",
    "                #could not verify the side corresponds to site in findings\n",
    "                return (True,-1)\n",
    "        return (False,None)\n",
    "    \n",
    "    #Sentences with terms that may or may not suggest acute occlusion\n",
    "    #Are flagged for user review\n",
    "    def confirm_limitation(self, sent, site):\n",
    "        mtf = modifier_target_finder(sent,self.modData,self.targData)\n",
    "        if mtf.find_mod_target_pair('uncertain_occlusion','categoryString',site,'getLiteral'):\n",
    "            return ('uncertain occlusion',site,sent)\n",
    "        if mtf.find_mod_target_pair('uncertain_flow','categoryString',site,'getLiteral'):\n",
    "            if mtf.find_mod_target_pair('negated_existence','categoryString','uncertain_flow','categoryString'):\n",
    "                return ('uncertain negated flow',site,sent)\n",
    "        return None\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f44411dc800>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = ('t1','t2')\n",
    "p = ('t3')\n",
    "zip('t1','t2','t3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occlusions_list = filtered_edges(('anterior_circulation' or 'posterior_circulation'),'categoryString','occlusion','categoryString')\n",
    "# side_circ_list = []\n",
    "# #occlusions_list = [('M1', 'occlusion'), ('M2', 'occlusion')]\n",
    "# for (modifier,target) in occlusions_list:\n",
    "#     #side_circ_list = [('right', 'M1'), ('right', 'M2')]\n",
    "#     side_circ_list += filtered_edges('side','categoryString',modifier,'getLiteral')\n",
    "# for side,circ in side_circ_list:\n",
    "#     #list evaluates to [('no', 'occlusion')], or is empty\n",
    "#     no_occlusion = filtered_edges('negated_existence','categoryString','occlusion','categoryString')\n",
    "#     if no_occlusion:\n",
    "#         print(\"No occlusion at \"+ side + \" \" + circ)\n",
    "#     else:\n",
    "#         print(\"Occlusion at \"+ side + \" \" + circ)\n",
    "\n",
    "# # else:\n",
    "# #     for (modifier,target) in occlusions_list:\n",
    "# #             print(\"occlusion at \" + occlusions_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L CCA': None,\n",
       " 'L carotid bifurcation': None,\n",
       " 'L ICA proximal': None,\n",
       " 'L ICA distal': None,\n",
       " 'L ICA terminus': None,\n",
       " 'L M1': None,\n",
       " 'L M2 branches': None,\n",
       " 'L A2': None,\n",
       " 'R CCA': None,\n",
       " 'R carotid bifurcation': None,\n",
       " 'R ICA proximal': None,\n",
       " 'R ICA distal': None,\n",
       " 'R ICA terminus': None,\n",
       " 'R M1': None,\n",
       " 'R M2 branches': None,\n",
       " 'R A2': None,\n",
       " 'L Vertebral Artery': None,\n",
       " 'R Vertebral Artery': None,\n",
       " 'Basilar Artery': None,\n",
       " 'L PCA': None,\n",
       " 'R PCA': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = ca_templates_dicts[325]\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L CCA': -2,\n",
       " 'L ICA': -3,\n",
       " 'L M1': -1,\n",
       " 'L M2': -1,\n",
       " 'L A2': -1,\n",
       " 'R CCA': -2,\n",
       " 'R ICA': -3,\n",
       " 'R M1': -1,\n",
       " 'R M2': -1,\n",
       " 'R A2': -1,\n",
       " 'L vertebral': -1,\n",
       " 'R vertebral': -1,\n",
       " 'basilar': -1,\n",
       " 'L PCA': -1,\n",
       " 'R PCA': -1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pog = possible_occlusions_generator(test_dict)\n",
    "pog.filter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L CCA': -2,\n",
       " 'L ICA': -3,\n",
       " 'L ACA': -1,\n",
       " 'L A1': -1,\n",
       " 'L A2': -1,\n",
       " 'L A3': -1,\n",
       " 'L A4': -1,\n",
       " 'L A5': -1,\n",
       " 'L MCA': -2,\n",
       " 'L M1': -1,\n",
       " 'L M2': -1,\n",
       " 'L M3': -1,\n",
       " 'L M4': -1,\n",
       " 'R CCA': -2,\n",
       " 'R ICA': -3,\n",
       " 'R ACA': -1,\n",
       " 'R A1': -1,\n",
       " 'R A2': -1,\n",
       " 'R A3': -1,\n",
       " 'R A4': -1,\n",
       " 'R A5': -1,\n",
       " 'R MCA': -2,\n",
       " 'R M1': -1,\n",
       " 'R M2': -1,\n",
       " 'R M3': -1,\n",
       " 'R M4': -1,\n",
       " 'L vertebral': -1,\n",
       " 'L V1': -1,\n",
       " 'L V2': -1,\n",
       " 'L V3': -1,\n",
       " 'L V4': -1,\n",
       " 'R vertebral': -1,\n",
       " 'R V1': -1,\n",
       " 'R V2': -1,\n",
       " 'R V3': -1,\n",
       " 'R V4': -1,\n",
       " 'basilar': -1,\n",
       " 'L PCA': -1,\n",
       " 'L P1': -1,\n",
       " 'L P2': -1,\n",
       " 'L P3': -1,\n",
       " 'L P4': -1,\n",
       " 'R PCA': -1,\n",
       " 'R P1': -1,\n",
       " 'R P2': -1,\n",
       " 'R P3': -1,\n",
       " 'R P4': -1}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pog.filled_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L CCA',\n",
       " 'L ICA',\n",
       " 'L ACA',\n",
       " 'L A1',\n",
       " 'L A2',\n",
       " 'L A3',\n",
       " 'L A4',\n",
       " 'L A5',\n",
       " 'L MCA',\n",
       " 'L M1',\n",
       " 'L M2',\n",
       " 'L M3',\n",
       " 'L M4',\n",
       " 'R CCA',\n",
       " 'R ICA',\n",
       " 'R ACA',\n",
       " 'R A1',\n",
       " 'R A2',\n",
       " 'R A3',\n",
       " 'R A4',\n",
       " 'R A5',\n",
       " 'R MCA',\n",
       " 'R M1',\n",
       " 'R M2',\n",
       " 'R M3',\n",
       " 'R M4',\n",
       " 'L vertebral',\n",
       " 'L V1',\n",
       " 'L V2',\n",
       " 'L V3',\n",
       " 'L V4',\n",
       " 'R vertebral',\n",
       " 'R V1',\n",
       " 'R V2',\n",
       " 'R V3',\n",
       " 'R V4',\n",
       " 'basilar',\n",
       " 'L PCA',\n",
       " 'L P1',\n",
       " 'L P2',\n",
       " 'L P3',\n",
       " 'L P4',\n",
       " 'R PCA',\n",
       " 'R P1',\n",
       " 'R P2',\n",
       " 'R P3',\n",
       " 'R P4']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possibilities = pog.get_possibilities()\n",
    "possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMINATION: Computed tomography angiography (CTA) of the head without and with contrast\r\n",
      "Computed tomography angiography (CTA) of the neck with contrast\r\n",
      "\r\n",
      "HISTORY: Trauma.\r\n",
      "\r\n",
      "TECHNIQUE: Computed tomography of the head was performed without contrast according to standard protocol. Computed tomographic angiography was then obtained from the aortic arch to the vertex following the uneventful administration of intravenous contrast. 3D images were generated on a dedicated workstation.\r\n",
      "\r\n",
      "Contrast information:\r\n",
      "100 mL Optiray-350\r\n",
      "\r\n",
      "COMPARISON: Head CT from 12/20/2019. \r\n",
      "\r\n",
      "FINDINGS:\r\n",
      "\r\n",
      "Topogram demonstrates no lytic lesions or fractures. \r\n",
      "\r\n",
      "There is interval increase in size of a left convexity subdural hematoma now measuring 2.1 cm, previously 1.2 cm but with decreasing hyperdensity.  There is increasing mass effect with a new 5 mm rightward midline shift.\r\n",
      "\r\n",
      "Ventricles are of normal size and morphology. No mass effect or midline shift is present. The gray-white matter differentiation is normal. The visualized portions of the orbits are normal. There are underdeveloped mastoid air cells that are completely opacified. There is mild mucosal thickening of the left maxillary sinus and minimal mucosal thickening of the right maxillary sinus.  The rest of the paranasal sinuses are normal. No fractures are identified.\r\n",
      "\r\n",
      "Scattered subcentimeter lymph nodes are seen in the neck. None are pathologically enlarged or abnormally enhancing. The muscles of the neck are normal. Vessels of the neck demonstrate normal course and caliber. Fascial planes are preserved and the deep spaces of the neck are normal. The visualized airway is widely patent. \r\n",
      "\r\n",
      "There are degenerative changes of the cervical spine.  \r\n",
      "\r\n",
      "There is moderate left and moderate to large right pleural effusion with passive atelectasis at the lung bases.  There is pulmonary edema an extensive airspace opacities, incompletely evaluated the patient is intubated with a gastric tube.  Median sternotomy wires are seen.  Extensive mediastinal lymph nodes may be reactive.\r\n",
      "\r\n",
      "Angiographic findings:\r\n",
      "\r\n",
      "The visualized aortic arch appears normal with normal configuration of the great vessels. The innominate artery and both subclavian arteries are normal in course and caliber. Mild atherosclerotic calcification at the carotid bifurcations. The course and caliber of the internal carotid arteries are normal. No areas of atherosclerotic narrowing or filling defects are identified.\r\n",
      "\r\n",
      "The circle-of-Willis is complete.  The right A1 segment is hypoplastic. The anterior and middle cerebral arteries are normal. The right vertebral artery is dominant. The basilar artery is normal. The posterior cerebral arteries are normal. There is no aneurysm or vascular malformation identified. \r\n",
      "\r\n",
      "IMPRESSION:\r\n",
      "\r\n",
      "1. Interval increase in size of a left convexity subdural hematoma now measuring 2.1 cm, previously 1.2 cm but with decreasing hyperdensity.  \r\n",
      "\r\n",
      "2. Increasing mass effect with a new 5 mm rightward midline shift.\r\n",
      "\r\n",
      "3. No CT evidence of acute stroke and no findings of large vessel occlusion.\r\n",
      "\r\n",
      "4.  Bilateral pleural effusions with passive atelectasis and extensive nonenhancing airspace opacities concerning for multifocal pneumonia in the background of pulmonary edema.\r\n",
      "\r\n",
      "The above findings were communicated with the neurology team by Dr. Vu at the scanner 1/7/2020 12:38 PM.\r\n",
      "\r\n",
      "\r\n",
      " \r\n",
      "\r\n",
      "**\r\n",
      "\r\n",
      " \r\n",
      "\r\n",
      "Dictated by: John Vu, M.D.\r\n",
      "\r\n",
      "The radiology attending physician has personally reviewed this study, and had reviewed and/or edited this written report and agrees with it.\r\n",
      "\r\n",
      "Electronically signed by: Martin Nicholas Reis, M.D.\n",
      "\n",
      "Addendum: CT perfusion imaging was also performed using RAPID software according to standard protocol. These images showed no significant changes in cerebral blood flow or Tmax. The remainder of the dictation is unchanged.\r\n",
      "\r\n",
      "Electronically signed by: Martin Nicholas Reis, M.D.\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df[\"Report Text\"].iloc[325])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi = occlusions_investigator(possibilities,impressions[325],itemData_init.get_itemData()[0],itemData_init.get_itemData()[1])\n",
    "predicted_occlusions = oi.get_occlusions()\n",
    "predicted_occlusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicted_occlusions vs possibilities\n",
    "extra_poss = list(set(predicted_occlusions) - set(possibilities))\n",
    "extra_poss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi.unmatched_occlusion_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi.uncertain_occlusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "oi.confirm_uncertain_occlusion('cut-off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('uncertain occlusion',\n",
       " 'M2',\n",
       " 'Abrupt cut off of distal right M2/ proximal M3 with geographic paucity of vasculature compared to contralateral side. ')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi.confirm_limitation('Abrupt cut off of distal right M2/ proximal M3 with geographic paucity of vasculature compared to contralateral side. ','M2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi.filt_txt_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oi.occlusion_ratings\n",
    "oi.get_possibility_ratings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi.filt_sent_has_occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 0,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi.sent_has_occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 1,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi.sent_maybe_occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi.filt_txt_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vessel_site_classifier:\n",
    "    \n",
    "    def __init__(self,occl_list):\n",
    "        self.occl_list = occl_list\n",
    "        #Large vessels: CCA, ICA, ACA, MCA, PCA, Vertebrals, Basilar\n",
    "        #Segments (of large vessels):\n",
    "        #A1-5, M1-4, V1-4, P1-4\n",
    "        self.vessels = []\n",
    "        self.sites = []\n",
    "        self.vs_dict = {\"CCA\": [],\n",
    "                       \"ICA\": [],\n",
    "                       \"ACA\": [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\"],\n",
    "                       \"MCA\": [\"M1\",\"M2\",\"M3\",\"M4\"],\n",
    "                       \"PCA\": [\"P1\",\"P2\",\"P3\",\"P4\"],\n",
    "                       \"vertebral\": [\"V1\",\"V2\",\"V3\",\"V4\"],\n",
    "                       \"basilar\": []}\n",
    "    def classify(self):\n",
    "        for occlusions in self.occl_list:\n",
    "            vessel_occlusions = []\n",
    "            site_occlusions = []\n",
    "            for occlusion in occlusions:\n",
    "                if len(occlusion.split()) > 1:\n",
    "                    side = occlusion.split()[0]\n",
    "                    occl = occlusion.split()[1]\n",
    "                else:\n",
    "                    side = ''\n",
    "                    occl = occlusion\n",
    "                if occl in self.vs_dict.keys():\n",
    "                    vessel_occlusions.append(side + \" \" + occl)\n",
    "                else:\n",
    "                    for vessel,segments in self.vs_dict.items():\n",
    "                        if occl in segments:\n",
    "                            vessel_occlusions.append(side + \" \" + vessel)\n",
    "                            #Ex ('MCA', 'L')\n",
    "                            site_occlusions.append(side + \" \" + occl)\n",
    "            #list(set()) removes duplicates (ex. multiple MCA occlucions -> ['MCA'])\n",
    "            self.vessels.append(list(set(vessel_occlusions)))\n",
    "            self.sites.append(site_occlusions)\n",
    "        return self.vessels,self.sites\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test vessel_site_classifier\n",
    "test_vsc = vessel_site_classifier([predicted_occlusions])\n",
    "predicted_vessel_occ, predicted_segments_occ = test_vsc.classify()\n",
    "lower = lambda x:x.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_vessel_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_segments_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(map(lower,predicted_vessel_occ[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vertebral', 'MCA', 'ICA']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = lambda q:q.split()[-1]\n",
    "f = [\" MCA\",\"R ICA\", \"L vertebral\",\"L ICA\"]\n",
    "list(set(map(l,f)))\n",
    "#s.split()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "class stats_generator:\n",
    "    def __init__(self, X, y, check_side=None):\n",
    "        self.oneHotEncoder = MultiLabelBinarizer()\n",
    "        \n",
    "        if check_side:\n",
    "            #Get the 'L', or 'R' if there is a side, or put \"None\"\n",
    "            self.splitter = lambda w:w.split()[0] if ('L' in w) or ('R' in w) else 'None'\n",
    "        else:\n",
    "            #Get rid of side labels\n",
    "            #[\"L MCA\"] -> [\"MCA\"]\n",
    "            #***Merges both side occlusions ex. [\"R MCA\",\"L MCA\"] -> [\"MCA\"]\n",
    "            self.splitter = lambda l:l.split()[-1]\n",
    "        self.X = [list(set(map(self.splitter,i))) for i in X]\n",
    "        self.y = [list(set(map(self.splitter,i))) for i in y]\n",
    "        #If only self.y, classes in self.X not in self.y would\n",
    "        #cause an error when classification_report(...) generated\n",
    "        #Case where predictions vary from y labels \n",
    "        self.oneHotEncoder.fit(self.y+self.X)\n",
    "        self.encoded_X = self.oneHotEncoder.transform(self.X)\n",
    "        self.encoded_y = self.oneHotEncoder.transform(self.y)\n",
    "    def getStats(self):\n",
    "        print(classification_report(self.encoded_y, self.encoded_X, target_names=self.oneHotEncoder.classes_))\n",
    "        print(multilabel_confusion_matrix(self.encoded_y, self.encoded_X))\n",
    "        print(accuracy_score(self.encoded_y, self.encoded_X))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.vessels = [\"CCA\",\"ICA\",\"ACA\",\"MCA\",\"PCA\",\"vertebral\",\"basilar\"]\n",
    "#         self.sites = [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"M1\",\"M2\",\"M3\",\"M4\",\"P1\",\"P2\",\"P3\",\"P4\",\"V1\",\"V2\",\"V3\",\"V4\"]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 1 0 0]]\n",
      "[[0 0 1 1]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['ICA', 'MCA', 'basilar', 'vertebral'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX = [['R basilar'],['L vertebral'],['L MCA','L ICA']]\n",
    "testy = [['L vertebral', ' basilar', 'R vertebral'],['L vertebral'],['L ICA']]\n",
    "sg = stats_generator(testX, testy)\n",
    "print(sg.encoded_X)\n",
    "print(sg.encoded_y)\n",
    "sg.oneHotEncoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ICA       1.00      1.00      1.00         1\n",
      "         MCA       0.00      0.00      0.00         0\n",
      "     basilar       1.00      1.00      1.00         1\n",
      "   vertebral       1.00      0.50      0.67         2\n",
      "\n",
      "   micro avg       0.75      0.75      0.75         4\n",
      "   macro avg       0.75      0.62      0.67         4\n",
      "weighted avg       1.00      0.75      0.83         4\n",
      " samples avg       0.83      0.83      0.78         4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(sg.encoded_y, sg.encoded_X, target_names=sg.oneHotEncoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "[[1 1 1]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['L', 'None', 'R'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testX = [['R basilar'],['L vertebral'],[]]\n",
    "# testy = [['L vertebral', ' basilar', 'R vertebral'],['L vertebral'],['R MCA']]\n",
    "sg = stats_generator(testX, testy, \"sides\")\n",
    "print(sg.encoded_X)\n",
    "print(sg.encoded_y)\n",
    "sg.oneHotEncoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           L       1.00      0.67      0.80         3\n",
      "        None       0.00      0.00      0.00         1\n",
      "           R       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      0.60      0.75         5\n",
      "   macro avg       0.67      0.56      0.60         5\n",
      "weighted avg       0.80      0.60      0.68         5\n",
      " samples avg       1.00      0.78      0.83         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(sg.encoded_y, sg.encoded_X, target_names=sg.oneHotEncoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class labels_extractor:\n",
    "    \n",
    "    def __init__(self,side_column,occlusion_column):\n",
    "        self.side_labels = self.extract_occlusion_labels(side_column)\n",
    "        self.occl_labels = self.extract_occlusion_labels(occlusion_column)\n",
    "    \n",
    "    def extract_occlusion_labels(self,df):\n",
    "        #side = (df['Side'])\n",
    "        cleaned_df = list(df.fillna(''))\n",
    "\n",
    "        #df_normalize_both = [df.lower() if \"both\" in df.lower() else df for df in cleaned_df]\n",
    "\n",
    "        df_no_spaces = [df.replace(\" \",\"\") for df in cleaned_df]\n",
    "\n",
    "        df_no_seps = [re.split(\"\\/|&\",df) if (\"/\" in df or \"&\" in df) else df for df in df_no_spaces]\n",
    "\n",
    "\n",
    "        return df_no_seps\n",
    "    \n",
    "    def return_labels(self):\n",
    "        labels = []\n",
    "        for side, occl in zip(self.side_labels,self.occl_labels):\n",
    "            #Occl is '', i.e. no occlusion\n",
    "            if not occl:\n",
    "                labels.append([])\n",
    "            else:\n",
    "                #Lowercase vertebral and basilar to match predictions\n",
    "                if occl in [\"Vertebral\", \"Basilar\"]:\n",
    "                    occl = occl.lower()\n",
    "                #Ex. [\"M2\", \"M1\"]\n",
    "                if isinstance(occl,list):\n",
    "                    #Ex. [\"L\", \"R\"]\n",
    "                    if isinstance(side,list):\n",
    "                        labels.append([\" \".join(tup) for tup in zip(side,occl)])\n",
    "#                         for s, o in zip(side,occl):\n",
    "#                             labels.append(s + \" \" + o)\n",
    "                    else:\n",
    "                        if side:\n",
    "                            side += \" \"\n",
    "                        l = []\n",
    "                        for o in occl:\n",
    "                            if \"both\" in side.lower():\n",
    "                                l.append(\"R \"+ o)\n",
    "                                l.append(\"L \"+ o)\n",
    "                            else:\n",
    "                                l.append(side + o)\n",
    "                        labels.append(l)\n",
    "                else:\n",
    "                    if side:\n",
    "                        side += \" \"\n",
    "                    if \"both\" in side.lower():\n",
    "                        labels.append([\"R \"+ occl, \"L \"+ occl])\n",
    "#                         l.append(\"R \"+ occl)\n",
    "#                         l.append(\"L \"+ occl)\n",
    "                    else:\n",
    "                        labels.append([side + occl])\n",
    "        return labels\n",
    "                        \n",
    "                    \n",
    "\n",
    "    \n",
    "# side_labels = extract_occlusion_labels(filtered_df['Side'])\n",
    "# side_labels\n",
    "le = labels_extractor(filtered_df['Side'],filtered_df['site occlusion'])\n",
    "le.side_labels[421]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occl_labels = extract_occlusion_labels(filtered_df['site occlusion'])\n",
    "# occl_labels\n",
    "# le.occl_labels[452]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = le.return_labels()\n",
    "# labels[452]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels[452]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_occlusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_occlusions == labels[129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class test_occlusions:\n",
    "    \n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        self.df_report_column = self.df[\"Report Text\"]\n",
    "        \n",
    "        #For output excel file\n",
    "        self.output_dir = \"\"\n",
    "        \n",
    "        #Class to extract desired portions of reports\n",
    "        self.report_extractor = report_text_extractor(self.df_report_column)\n",
    "        #Cerebral artery templates\n",
    "        #NOT USED\n",
    "        self.report_ca_templates = self.report_extractor.get_report_templates()\n",
    "        #End 'Impression:' section\n",
    "        self.impressions = self.report_extractor.get_impressions()\n",
    "        #CTP Perfusion section\n",
    "        self.ctp_sections = self.report_extractor.get_ctps()\n",
    "        \n",
    "        self.ctpve = ctp_volumes_extractor(self.ctp_sections)\n",
    "        self.ctpve.extract_volumes()\n",
    "        ##List of tuples of format (ischemic core volume,hypoperfusion volume)\n",
    "        self.ctp_volumes = self.ctpve.get_volumes()\n",
    "        \n",
    "        #Goes reads through all templates and populates a dictionary\n",
    "        self.ca_templates_reader = ca_templates_reader(self.df_report_column)\n",
    "        self.ca_templates_reader.compile_regex()\n",
    "        self.ca_templates_dicts = self.ca_templates_reader.read_templates()\n",
    "        \n",
    "        #Setting up pyConText\n",
    "        self.mod_path = \"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/modifiers.yml\"\n",
    "        self.targ_path = \"/mnt/c/Users/zohai/Documents/Research/Neuro_ML/Sample_Data/targets.yml\"\n",
    "        self.itemData_init = pyConText_itemData_initializer(self.mod_path,self.targ_path)\n",
    "        self.modifiers = self.itemData_init.get_itemData()[0]\n",
    "        self.targets = self.itemData_init.get_itemData()[1]\n",
    "        \n",
    "        #Finding possible occlusions based on cerebral arteries templates\n",
    "        self.ca_to_investigate = []\n",
    "        self.pogs = []\n",
    "        for template in self.ca_templates_dicts:\n",
    "            pog = possible_occlusions_generator(template)\n",
    "            self.pogs.append(pog)\n",
    "            self.ca_to_investigate.append(pog.get_possibilities())\n",
    "        \n",
    "        #Investigating the possible occlusions of the impressions of each report\n",
    "        #Using pyConText (used by occlusions_investigator)\n",
    "        \n",
    "        \n",
    "        #self.predicted_occlusions -> Raw occlusions found, used to populate self.occlusion_labels,\n",
    "        # self.predicted_vessels, and self.predicted_segments\n",
    "        self.predicted_occlusions = []\n",
    "        #For debugging, -1 represents flag of uncertainty\n",
    "        self.possibility_ratings = []\n",
    "        self.markups = []\n",
    "        self.unmatched_occlusion_sents = []\n",
    "        self.uncertain_occlusions = []\n",
    "        \n",
    "        for possibilities,impression in zip(self.ca_to_investigate,self.impressions):\n",
    "            \n",
    "            \n",
    "            oi = occlusions_investigator(possibilities,impression,self.modifiers,self.targets)\n",
    "            self.predicted_occlusions.append(oi.get_occlusions())\n",
    "            self.possibility_ratings.append(oi.get_possibility_ratings())\n",
    "            self.markups.append(oi.get_markups())\n",
    "            self.unmatched_occlusion_sents.append(oi.get_unmatched_occlusion_sents())\n",
    "            self.uncertain_occlusions.append(oi.get_uncertain_occlusions())\n",
    "            \n",
    "            \n",
    "        self.pred_occl_statuses = [0 if not occl else 1 for occl in self.predicted_occlusions]\n",
    "        self.occl_status_labels = [1 if val > 0 else val for val in self.df['LVO']]\n",
    "        \n",
    "        self.pred_classifier = vessel_site_classifier(self.predicted_occlusions)\n",
    "        self.predicted_vessels, self.predicted_segments = self.pred_classifier.classify()\n",
    "        \n",
    "        #Extracting labels to verify the predicted occlusions\n",
    "        self.labels_extractor = labels_extractor(self.df['Side'],self.df['site occlusion'])\n",
    "        self.labels = self.labels_extractor.return_labels()\n",
    "        \n",
    "        self.labels_classifier = vessel_site_classifier(self.labels)\n",
    "        self.labeled_vessels, self.labeled_segments = self.labels_classifier.classify()        \n",
    "                \n",
    "        #self.differences = [(prediction,label) if list(set(map(self.lower,prediction))^set(map(self.lower,label))) \n",
    "                       #else () for prediction,label in zip(self.predicted_occlusions,self.labels) ]\n",
    "        self.diffs = self.differences(self.predicted_occlusions,self.labels)\n",
    "        #Indices of differences of occlusion status\n",
    "        self.occl_status_diffs = np.where((abs(np.subtract(self.pred_occl_statuses,self.occl_status_labels)).astype(bool)))\n",
    "        self.vessels_diffs = self.differences(self.predicted_vessels,self.labeled_vessels)\n",
    "        self.segments_diffs = self.differences(self.predicted_segments,self.labeled_segments)\n",
    "    \n",
    "\n",
    "    \n",
    "    def differences(self,preds,labels):\n",
    "        #Compare lower case labels to lower case predictions\n",
    "        #Set ^ operation returns list with elements unique to either prediction or label but not both\n",
    "        lower = lambda x:x.lower()\n",
    "        differences = []\n",
    "        for prediction,label in zip(preds,labels):\n",
    "            if list(set(map(lower,prediction))^set(map(lower,label))):\n",
    "                differences.append((prediction,label))\n",
    "            else:\n",
    "                differences.append(())\n",
    "        return differences\n",
    "    \n",
    "     \n",
    "    #An uncertainty flag for user review\n",
    "    #may show a template error\n",
    "    #returns the sentences and report indices where occlusions are mentioned in the findings \n",
    "    #but the algo did not register any at those same cerebral arteries in template\n",
    "    def get_unmatched_occlusions(self):\n",
    "        return [(sent,i) for sent,i in zip(self.unmatched_occlusion_sents,range(len(self.unmatched_occlusion_sents))) if sent]\n",
    "    \n",
    "    #For user review of instances where occlusion is possible because of similar terms used, but occlusion not used\n",
    "    def get_uncertain_occlusions(self):\n",
    "        return [(uo,i) for uo,i in zip(self.uncertain_occlusions,range(len(self.uncertain_occlusions))) if uo] \n",
    "        #return self.uncertain_occlusions\n",
    "    \n",
    "    def get_occlusion_metrics(self):\n",
    "            #Precision/PPV = TP/TP+FP i.e. out of true calls, how many actually true\n",
    "            #Recall = TP/TP+FN i.e. out of all trues, how many algo called true\n",
    "            #F1-score = Accuracy taking into account precision and recall\n",
    "            print(classification_report(to.occl_status_labels,to.pred_occl_statuses))\n",
    "    \n",
    "    def get_vessel_metrics(self):\n",
    "        vessel_sg = stats_generator(self.predicted_vessels,self.labeled_vessels)\n",
    "        vessel_sg.getStats()\n",
    "        \n",
    "    def get_segment_metrics(self):\n",
    "        segment_sg = stats_generator(self.predicted_segments,self.labeled_segments)\n",
    "        segment_sg.getStats()\n",
    "        \n",
    "    def get_side_metrics(self):\n",
    "        side_sg = stats_generator(self.predicted_segments,self.labeled_segments,\"sides\")\n",
    "        side_sg.getStats()\n",
    "    \n",
    "    def get_occlusion_accuracy(self):\n",
    "        #Subtracts the 1's and 0's of the predicted and labeled occl statuses and turns them positive, sums, divides\n",
    "        #by the total to get the error rate, then yields accuracy by 1-error_rate\n",
    "        return(1-(sum(abs(np.subtract(self.pred_occl_statuses,self.occl_status_labels)))/len(self.occl_status_labels)))\n",
    "    \n",
    "    def get_vessel_accuracy(self):\n",
    "        return self.get_accuracy(self.vessels_diffs)\n",
    "    \n",
    "    def get_segments_accuracy(self):\n",
    "        return self.get_accuracy(self.segments_diffs)\n",
    "    \n",
    "    def get_raw_accuracy(self):\n",
    "        return self.get_accuracy(self.diffs)\n",
    "        \n",
    "        \n",
    "    def get_accuracy(self,diffs_list):\n",
    "        score = 0\n",
    "        for tup in diffs_list:\n",
    "            if not tup:\n",
    "                score += 1\n",
    "        return(score/len(diffs_list))\n",
    "    \n",
    "    def get_volumes(self):\n",
    "        return self.ctp_volumes\n",
    "      \n",
    "    #Writes to an excel file\n",
    "    #(Takes input excel file, adds columns, and outputs it)\n",
    "    #Input: Input excel file dataframe, input excel file path\n",
    "    def write_to_excel(self,raw_df, raw_dir):\n",
    "        output_df = raw_df\n",
    "        self.output_dir = raw_dir.split(\".\")[0] + \"_AUTOMATED_OCCLUSION_PREDICTIONS.xlsx\"\n",
    "        outputs = {\"pred_occlusion\": self.pred_occl_statuses,\n",
    "                   \"pred_vessels\": self.predicted_vessels,\n",
    "                   \"pred_segments\": self.predicted_segments,\n",
    "                   \"true_vessels\": self.labeled_vessels,\n",
    "                   \"true_segments\": self.labeled_segments,\n",
    "                   \"volumes\": self.get_volumes(),\n",
    "                   \"unmatched_occlusions\": self.unmatched_occlusion_sents,\n",
    "                   \"uncertain_occlusions\": self.uncertain_occlusions\n",
    "                  }\n",
    "        for col_name, col_data in outputs.items():\n",
    "            output_df[col_name] = col_data\n",
    "        output_df.to_excel(self.output_dir,index=False)\n",
    "    \n",
    "    def get_output_excel_path(self):\n",
    "        return self.output_dir\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200    0\n",
      "201    0\n",
      "202    1\n",
      "203    0\n",
      "204    0\n",
      "205    0\n",
      "206    1\n",
      "207    1\n",
      "208    0\n",
      "209    0\n",
      "Name: LVO, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df[\"LVO\"].iloc[200:210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ('','')\n",
    "if not q:\n",
    "    print('not q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/pyConTextNLP/itemData.py:40: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  context_items =  [contextItem((d[\"Lex\"],\n"
     ]
    }
   ],
   "source": [
    "to = test_occlusions(filtered_df)\n",
    "#print(to.report_ca_templates[90])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.pred_occl_statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.occl_status_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       320\n",
      "           1       0.92      0.85      0.89       122\n",
      "\n",
      "    accuracy                           0.94       442\n",
      "   macro avg       0.93      0.91      0.92       442\n",
      "weighted avg       0.94      0.94      0.94       442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "to.get_occlusion_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ACA       0.60      1.00      0.75         3\n",
      "         CCA       0.33      0.50      0.40         2\n",
      "         ICA       0.83      0.87      0.85        23\n",
      "         MCA       0.90      0.84      0.87        86\n",
      "         PCA       1.00      0.71      0.83         7\n",
      "     basilar       0.67      1.00      0.80         4\n",
      "   vertebral       0.47      0.82      0.60        11\n",
      "\n",
      "   micro avg       0.80      0.84      0.82       136\n",
      "   macro avg       0.69      0.82      0.73       136\n",
      "weighted avg       0.84      0.84      0.83       136\n",
      " samples avg       0.22      0.23      0.22       136\n",
      "\n",
      "[[[437   2]\n",
      "  [  0   3]]\n",
      "\n",
      " [[438   2]\n",
      "  [  1   1]]\n",
      "\n",
      " [[415   4]\n",
      "  [  3  20]]\n",
      "\n",
      " [[348   8]\n",
      "  [ 14  72]]\n",
      "\n",
      " [[435   0]\n",
      "  [  2   5]]\n",
      "\n",
      " [[436   2]\n",
      "  [  0   4]]\n",
      "\n",
      " [[421  10]\n",
      "  [  2   9]]]\n",
      "0.9027149321266968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/c/Users/zohai/Documents/Research/Neuro_ML/miniconda3/envs/neuroml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "to.get_vessel_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.00      0.00      0.00         0\n",
      "          A2       0.67      0.67      0.67         3\n",
      "          M1       0.85      0.80      0.82        44\n",
      "          M2       0.82      0.84      0.83        43\n",
      "          M3       0.00      0.00      0.00         3\n",
      "          M4       0.00      0.00      0.00         0\n",
      "          P1       1.00      1.00      1.00         1\n",
      "          P2       0.67      0.67      0.67         3\n",
      "          P3       1.00      1.00      1.00         1\n",
      "          V1       0.00      0.00      0.00         0\n",
      "          V2       0.00      0.00      0.00         0\n",
      "          V3       0.50      0.25      0.33         4\n",
      "          V4       0.50      0.50      0.50         8\n",
      "\n",
      "   micro avg       0.73      0.75      0.74       110\n",
      "   macro avg       0.46      0.44      0.45       110\n",
      "weighted avg       0.77      0.75      0.76       110\n",
      " samples avg       0.17      0.17      0.17       110\n",
      "\n",
      "[[[440   2]\n",
      "  [  0   0]]\n",
      "\n",
      " [[438   1]\n",
      "  [  1   2]]\n",
      "\n",
      " [[392   6]\n",
      "  [  9  35]]\n",
      "\n",
      " [[391   8]\n",
      "  [  7  36]]\n",
      "\n",
      " [[434   5]\n",
      "  [  3   0]]\n",
      "\n",
      " [[441   1]\n",
      "  [  0   0]]\n",
      "\n",
      " [[441   0]\n",
      "  [  0   1]]\n",
      "\n",
      " [[438   1]\n",
      "  [  1   2]]\n",
      "\n",
      " [[441   0]\n",
      "  [  0   1]]\n",
      "\n",
      " [[441   1]\n",
      "  [  0   0]]\n",
      "\n",
      " [[441   1]\n",
      "  [  0   0]]\n",
      "\n",
      " [[437   1]\n",
      "  [  3   1]]\n",
      "\n",
      " [[430   4]\n",
      "  [  4   4]]]\n",
      "0.9072398190045249\n"
     ]
    }
   ],
   "source": [
    "to.get_segment_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           L       0.86      0.79      0.83        48\n",
      "           R       0.86      0.81      0.83        52\n",
      "\n",
      "   micro avg       0.86      0.80      0.83       100\n",
      "   macro avg       0.86      0.80      0.83       100\n",
      "weighted avg       0.86      0.80      0.83       100\n",
      " samples avg       0.18      0.18      0.18       100\n",
      "\n",
      "[[[388   6]\n",
      "  [ 10  38]]\n",
      "\n",
      " [[383   7]\n",
      "  [ 10  42]]]\n",
      "0.9298642533936652\n"
     ]
    }
   ],
   "source": [
    "to.get_side_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(311, 9, 18, 104)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(to.occl_status_labels,to.pred_occl_statuses).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R P2'], ['R P2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], ['R A2']),\n",
       " ([], []),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L V4', 'R V4'], []),\n",
       " ([], []),\n",
       " ([], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], ['R M3']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1', 'R M2'], []),\n",
       " (['L V4'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L V4'], ['L V4']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R V4'], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " (['R M1', 'R M2', 'R M3'], []),\n",
       " ([], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R A2', 'R A2'], ['R A2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L V1', 'R V1'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], ['L P2']),\n",
       " (['R M1', 'R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], ['R V3', 'R V4']),\n",
       " (['L M1', 'L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1', 'L M2', 'L M4'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R P1'], ['R P1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R P3', 'L P2', 'R P2'], ['R P3', 'L V4']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2', 'R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R A2'], ['R A2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], ['R M2', 'R M3']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2', 'R M3'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], ['R V3', 'R V4']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L A1', 'L M1'], []),\n",
       " ([], []),\n",
       " (['L M2', 'R M2'], ['R M2', 'L M2']),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L A1', 'L A2', 'L M1', 'L M2'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L V3', 'L V4'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], ['L M2']),\n",
       " ([], ['L M3']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2', 'R M3'], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R P2'], ['R P2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['R M2', 'R M3'], ['R M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2', 'L M3'], ['L M2']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], []),\n",
       " (['R M2'], ['R M1', 'R M2']),\n",
       " ([], ['L M1']),\n",
       " (['L M1', 'L M1'], ['L M1']),\n",
       " ([], ['L V3', 'L V4']),\n",
       " ([], ['L M1', 'L M2']),\n",
       " (['L V2', 'L V3', 'L V4'], ['L V3', 'L V4']),\n",
       " (['L V4'], ['L V4']),\n",
       " (['L M1', 'L M2'], ['L M1', 'L M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " ([], []),\n",
       " (['R M1', 'R M2'], ['R M1']),\n",
       " ([], []),\n",
       " ([], ['R M1', 'R M2']),\n",
       " (['R V4'], ['R V4']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1', 'R M2'], ['R M1']),\n",
       " ([], []),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], ['L M1']),\n",
       " ([], []),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M1'], ['R M1'])]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segs = list(zip(to.predicted_segments, to.labeled_segments))\n",
    "segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = ['L M1', ' basilar', 'M']\n",
    "seg_m1 = lambda x: int(not not sum([\"M1\" in l for l in x]))\n",
    "seg_m2 = lambda x: int(not not sum([\"M2\" in l for l in x]))\n",
    "seg_m2(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['R M1'], ['R M1']),\n",
       " ([], ['R M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], ['L M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], ['R M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1', 'R M2'], ['R M2']),\n",
       " (['L M1', 'L M2'], ['L M2']),\n",
       " (['L M1', 'L M2', 'L M4'], ['L M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2', 'R M2'], ['R M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " ([], ['R M2']),\n",
       " ([], ['R M2', 'R M3']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2', 'R M2'], ['R M2', 'L M2']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], ['L M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], ['L M2']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M2', 'R M3'], ['R M2']),\n",
       " (['L M2', 'L M3'], ['L M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " ([], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M2'], ['R M1', 'R M2']),\n",
       " ([], ['L M1']),\n",
       " (['L M1', 'L M1'], ['L M1']),\n",
       " ([], ['L M1', 'L M2']),\n",
       " (['L M1', 'L M2'], ['L M1', 'L M2']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1', 'R M2'], ['R M1']),\n",
       " ([], ['R M1', 'R M2']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M2'], ['R M2']),\n",
       " (['R M1', 'R M2'], ['R M1']),\n",
       " (['L M1'], ['L M1']),\n",
       " (['R M1'], ['R M1']),\n",
       " ([], ['L M1']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['L M2'], ['L M2']),\n",
       " (['R M1'], ['R M1'])]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only M1 or M2s in segs\n",
    "segs = [(segs[i][0],segs[i][1]) for i in range(len(segs)) if (seg_m1(segs[i][1]) or seg_m2(segs[i][1]))]\n",
    "segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted  labels\n",
       "0           1       1\n",
       "1           0       1\n",
       "2           0       0\n",
       "3           1       1\n",
       "4           0       0\n",
       "..        ...     ...\n",
       "76          1       1\n",
       "77          0       1\n",
       "78          0       0\n",
       "79          0       0\n",
       "80          1       1\n",
       "\n",
       "[81 rows x 2 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_m1_status_df = pd.DataFrame([(seg_m1(p),seg_m1(l)) for (p,l) in segs])\n",
    "seg_m1_status_df.columns = [\"predicted\",\"labels\"]\n",
    "seg_m1_status_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m1_df = pd.DataFrame([(seg_m1(p),seg_m1(l)) for (p,l) in segs])\n",
    "# seg_m1_df.columns = [\"predicted\",\"labels\"]\n",
    "# seg_m2_df = pd.DataFrame([(seg_m2(p),seg_m2(l)) for (p,l) in segs])\n",
    "# seg_m2_df.columns = [\"predicted\",\"labels\"]\n",
    "# seg_m1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m1_df.labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m1_df.predicted.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m2_df.labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_m2_df.predicted.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.86        37\n",
      "           1       0.95      0.80      0.86        44\n",
      "\n",
      "    accuracy                           0.86        81\n",
      "   macro avg       0.87      0.87      0.86        81\n",
      "weighted avg       0.88      0.86      0.86        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(seg_m1_status_df.labels, seg_m1_status_df.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 2, 9, 35)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(seg_m1_status_df.labels, seg_m1_status_df.predicted).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted  labels\n",
       "0           0       0\n",
       "1           0       0\n",
       "2           1       1\n",
       "3           0       0\n",
       "4           1       1\n",
       "..        ...     ...\n",
       "76          0       0\n",
       "77          0       0\n",
       "78          1       1\n",
       "79          1       1\n",
       "80          0       0\n",
       "\n",
       "[81 rows x 2 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_m2_df = pd.DataFrame([(seg_m2(p),seg_m2(l)) for (p,l) in segs])\n",
    "seg_m2_df.columns = [\"predicted\",\"labels\"]\n",
    "seg_m2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.88        38\n",
      "           1       0.92      0.84      0.88        43\n",
      "\n",
      "    accuracy                           0.88        81\n",
      "   macro avg       0.88      0.88      0.88        81\n",
      "weighted avg       0.88      0.88      0.88        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(seg_m2_df.labels, seg_m2_df.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 3, 7, 36)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(seg_m2_df.labels, seg_m2_df.predicted).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R PCA']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.predicted_vessels[163]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R PCA']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.labeled_vessels[163]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_df = raw_df\n",
    "# output_dir = raw_dir.split(\".\")[0] + \"_AUTOMATED_OCCLUSION_PREDICTIONS.xlsx\"\n",
    "# outputs = {\"pred_occlusion\":to.pred_occl_statuses,\n",
    "#            \"pred_vessels\":to.predicted_vessels,\n",
    "#            \"pred_segments\":to.predicted_segments,\n",
    "#            \"volumes\":to.get_volumes()\n",
    "#           }\n",
    "# for col_name, col_data in outputs.items():\n",
    "#     output_df[col_name] = col_data\n",
    "\n",
    "# output_df.to_excel(output_dir,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.write_to_excel(raw_df,raw_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Organization</th>\n",
       "      <th>Point of Care</th>\n",
       "      <th>Accession Number</th>\n",
       "      <th>Modality</th>\n",
       "      <th>Exam Code</th>\n",
       "      <th>Exam Description</th>\n",
       "      <th>CPT Code</th>\n",
       "      <th>Report Text</th>\n",
       "      <th>LVO</th>\n",
       "      <th>Side</th>\n",
       "      <th>...</th>\n",
       "      <th>Report Finalized By</th>\n",
       "      <th>Report Finalized Date</th>\n",
       "      <th>pred_occlusion</th>\n",
       "      <th>pred_vessels</th>\n",
       "      <th>pred_segments</th>\n",
       "      <th>true_vessels</th>\n",
       "      <th>true_segments</th>\n",
       "      <th>volumes</th>\n",
       "      <th>unmatched_occlusions</th>\n",
       "      <th>uncertain_occlusions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 0105</td>\n",
       "      <td>55427372</td>\n",
       "      <td>CTA</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>70496</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Eldaya, Rami W</td>\n",
       "      <td>2020-09-20 17:51:50.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 0082</td>\n",
       "      <td>55427150</td>\n",
       "      <td>CTA</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>70496</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Goyal, Manu Shri</td>\n",
       "      <td>2020-09-20 19:32:25.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 94ICU</td>\n",
       "      <td>55427201</td>\n",
       "      <td>CTA</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>70496</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Goyal, Manu Shri</td>\n",
       "      <td>2020-09-20 20:04:05.999999</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>(0, 5)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 94ICU</td>\n",
       "      <td>55426263</td>\n",
       "      <td>CTA</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>70496</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>1</td>\n",
       "      <td>R/L</td>\n",
       "      <td>...</td>\n",
       "      <td>Goyal, Manu Shri</td>\n",
       "      <td>2020-09-20 09:56:07.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>['R MCA']</td>\n",
       "      <td>['R M1']</td>\n",
       "      <td>['R MCA']</td>\n",
       "      <td>['R M1']</td>\n",
       "      <td>(51, 42)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BJH</td>\n",
       "      <td>BJH 0082</td>\n",
       "      <td>55408274</td>\n",
       "      <td>CTA</td>\n",
       "      <td>IMG575</td>\n",
       "      <td>CTA/CTP RAPID STROKE (C)</td>\n",
       "      <td>70496</td>\n",
       "      <td>EXAMINATION: Computed tomography angiography (...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Vo, Katie D.</td>\n",
       "      <td>2020-09-17 07:33:09.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Organization Point of Care  Accession Number Modality Exam Code  \\\n",
       "0          BJH      BJH 0105          55427372      CTA    IMG575   \n",
       "1          BJH      BJH 0082          55427150      CTA    IMG575   \n",
       "2          BJH     BJH 94ICU          55427201      CTA    IMG575   \n",
       "3          BJH     BJH 94ICU          55426263      CTA    IMG575   \n",
       "4          BJH      BJH 0082          55408274      CTA    IMG575   \n",
       "\n",
       "           Exam Description  CPT Code  \\\n",
       "0  CTA/CTP RAPID STROKE (C)     70496   \n",
       "1  CTA/CTP RAPID STROKE (C)     70496   \n",
       "2  CTA/CTP RAPID STROKE (C)     70496   \n",
       "3  CTA/CTP RAPID STROKE (C)     70496   \n",
       "4  CTA/CTP RAPID STROKE (C)     70496   \n",
       "\n",
       "                                         Report Text  LVO Side  ...  \\\n",
       "0  EXAMINATION: Computed tomography angiography (...    0  NaN  ...   \n",
       "1  EXAMINATION: Computed tomography angiography (...    0  NaN  ...   \n",
       "2  EXAMINATION: Computed tomography angiography (...    0  NaN  ...   \n",
       "3  EXAMINATION: Computed tomography angiography (...    1  R/L  ...   \n",
       "4  EXAMINATION: Computed tomography angiography (...    0  NaN  ...   \n",
       "\n",
       "  Report Finalized By      Report Finalized Date  pred_occlusion pred_vessels  \\\n",
       "0      Eldaya, Rami W 2020-09-20 17:51:50.000000               0           []   \n",
       "1    Goyal, Manu Shri 2020-09-20 19:32:25.000000               0           []   \n",
       "2    Goyal, Manu Shri 2020-09-20 20:04:05.999999               0           []   \n",
       "3    Goyal, Manu Shri 2020-09-20 09:56:07.000000               1    ['R MCA']   \n",
       "4        Vo, Katie D. 2020-09-17 07:33:09.000000               0           []   \n",
       "\n",
       "   pred_segments  true_vessels true_segments   volumes  unmatched_occlusions  \\\n",
       "0             []            []            []    (0, 0)                    []   \n",
       "1             []            []            []    (0, 0)                    []   \n",
       "2             []            []            []    (0, 5)                    []   \n",
       "3       ['R M1']     ['R MCA']      ['R M1']  (51, 42)                    []   \n",
       "4             []            []            []    (0, 0)                    []   \n",
       "\n",
       "   uncertain_occlusions  \n",
       "0                    []  \n",
       "1                    []  \n",
       "2                    []  \n",
       "3                    []  \n",
       "4                    []  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_excel(to.get_output_excel_path()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_pred_vessels = mlb_fit.transform(to.predicted_vessels)\n",
    "# bin_labeled_vessels = mlb_fit.transform(to.labeled_vessels)\n",
    "# metrics.confusion_matrix(bin_labeled_vessels.argmax(axis=1), bin_pred_vessels.argmax(axis=1), labels=mlb_fit.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  5,  45,  49,  55,  60,  81,  82, 103, 126, 137, 189, 199, 214,\n",
       "        219, 224, 233, 270, 334, 337, 350, 351, 363, 402, 413, 416, 428,\n",
       "        437]),)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.occl_status_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R MCA', 'R ICA', 'R CCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R MCA', 'R CCA'], ['R CCA']),\n",
       " (['L vertebral'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA'], []),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', ' basilar'], ['L vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R vertebral', ' basilar'], [' basilar']),\n",
       " (),\n",
       " (),\n",
       " (['R MCA'], []),\n",
       " ([], ['R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R vertebral', 'L vertebral'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L PCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R vertebral', 'L PCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R PCA', 'R vertebral', 'L vertebral', 'L PCA'], ['R PCA', 'L vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'R vertebral'], ['L MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R MCA', 'R ICA'], ['R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R vertebral', 'L vertebral', ' basilar'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R MCA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R vertebral']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L ICA']),\n",
       " (['L MCA', 'L vertebral'], ['L MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L ICA', 'L ACA'], ['L ICA']),\n",
       " (),\n",
       " (),\n",
       " (['R MCA', 'R vertebral'], ['R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L ICA'], ['L MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L CCA', 'L ICA'], ['L ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L ACA'], ['L ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L MCA']),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral'], []),\n",
       " (['L MCA', 'L ICA'], ['L ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L MCA']),\n",
       " ([], ['L MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R MCA'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R MCA', 'R ICA'], ['R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R PCA', 'R vertebral'], ['R PCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'R MCA'], ['L MCA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L MCA']),\n",
       " (),\n",
       " (['L vertebral', ' basilar'], ['L vertebral', 'L basilar']),\n",
       " ([], ['L MCA']),\n",
       " (),\n",
       " (['L vertebral', ' basilar'], ['L vertebral', 'L basilar']),\n",
       " (['L CCA', 'L ICA', 'L MCA'], ['L MCA', 'L ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L ICA'], ['L ICA']),\n",
       " ([], ['R MCA']),\n",
       " (['R vertebral', ' basilar'], ['R vertebral', 'R basilar']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L MCA']),\n",
       " (['L ICA', 'R ICA'], ['L ICA']),\n",
       " (),\n",
       " (),\n",
       " ()]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.vessels_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " ['R MCA', 'R ICA', 'R CCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R PCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L vertebral'],\n",
       " [],\n",
       " [],\n",
       " ['R ACA'],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R vertebral', 'L vertebral'],\n",
       " ['L ICA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R CCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L vertebral'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [' basilar'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA', 'L ICA'],\n",
       " [],\n",
       " [],\n",
       " ['R ACA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L PCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R vertebral', 'L PCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA', 'L ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R PCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R PCA', 'L vertebral'],\n",
       " [],\n",
       " [],\n",
       " ['L ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA', 'L ICA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R ACA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " ['R ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R vertebral'],\n",
       " ['R MCA'],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['L ICA'],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['L ICA'],\n",
       " [],\n",
       " ['R MCA', 'L MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " ['L ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L ICA'],\n",
       " [],\n",
       " [],\n",
       " ['L ICA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R vertebral'],\n",
       " [],\n",
       " [],\n",
       " ['R ICA'],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L ICA'],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L ICA'],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA', 'R ICA'],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R PCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['R MCA'],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " ['R ICA'],\n",
       " ['R MCA'],\n",
       " ['L MCA'],\n",
       " ['L MCA', 'L ICA'],\n",
       " ['L vertebral', 'L basilar'],\n",
       " ['L MCA'],\n",
       " ['L vertebral'],\n",
       " ['L vertebral', 'L basilar'],\n",
       " ['L MCA', 'L ICA'],\n",
       " ['R MCA'],\n",
       " ['R MCA'],\n",
       " [],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " ['R MCA'],\n",
       " ['R MCA'],\n",
       " ['L ICA'],\n",
       " ['R MCA'],\n",
       " ['R vertebral', 'R basilar'],\n",
       " ['R MCA'],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " ['R MCA', 'R ICA'],\n",
       " ['R PCA'],\n",
       " ['L MCA'],\n",
       " ['R MCA'],\n",
       " ['L MCA'],\n",
       " ['L ICA'],\n",
       " ['L MCA'],\n",
       " ['L MCA'],\n",
       " ['R MCA']]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.labeled_vessels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8438914027149321"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.get_raw_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9389140271493213"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.get_occlusion_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8891402714932126"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.get_vessel_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9072398190045249"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.get_segments_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(),\n",
       " (),\n",
       " (),\n",
       " (['R M1'], ['R M1', 'L Vertebral']),\n",
       " (),\n",
       " ([], ['R CCA', 'R ICA', 'R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R PCA', 'R P2'], ['R P2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ACA'], ['R A2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'L V4', 'R V4'], ['R vertebral', 'L vertebral']),\n",
       " (),\n",
       " ([], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M3']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R CCA', 'R M1', 'R M2'], ['R CCA']),\n",
       " (['L vertebral', 'L V4'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA'], []),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'basilar', 'L V4'], [' Basilar', 'L V4']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R V4', 'basilar'], ['basilar']),\n",
       " (),\n",
       " (),\n",
       " (['R M1', 'R M2', 'R M3'], []),\n",
       " ([], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ACA', 'R A2', 'R A2'], ['R A2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'L V1', 'R vertebral', 'R V1'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L P2']),\n",
       " (['R M1', 'R M2'], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R V3', 'R V4', 'L PCA']),\n",
       " (['L M1', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L ICA', 'L M1', 'L M2', 'L M4'], ['L ICA', 'L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'R vertebral', 'R P3', 'L P2', 'R PCA', 'R P2'],\n",
       "  ['R P3', 'L V4']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M1'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M2', 'R M3']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'R M1'], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'R vertebral', 'basilar'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M2', 'R M3'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R V3', 'R V4']),\n",
       " (),\n",
       " (),\n",
       " (['R MCA', 'R M1'], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L ICA']),\n",
       " (['L M2', 'L vertebral'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L ICA', 'L A1', 'L M1'], ['L ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L ICA', 'L M1'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L CCA', 'L ICA'], ['L ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L A1', 'L A2', 'L M1', 'L M2'], ['L ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (['L vertebral', 'L V3', 'L V4'], []),\n",
       " (['L ICA', 'L MCA'], ['L ICA']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L M2']),\n",
       " ([], ['L M3']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M2', 'R M3'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'R M2'], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R vertebral', 'R P2'], ['R P2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M2', 'R M3'], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (['L M2', 'L M3'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'L M1'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M1']),\n",
       " (),\n",
       " (['R MCA', 'R M1', 'R M2'], ['R M1', 'R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L MCA', 'R MCA'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (['R M2'], ['R M1', 'R M2']),\n",
       " ([], ['L M1']),\n",
       " (),\n",
       " (['L vertebral', 'basilar'], ['L basilar', 'L V3', 'L V4']),\n",
       " ([], ['L M1', 'L M2']),\n",
       " (['L V2', 'L V3', 'L V4'], ['L V3', 'L V4']),\n",
       " (['L V4', 'basilar'], ['L V4', 'L basilar']),\n",
       " (['L CCA', 'L ICA', 'L M1', 'L M2'], ['L ICA', 'L M1', 'L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1', 'R M2'], ['R M1']),\n",
       " (['L ICA', 'L MCA'], ['L ICA']),\n",
       " ([], ['R M1', 'R M2']),\n",
       " (['R vertebral', 'R vertebral', 'basilar', 'R V4'], ['R V4', 'R basilar']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R ICA', 'R M1', 'R M2'], ['R ICA', 'R M1']),\n",
       " (),\n",
       " (['L MCA', 'L M1'], ['L M1']),\n",
       " (),\n",
       " ([], ['L M1']),\n",
       " (['L ICA', 'R ICA'], ['L ICA']),\n",
       " (),\n",
       " (),\n",
       " ()]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = to.diffs\n",
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R A2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L V4', 'R V4'], []),\n",
       " (),\n",
       " ([], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M3']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1', 'R M2'], []),\n",
       " (['L V4'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R V4'], []),\n",
       " (),\n",
       " (),\n",
       " (['R M1', 'R M2', 'R M3'], []),\n",
       " ([], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L V1', 'R V1'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L P2']),\n",
       " (['R M1', 'R M2'], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R V3', 'R V4']),\n",
       " (['L M1', 'L M2'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L M1', 'L M2', 'L M4'], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R P3', 'L P2', 'R P2'], ['R P3', 'L V4']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M2', 'R M3']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M2', 'R M3'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R V3', 'R V4']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L A1', 'L M1'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['L A1', 'L A2', 'L M1', 'L M2'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (['L V3', 'L V4'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L M2']),\n",
       " ([], ['L M3']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M2', 'R M3'], []),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M2', 'R M3'], ['R M2']),\n",
       " (),\n",
       " (),\n",
       " (['L M2', 'L M3'], ['L M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (['R M2'], ['R M1', 'R M2']),\n",
       " ([], ['L M1']),\n",
       " (),\n",
       " ([], ['L V3', 'L V4']),\n",
       " ([], ['L M1', 'L M2']),\n",
       " (['L V2', 'L V3', 'L V4'], ['L V3', 'L V4']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1', 'R M2'], ['R M1']),\n",
       " (),\n",
       " ([], ['R M1', 'R M2']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " (['R M1', 'R M2'], ['R M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ([], ['L M1']),\n",
       " (),\n",
       " (),\n",
       " (),\n",
       " ()]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to.segments_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(([], ['R MCA', 'R ICA', 'R CCA']), 5),\n",
       " (([], ['L MCA']), 45),\n",
       " (([], ['R MCA']), 49),\n",
       " ((['R MCA', 'R CCA'], ['R CCA']), 54),\n",
       " ((['L vertebral'], []), 55),\n",
       " ((['R ICA'], []), 60),\n",
       " ((['L vertebral', ' basilar'], ['L vertebral']), 63),\n",
       " ((['R vertebral', ' basilar'], [' basilar']), 78),\n",
       " ((['R MCA'], []), 81),\n",
       " (([], ['R MCA']), 82),\n",
       " ((['R vertebral', 'L vertebral'], []), 103),\n",
       " (([], ['L PCA']), 126),\n",
       " (([], ['R vertebral', 'L PCA']), 137),\n",
       " ((['R PCA', 'R vertebral', 'L vertebral', 'L PCA'], ['R PCA', 'L vertebral']),\n",
       "  169),\n",
       " ((['L MCA', 'R vertebral'], ['L MCA']), 183),\n",
       " (([], ['R MCA']), 189),\n",
       " (([], ['R MCA']), 199),\n",
       " ((['R MCA', 'R ICA'], ['R MCA']), 206),\n",
       " ((['L vertebral'], []), 214),\n",
       " ((['R vertebral', 'L vertebral', ' basilar'], []), 219),\n",
       " ((['R MCA'], []), 224),\n",
       " (([], ['R vertebral']), 233),\n",
       " (([], ['L ICA']), 270),\n",
       " ((['L MCA', 'L vertebral'], ['L MCA']), 271),\n",
       " ((['L MCA', 'L ICA', 'L ACA'], ['L ICA']), 280),\n",
       " ((['R MCA', 'R vertebral'], ['R MCA']), 283),\n",
       " ((['L MCA', 'L ICA'], ['L MCA']), 299),\n",
       " ((['L CCA', 'L ICA'], ['L ICA']), 306),\n",
       " ((['L MCA', 'L ACA'], ['L ICA']), 321),\n",
       " (([], ['L MCA']), 334),\n",
       " ((['L vertebral'], []), 337),\n",
       " ((['L MCA', 'L ICA'], ['L ICA']), 338),\n",
       " (([], ['L MCA']), 350),\n",
       " (([], ['L MCA']), 351),\n",
       " ((['R MCA'], []), 363),\n",
       " ((['R MCA', 'R ICA'], ['R MCA']), 375),\n",
       " ((['R PCA', 'R vertebral'], ['R PCA']), 384),\n",
       " (([], ['R MCA']), 402),\n",
       " ((['L MCA', 'R MCA'], ['L MCA']), 409),\n",
       " (([], ['L MCA']), 413),\n",
       " ((['L vertebral', ' basilar'], ['L vertebral', 'L basilar']), 415),\n",
       " (([], ['L MCA']), 416),\n",
       " ((['L vertebral', ' basilar'], ['L vertebral', 'L basilar']), 418),\n",
       " ((['L CCA', 'L ICA', 'L MCA'], ['L MCA', 'L ICA']), 419),\n",
       " ((['L MCA', 'L ICA'], ['L ICA']), 427),\n",
       " (([], ['R MCA']), 428),\n",
       " ((['R vertebral', ' basilar'], ['R vertebral', 'R basilar']), 429),\n",
       " (([], ['L MCA']), 437),\n",
       " ((['L ICA', 'R ICA'], ['L ICA']), 438)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_vessels_diffs = list(filter(lambda xy:True if xy[0] else False,(zip(to.vessels_diffs,range(len(to.vessels_diffs))))))\n",
    "ind_vessels_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['  The right vertebral artery is dominant which is severely narrowed/occluded at the origin but reconstitutes distally'],\n",
       "  43),\n",
       " (['  Occlusion of distal right M3/M4 anterior branch within the area of previously noted hypoattenuation/infarct with a paucity of branches noted within the area as compared to the left side with associated area of perfusion abnormality as above in, keeping with diagnosis of distal MCA infarct',\n",
       "   '  High-grade stenosis/near occlusion of a posterior right distal M3 branch, likely corresponding to the more posterior area of hypoperfusion seen on perfusion imaging'],\n",
       "  49),\n",
       " (['  This may be due to a congenitally diminutive or anomalous vessel origin versus possible occlusion, although there is no associated CT perfusion abnormality'],\n",
       "  107),\n",
       " (['  Findings of subacute infarct involving the left parietal lobe with likely occlusion of the distal left M2 branch or M3 branch'],\n",
       "  129),\n",
       " (['Occlusion of the right M2 segment of the middle cerebral artery with associated abnormal perfusion parameters'],\n",
       "  189),\n",
       " (['  Geographic paucity involving the right parietal lobe likely represents occlusion of the cortical branches of right middle cerebral artery resulting in infarct'],\n",
       "  239),\n",
       " ([' Findings represent a left carotid T-occlusion'], 321),\n",
       " ([' Upon further review and correlation with hyperacute MRI evaluation, there is a occlusion of the left posterior M2 branch (CTA source image 210/669; Coronal MIP images 14-19/56), with areas of hypoattenuation within the left centrum semi-ovale and periventricular white matter'],\n",
       "  331),\n",
       " (['  Given the MRI demonstrates diffusion restriction consistent with an acute stroke in the left thalamus, retrospective evaluation of the CT angiogram of the head and neck was performed and there is note of abrupt occlusion of a distal left M2 branch which is seen on series 11 image 142 and which becomes nonopacified on series 11 image 140',\n",
       "   ' This distal left M2 occlusion seen on CTA likely corresponds to the area of infarction seen on the MRI of the brain'],\n",
       "  334),\n",
       " (['  Nonfilling of the distal right P2 segment of the right posterior cerebral artery, which could be secondary to intracranial atherosclerotic disease versus embolic occlusion'],\n",
       "  335),\n",
       " (['Hypoperfusion of the left parietal lobe without ischemic core, which may correlate with an M3 segment occlusion'],\n",
       "  351),\n",
       " (['Right middle cerebral artery occlusion with ischemic change in the right striatum with effacement of the right ventricle and edema adjacent temporal pole'],\n",
       "  402)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmatched_occlusion_sents = to.get_unmatched_occlusions()\n",
    "unmatched_occlusion_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '2 CTA demonstrates nonopacification at the right carotid bifurcation extending into the proximal external carotid artery with partial opacification of the proximal external carotid artery branches with complete nonopacification of the right cervical ICA'),\n",
       "   ('uncertain occlusion',\n",
       "    'MCA',\n",
       "    '  Suboptimal collateral opacification of the carotid terminus on the right is noted with subsequent nonopacification of the right M1 segment and paucity of the distal MCA branches with CTA rapid demonstrating less than 45% contrast density within the right MCA'),\n",
       "   ('uncertain occlusion',\n",
       "    'M1',\n",
       "    '  Suboptimal collateral opacification of the carotid terminus on the right is noted with subsequent nonopacification of the right M1 segment and paucity of the distal MCA branches with CTA rapid demonstrating less than 45% contrast density within the right MCA')],\n",
       "  5),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    'Nonocclusive filling defect within the right internal carotid artery just distal to the bifurcation')],\n",
       "  60),\n",
       " ([('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    'Occlusion of the basilar artery and non-opacification of the V4 segment of the left vertebral artery'),\n",
       "   ('uncertain occlusion',\n",
       "    'V4',\n",
       "    'Occlusion of the basilar artery and non-opacification of the V4 segment of the left vertebral artery'),\n",
       "   ('uncertain occlusion',\n",
       "    'basilar',\n",
       "    'Occlusion of the basilar artery and non-opacification of the V4 segment of the left vertebral artery')],\n",
       "  63),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  Nonopacification of the right internal carotid artery just distal to the cervical origin, with distal reconstitution of the cavernous segment'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  Nonopacification of the right internal carotid artery just distal to the cervical origin, with distal reconstitution of the cavernous segment')],\n",
       "  68),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '   Nonopacification of the cervical internal carotid artery with diminutive flow in the left distal ICA')],\n",
       "  81),\n",
       " ([('uncertain occlusion',\n",
       "    'M1',\n",
       "    ' Nonopacification distal to a distal right M1 segment stent')],\n",
       "  108),\n",
       " ([('uncertain occlusion', 'P2', ' Abrupt cut-off of the left P2 segment'),\n",
       "   ('uncertain occlusion', 'P2', ' Abrupt cut-off of the left P2 segment')],\n",
       "  126),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    'Eccentric filling defect at the right internal carotid artery')],\n",
       "  189),\n",
       " ([('uncertain occlusion',\n",
       "    'M2',\n",
       "    '  Abrupt cut off of distal right M2/ proximal M3 with geographic paucity of vasculature compared to contralateral side'),\n",
       "   ('uncertain occlusion',\n",
       "    'M3',\n",
       "    '  Abrupt cut off of distal right M2/ proximal M3 with geographic paucity of vasculature compared to contralateral side')],\n",
       "  199),\n",
       " ([('uncertain occlusion',\n",
       "    'M1',\n",
       "    '  Abrupt cut off of the distal M1 segment of the left middle cerebral artery, consistent with a large vessel occlusion')],\n",
       "  202),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    ' Nonopacification of the right cervical internal carotid artery just beyond the carotid bifurcation, which may represent occlusion or high-grade stenosis, with reconstitution in the right cavernous internal carotid artery')],\n",
       "  207),\n",
       " ([('uncertain occlusion',\n",
       "    'M2',\n",
       "    '  Potential abrupt cut off a posteriorly directed right M2/M3 branch indicating an age indeterminate occlusion; however, given an associated area of encephalomalacia in this vascular distribution, this may represent an old infarct'),\n",
       "   ('uncertain occlusion',\n",
       "    'M3',\n",
       "    '  Potential abrupt cut off a posteriorly directed right M2/M3 branch indicating an age indeterminate occlusion; however, given an associated area of encephalomalacia in this vascular distribution, this may represent an old infarct')],\n",
       "  224),\n",
       " ([('uncertain occlusion',\n",
       "    'M1',\n",
       "    'Filling defect in the distal right M1 likely represents partially occlusive thrombus with total occlusion of anterior branch of M2'),\n",
       "   ('uncertain occlusion',\n",
       "    'M2',\n",
       "    'Filling defect in the distal right M1 likely represents partially occlusive thrombus with total occlusion of anterior branch of M2')],\n",
       "  234),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    ' Occlusion and nonopacification of the right ICA from the level of bifurcation with large ischemic core as above')],\n",
       "  314),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    ' Mixed atherosclerotic plaque at the left carotid bifurcation resulting in approximately 50% stenosis and complete nonopacification of the distal left ICA'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    ' Mixed atherosclerotic plaque at the left carotid bifurcation resulting in approximately 50% stenosis and complete nonopacification of the distal left ICA')],\n",
       "  321),\n",
       " ([('uncertain occlusion',\n",
       "    'MCA',\n",
       "    'Hyperdense left M2 segment of MCA which corresponds to an area of hypoperfusion and abrupt cut off of the left M2 proximal to the bifurcation on the CT angiogram; the constellation of findings are concerning for acute left M2 segment of MCA ischemia'),\n",
       "   ('uncertain occlusion',\n",
       "    'M2',\n",
       "    'Hyperdense left M2 segment of MCA which corresponds to an area of hypoperfusion and abrupt cut off of the left M2 proximal to the bifurcation on the CT angiogram; the constellation of findings are concerning for acute left M2 segment of MCA ischemia')],\n",
       "  350),\n",
       " ([('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    ' Nonopacification of the proximal vertebral arteries extending from their origin to the C2-C3 level on the left and C3-C4 level on the right with distal reconstitution'),\n",
       "   ('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    ' Nonopacification of the proximal vertebral arteries extending from their origin to the C2-C3 level on the left and C3-C4 level on the right with distal reconstitution')],\n",
       "  359),\n",
       " ([('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    'Non-opacification of the proximal right vertebral artery is favored to be chronic or due to diminutive caliber'),\n",
       "   ('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    'Non-opacification of the proximal right vertebral artery is favored to be chronic or due to diminutive caliber')],\n",
       "  371),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    'Long segment nonopacification of the right internal carotid artery is consistent with right internal carotid artery occlusion')],\n",
       "  375),\n",
       " ([('uncertain occlusion',\n",
       "    'M1',\n",
       "    'Abrupt cut off of the left M1 mid segment with reconstitution of the distal M1 segment'),\n",
       "   ('uncertain occlusion',\n",
       "    'M1',\n",
       "    'Abrupt cut off of the left M1 mid segment with reconstitution of the distal M1 segment'),\n",
       "   ('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    'Segmental non-opacification of the right vertebral artery'),\n",
       "   ('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    'Segmental non-opacification of the right vertebral artery')],\n",
       "  409),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  Nonopacification of the right internal carotid artery just distal to the right carotid bifurcation with reconstitution of flow at the right paraclinoid region'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  Nonopacification of the right internal carotid artery just distal to the right carotid bifurcation with reconstitution of flow at the right paraclinoid region')],\n",
       "  411),\n",
       " ([('uncertain occlusion',\n",
       "    'basilar',\n",
       "    ' Abrupt left V4 cut off with basilar occlusion and diminutive reconstitution distally as above')],\n",
       "  418),\n",
       " ([('uncertain occlusion',\n",
       "    'CCA',\n",
       "    ' Long segment occlusion/nonopacification  of the cervical left internal carotid artery beginning just distal to the common carotid artery bifurcation with collateral filling of the intracranial segments with tandem  occlusion of the left middle cerebral artery at the M1/M2 bifurcation'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    ' Long segment occlusion/nonopacification  of the cervical left internal carotid artery beginning just distal to the common carotid artery bifurcation with collateral filling of the intracranial segments with tandem  occlusion of the left middle cerebral artery at the M1/M2 bifurcation'),\n",
       "   ('uncertain occlusion',\n",
       "    'M1',\n",
       "    ' Long segment occlusion/nonopacification  of the cervical left internal carotid artery beginning just distal to the common carotid artery bifurcation with collateral filling of the intracranial segments with tandem  occlusion of the left middle cerebral artery at the M1/M2 bifurcation'),\n",
       "   ('uncertain occlusion',\n",
       "    'M2',\n",
       "    ' Long segment occlusion/nonopacification  of the cervical left internal carotid artery beginning just distal to the common carotid artery bifurcation with collateral filling of the intracranial segments with tandem  occlusion of the left middle cerebral artery at the M1/M2 bifurcation'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    ' Long segment occlusion/nonopacification  of the cervical left internal carotid artery beginning just distal to the common carotid artery bifurcation with collateral filling of the intracranial segments with tandem  occlusion of the left middle cerebral artery at the M1/M2 bifurcation')],\n",
       "  419),\n",
       " ([('uncertain occlusion',\n",
       "    'vertebral',\n",
       "    ' Occlusion of the right vertebral artery involving its distal most V4 segments with a central filling defect extending into the proximal basilar artery that likely represents thrombus'),\n",
       "   ('uncertain occlusion',\n",
       "    'V4',\n",
       "    ' Occlusion of the right vertebral artery involving its distal most V4 segments with a central filling defect extending into the proximal basilar artery that likely represents thrombus'),\n",
       "   ('uncertain occlusion',\n",
       "    'basilar',\n",
       "    ' Occlusion of the right vertebral artery involving its distal most V4 segments with a central filling defect extending into the proximal basilar artery that likely represents thrombus')],\n",
       "  429),\n",
       " ([('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  There is non-opacification of the distal and terminus of the left internal carotid artery and there is partial reconstitution via the anterior communicating artery of the middle cerebral artery territory, which was also not previously seen on the prior angiography'),\n",
       "   ('uncertain occlusion',\n",
       "    'ICA',\n",
       "    '  There is non-opacification of the distal and terminus of the left internal carotid artery and there is partial reconstitution via the anterior communicating artery of the middle cerebral artery territory, which was also not previously seen on the prior angiography')],\n",
       "  438)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertain_occlusions = to.get_uncertain_occlusions()\n",
    "#uncertain_occlusions= [x for x in uncertain_occlusions if x]\n",
    "uncertain_occlusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[334, 49, 402, 189, 351]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_diffs = [ind for sent,ind in ind_vessels_diffs]\n",
    "ind_unmatched = [ind for sent,ind in unmatched_occlusion_sents]\n",
    "list(set(np.ndarray.tolist(to.occl_status_diffs[0]))&set(ind_unmatched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(([], ['R MCA', 'R ICA', 'R CCA']), 5),\n",
       " (([], ['L MCA']), 45),\n",
       " (([], ['R MCA']), 49),\n",
       " ((['R MCA', 'R CCA'], ['R CCA']), 54),\n",
       " ((['L vertebral'], []), 55),\n",
       " ((['R ICA'], []), 60),\n",
       " ((['L vertebral', ' basilar'], ['L vertebral']), 63),\n",
       " ((['R vertebral', ' basilar'], [' basilar']), 78),\n",
       " ((['R MCA'], []), 81),\n",
       " (([], ['R MCA']), 82),\n",
       " ((['R vertebral', 'L vertebral'], []), 103),\n",
       " (([], ['L PCA']), 126),\n",
       " (([], ['R vertebral', 'L PCA']), 137),\n",
       " ((['R PCA', 'R vertebral', 'L vertebral', 'L PCA'], ['R PCA', 'L vertebral']),\n",
       "  169),\n",
       " ((['L MCA', 'R vertebral'], ['L MCA']), 183),\n",
       " (([], ['R MCA']), 189),\n",
       " (([], ['R MCA']), 199),\n",
       " ((['R MCA', 'R ICA'], ['R MCA']), 206),\n",
       " ((['L vertebral'], []), 214),\n",
       " ((['R vertebral', 'L vertebral', ' basilar'], []), 219),\n",
       " ((['R MCA'], []), 224),\n",
       " (([], ['R vertebral']), 233),\n",
       " (([], ['L ICA']), 270),\n",
       " ((['L MCA', 'L vertebral'], ['L MCA']), 271),\n",
       " ((['L MCA', 'L ICA', 'L ACA'], ['L ICA']), 280),\n",
       " ((['R MCA', 'R vertebral'], ['R MCA']), 283),\n",
       " ((['L MCA', 'L ICA'], ['L MCA']), 299),\n",
       " ((['L CCA', 'L ICA'], ['L ICA']), 306),\n",
       " ((['L MCA', 'L ACA'], ['L ICA']), 321),\n",
       " (([], ['L MCA']), 334),\n",
       " ((['L vertebral'], []), 337),\n",
       " ((['L MCA', 'L ICA'], ['L ICA']), 338),\n",
       " (([], ['L MCA']), 350),\n",
       " (([], ['L MCA']), 351),\n",
       " ((['R MCA'], []), 363),\n",
       " ((['R MCA', 'R ICA'], ['R MCA']), 375),\n",
       " ((['R PCA', 'R vertebral'], ['R PCA']), 384),\n",
       " (([], ['R MCA']), 402),\n",
       " ((['L MCA', 'R MCA'], ['L MCA']), 409),\n",
       " (([], ['L MCA']), 413),\n",
       " ((['L vertebral', ' basilar'], ['L vertebral', 'L basilar']), 415),\n",
       " (([], ['L MCA']), 416),\n",
       " ((['L vertebral', ' basilar'], ['L vertebral', 'L basilar']), 418),\n",
       " ((['L CCA', 'L ICA', 'L MCA'], ['L MCA', 'L ICA']), 419),\n",
       " ((['L MCA', 'L ICA'], ['L ICA']), 427),\n",
       " (([], ['R MCA']), 428),\n",
       " ((['R vertebral', ' basilar'], ['R vertebral', 'R basilar']), 429),\n",
       " (([], ['L MCA']), 437),\n",
       " ((['L ICA', 'R ICA'], ['L ICA']), 438)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_vessels_diffs\n",
    "#5 missed occls 419,452, 125, 516, 109\n",
    "#4 falsely called occl 137, 334, 114, 528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unmatched_occlusion_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ind_vessels_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[224, 5, 199, 81, 126, 60, 189, 350]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_diffs = [ind for sent,ind in ind_vessels_diffs]\n",
    "ind_uncertain = [ind for sent,ind in uncertain_occlusions]\n",
    "list(set(np.ndarray.tolist(to.occl_status_diffs[0]))&set(ind_uncertain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncertain_occlusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Right distal P1 segment stenosis and/or occlusion', 'L', 'PCA', None),\n",
       " ('Right distal P1 segment stenosis and/or occlusion', 'L', 'P1', -1),\n",
       " ('Right distal P1 segment stenosis and/or occlusion', 'L', 'P2', None),\n",
       " ('Right distal P1 segment stenosis and/or occlusion', 'L', 'P3', None),\n",
       " ('Right distal P1 segment stenosis and/or occlusion', 'L', 'P4', None),\n",
       " ('Right distal P1 segment stenosis and/or occlusion', 'R', 'PCA', None),\n",
       " ('Right distal P1 segment stenosis and/or occlusion', 'R', 'P1', 1),\n",
       " ('Right distal P1 segment stenosis and/or occlusion', 'R', 'P2', None),\n",
       " ('Right distal P1 segment stenosis and/or occlusion', 'R', 'P3', None),\n",
       " ('Right distal P1 segment stenosis and/or occlusion', 'R', 'P4', None)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAKES NO SENSE\n",
    "#Fix poss ratings\n",
    "to.possibility_ratings[163]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-762241ce9065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmarkups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmarkups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m561\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "markups = list(enumerate(to.markups))\n",
    "markups[561]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#218\n",
    "print(filtered_df[\"Report Text\"].iloc[564])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.segments_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.get_volumes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_volumes = list(zip(to.get_volumes(),range(len(to.get_volumes()))))\n",
    "ind_volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for tup,i in ind_vessel_diffs if tup]\n",
    "ratings = [list(zip(ind_vessel_diffs,to.possibility_ratings))[i] for i in indices]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [['L vertebral', 'L ICA', 'L Basilar'],['L MCA']]\n",
    "labels = [['L ICA', 'L basilar', 'L Vertebral'],['L ICA']]\n",
    "\n",
    "lower = lambda x:x.lower()\n",
    "differences = [(prediction,label) if list(set(map(lower,prediction))^set(map(lower,label))) \n",
    "               else () for prediction,label in zip(predictions,labels) ]\n",
    "\n",
    "score = 0\n",
    "for tup in differences:\n",
    "    if not tup:\n",
    "        score += 1\n",
    "print(score)\n",
    "sets = [list(set(map(lower,prediction))^set(map(lower,label))) for prediction,label in zip(predictions,labels)]\n",
    "#list(map(lower,prediction)) != list(map(lower,label))\n",
    "differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Desc:\n",
    " Using set() to find the difference between two lists in Python\n",
    "\"\"\"\n",
    "\n",
    "def list_diff(list1, list2): \n",
    "    return (list(set(list1) - set(list2))) \n",
    "\n",
    "# Test Input\n",
    "list1 = [26, 41, 36]\n",
    "list2 = [11, 16, 21, 26, 31, 36, 41] \n",
    "\n",
    "\n",
    "# Run Test\n",
    "print(list_diff(list1, list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = [i for i in range(len(occl_labels)) if isinstance(occl_labels[i],list)]\n",
    "multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_occls = [occl_labels[i] for i in multi]\n",
    "multi_occls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_sides = [side_labels[i] for i in multi]\n",
    "m_sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_sides = [i for i in range(len(side_labels)) if isinstance(side_labels[i],list)]\n",
    "l_sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules \n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "# lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "# print(\"occlude :\", lemmatizer.lemmatize(\"occlude\")) \n",
    "#print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_shift_sents = []\n",
    "for file in raw_text_files:\n",
    "    for sent in file.split(\".\"):\n",
    "        if 'midline shift' in sent:\n",
    "            mid_shift_sents.append(sent)\n",
    "mid_shift_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [0,-1,2,-3,4,-5]\n",
    "r = -1\n",
    "sum(abs(np.multiply(q,r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = float(\"nan\")\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(q) == \"nan\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
